<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<title>A Dynamic Adaptive HTTP Streaming Video Service for Google Android. Luciano Rubio</title>
	<meta name="keywords" content="HTTP,live,video,streaming,MPEG-DASH,android"/>
	<meta name="description" content="" />
	<link href="default.css" rel="stylesheet" type="text/css" />
</head>
<body>

<div class="valid">
	<a href="http://validator.w3.org/check?uri=referer"><img src="http://www.w3.org/Icons/valid-xhtml10" alt="Valid XHTML 1.0 Strict" height="31" width="88" /></a>
</div>


<h1 class="title">A Dynamic Adaptive HTTP Streaming Video Service for Google Android</h1>

<div class="author">
Luciano Rubio
<div class="email">
lrr@kth.se
</div>
</div>

<div class="info">
	<p>Master of Science Thesis</p>
	<p>Academic supervisor: Gerald Q. Maguire Jr.</p>
	<p>Industrial supervisor: Thorsten Lohmar</p>
	<p>Royal Institute of Technology (KTH)</p>
	<p>October 6, 2011</p>
</div>

<div class="abstract">

<h1>Abstract</h1>

<p>Adaptive streaming approaches over Hypertext Transfer Protocol (HTTP), such as
Apple's HTTP Live streaming (HLS) and Microsoft's Live Smooth Streaming, have
recently become very popular. This master's thesis project developed and
evaluated several media rate adaptation algorithms optimized for mobile networks
with a client running on Google's Android operating system. The deployed service
supports HLS and the emerging ISO/IEC MPEG standard called Dynamic Adaptive
Streaming over HTTP (MPEG-DASH).</p>

<p>Live media was the focus of the evaluation, since this content can not be cached
in advance at the user's device, hence the quality of the user's experience
will be affected by the currently available bandwidth which the user can
utilize. Experiments were performed for multiple scenarios illustrating
different network capabilities, especially various amounts of bandwidth
available to the user.</p>

<p>This project has produced an implementation of HTTP-based adaptive
streaming. This implementation follows the MPEG standard and enables robust
and smooth playback of live video content via Google's Android devices. Results
of the experiments have shown that the proposed adaptation mechanisms
efficiently utilize the available bandwidth of the network. A clear conclusion
of this thesis is that adaptive streaming will in fact enable substantial
numbers of users to enjoy live media streaming to their devices.</p>

<p><strong>Keywords:</strong> <em>HTTP, live, video, streaming, MPEG-DASH, Android.</em></p>
</div>

<h1>Table of Contents</h1>

<div class="toc">
	<ul class="child">
		<li><a href="#acknowledgements">Acknowledgements</a></li>
	</ul>
	
	<ul class="child">
		<li><a href="#acronyms">List of Acronyms and Abbreviations</a></li>
	</ul>
	
	<ul class="child">
		<li><a href="#1">1 Introduction</a>
			<ul class="subchild">
				<li><a href="#1.1">1.1 The problem and motivation</a></li>
				<li><a href="#1.2">1.2 Goals</a></li>
				<li><a href="#1.3">1.3 Scope</a></li>
				<li><a href="#1.4">1.4 Audience</a></li>
				<li><a href="#1.5">1.5 Organization of the thesis</a></li>
			</ul>
		</li>
	</ul>

	<ul class="child">	
		<li><a href="#2">2 Background</a>
			<ul class="subchild">
				<li>
					<a href="#2.1">2.1 Traditional streaming</a>
					<ul class="subsubchild">
						<li><a href="#2.1.1">2.1.1 Real-Time Transport Protocol (RTP)</a></li>  
						<li><a href="#2.1.2">2.1.2 Real-Time Streaming Protocol (RTSP)</a></li> 
					</ul>
				</li>
				<li><a href="#2.2">2.2 Progressive download</a></li>
				<li>
					<a href="#2.3">2.3 Adaptive streaming</a>
					<ul class="subsubchild">
						<li><a href="#2.3.1">2.3.1 Transcoding</a></li>  
						<li><a href="#2.3.2">2.3.2 Scalable encoding</a></li> 
						<li><a href="#2.3.3">2.3.3 Stream switching</a></li> 
					</ul>
				</li>
				<li>
					<a href="#2.4">2.4 HTTP-based adaptive streaming</a>
					<ul class="subsubchild">
						<li><a href="#2.4.1">2.4.1 Why HTTP?</a></li>  
						<li><a href="#2.4.2">2.4.2 Apple's HTTP Live Streaming (HLS)</a></li> 
						<li><a href="#2.4.3">2.4.3 Microsoft's Live Smooth Streaming (LSS)</a></li> 
						<li><a href="#2.4.4">2.4.4 Adobe's HTTP Dynamic Streaming</a></li> 
						<li><a href="#2.4.5">2.4.5 MPEG Dynamic Adaptive Streaming over HTTP (MPEG-DASH)</a></li> 
					</ul>
				</li>
				<li>
					<a href="#2.5">2.5 Video CODECs</a>
					<ul class="subsubchild">
						<li><a href="#2.5.1">2.5.1 Video frames</a></li>  
						<li><a href="#2.5.2">2.5.2 Decoding and presentation time-stamps</a></li> 
						<li><a href="#2.5.3">2.5.3 H.263</a></li> 
						<li><a href="#2.5.4">2.5.4 H.264/MPEG-4 AVC</a></li> 
						<li><a href="#2.5.5">2.5.5 VP8</a></li> 
					</ul>
				</li>
				<li>
					<a href="#2.6">2.6 Audio CODECs</a>
					<ul class="subsubchild">
						<li><a href="#2.6.1">2.6.1 MP3</a></li>  
						<li><a href="#2.6.2">2.6.2 Advanced Audio Coding (AAC)</a></li> 
						<li><a href="#2.6.3">2.6.3 Vorbis</a></li> 
					</ul>
				</li>
				<li><a href="#2.7">2.7 Container formats</a></li>
				<li><a href="#2.8">2.8 Quality video levels</a></li>
				<li><a href="#2.9">2.9 Video on-demand and live streaming</a></li>
				<li>
					<a href="#2.10">2.10 Google's Android operating system</a>
					<ul class="subsubchild">
						<li><a href="#2.10.1">2.10.1 Media formats supported on Android</a></li>  
						<li><a href="#2.10.2">2.10.2 Adaptive protocols over HTTP supported on Android</a></li> 
						<li><a href="#2.10.3">2.10.3 Apple-HLS support</a></li> 
					</ul>
				</li>
				<li><a href="#2.11">2.11 Comparison among the different HTTP-based adaptive solutions</a></li>
			</ul>
		</li>
	</ul>
	
	<ul class="child">
		<li><a href="#3">3 Related work</a></li>
	</ul>
	
	<ul class="child">
		<li><a href="#4">4 Design and implementation</a>
			<ul class="subchild">
					<li>
						<a href="#4.1">4.1 Content preparation</a>
						<ul class="subsubchild">
							<li><a href="#4.1.1">4.1.1 Transcoder module</a></li>  
							<li><a href="#4.1.2">4.1.2 Segmenter and combiner modules</a></li> 
							<li><a href="#4.1.3">4.1.3 Indexing module</a></li> 
						</ul>
					</li>
					<li><a href="#4.2">4.2 Synchronization between server and client</a></li>
					<li>
						<a href="#4.3">4.3 HTTP Servers</a>
						<ul class="subsubchild">
							<li><a href="#4.3.1">4.3.1 On-demand server</a></li>  
							<li><a href="#4.3.2">4.3.2 Live server</a></li> 
						</ul>
					</li>
					<li><a href="#4.4">4.4 Client</a>
						<ul class="subsubchild">
							<li><a href="#4.4.1">4.4.1 Features</a></li>  
							<li>
								<a href="#4.4.2">4.4.2 Adaptation mechanisms</a>
								<ul class="subsubsubchild">
									<li><a href="#4.4.2.1">4.4.2.1 Aggressive adaptive mechanism</a></li>  
									<li><a href="#4.4.2.2">4.4.2.2 Conservative adaptive mechanism</a></li> 
									<li><a href="#4.4.2.3">4.4.2.3 Mean adaptive mechanism</a></li> 
								</ul>
							</li>
							<li>
								<a href="#4.4.3">4.4.3 Module characterization</a>
								<ul class="subsubsubchild">
									<li><a href="#4.4.3.1">4.4.3.1 Activities</a></li>  
								</ul>
							</li> 
							<li>
								<a href="#4.4.4">4.4.4 Player module</a> 
								<ul class="subsubsubchild">
									<li><a href="#4.4.4.1">4.4.4.1 Video surface management</a></li>  
									<li><a href="#4.4.4.2">4.4.4.2 Implementation</a></li> 
								</ul>
							</li>
							<li>
								<a href="#4.4.5">4.4.5 Parser module</a> 
								<ul class="subsubsubchild">
									<li><a href="#4.4.5.1">4.4.5.1 DOM and SAX</a></li>  
									<li><a href="#4.4.5.2">4.4.5.2 Implementation</a></li> 
								</ul>
							</li>
							<li>
								<a href="#4.4.6">4.4.6 Segment-downloader module</a> 
								<ul class="subsubsubchild">
									<li><a href="#4.4.6.1">4.4.6.1 Implementation</a></li>   
								</ul>
							</li>
							<li>
								<a href="#4.4.7">4.4.7 Rate adaptation module</a>
								<ul class="subsubsubchild">
									<li><a href="#4.4.7.1">4.4.7.1 Implementation</a></li>   
								</ul>
							</li>
							<li>
								<a href="#4.4.8">4.4.8 Transcoder module</a> 
								<ul class="subsubsubchild">
									<li><a href="#4.4.8.1">4.4.8.1 Implementation</a></li>   
								</ul>
							</li>
							<li>
								<a href="#4.4.9">4.4.9 Timer module</a> 
								<ul class="subsubsubchild">
									<li><a href="#4.4.9.1">4.4.9.1 Implementation</a></li>  
								</ul>
							</li>
							<li>
								<a href="#4.5">4.5 Network emulator</a> 
								<ul class="subsubsubchild">
									<li><a href="#4.5.1">4.5.1 Emulator requisites</a></li>  
									<li><a href="#4.5.2">4.5.2 Dummynet</a></li> 
								</ul>
							</li>
						</ul>
					</li>
			</ul>
		</li>
	</ul>
	
	<ul class="child">
		<li>
			<a href="#5">5 Evaluation</a>
			<ul class="subchild">
				<li>
					<a href="#5.1">5.1 Experimental environment</a>
					<ul class="subsubchild">
						<li><a href="#5.1.1">5.1.1 Experimental devices</a></li>
						<li><a href="#5.1.2">5.1.2 Content source</a></li>
						<li><a href="#5.1.3">5.1.3 Segmentation schemas</a></li>
						<li><a href="#5.1.4">5.1.4 Selection of media quality levels</a></li>
						<li><a href="#5.1.5">5.1.5 Input and output characterization</a></li>
						<li>
							<a href="#5.1.6">5.1.6 Metrics</a>
							<ul class="subsubsubchild">
								<li><a href="#5.1.6.1">5.1.6.1 Weighted functions</a></li>
								<li><a href="#5.1.6.2">5.1.6.2 Bandwidth utilization</a></li>
								<li><a href="#5.1.6.3">5.1.6.3 Bandwidth efﬁciency</a></li>
								<li><a href="#5.1.6.4">5.1.6.4 Buffering efﬁciency</a></li>
								<li><a href="#5.1.6.5">5.1.6.5 Segment-fetch efﬁciency</a></li>
								<li><a href="#5.1.6.6">5.1.6.6 Segment-retry efﬁciency</a></li>
								<li><a href="#5.1.6.7">5.1.6.7 End-to-end latency</a></li>
								<li><a href="#5.1.6.8">5.1.6.8 Active efﬁciency</a></li>
								<li><a href="#5.1.6.9">5.1.6.9 Start-up efﬁciency</a></li>
								<li><a href="#5.1.6.10">5.1.6.10 Reaction efﬁciency</a></li>
							</ul>
						</li>
						<li><a href="#5.1.7">5.1.7 Network scenarios</a></li>
					</ul>
				</li>
				<li>
					<a href="#5.2">5.2 Scenario 1: long-term variations of the available bandwidth</a>
					<ul class="subsubchild">
						<li>
							<a href="#5.2.1">5.2.1 Performance of the adaptation mechanisms</a>
								<ul class="subsubsubchild">
									<li><a href="#5.2.1.1">5.2.1.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li>
							<a href="#5.2.2">5.2.2 Performance with different duration segments</a>
								<ul class="subsubsubchild">
									<li><a href="#5.2.2.1">5.2.2.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li><a href="#5.2.3">5.2.3 Analysis of the end-to-end latency</a></li>
						<li><a href="#5.2.4">5.2.4 Discussion</a></li>
					</ul>
				</li>
				<li>
					<a href="#5.3">5.3 Scenario 2: short-term variations of the available bandwidth</a>
					<ul class="subsubchild">
						<li>
							<a href="#5.3.1">5.3.1 Performance of the adaptation mechanisms</a>
								<ul class="subsubsubchild">
									<li><a href="#5.3.1.1">5.3.1.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li>
							<a href="#5.3.2">5.3.2 Performance with different duration segments</a>
								<ul class="subsubsubchild">
									<li><a href="#">5.3.2.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li><a href="#5.3.3">5.3.3 Analysis of the end-to-end latency</a></li>
						<li><a href="#5.3.4">5.3.4 Discussion</a></li>
					</ul>
				</li>
				<li>
					<a href="#5.4">5.4 Scenario 3: peaks in the available bandwidth</a>
					<ul class="subsubchild">
						<li>
							<a href="#5.4.1">5.4.1 Performance of the adaptation mechanisms</a>
								<ul class="subsubsubchild">
									<li><a href="#">5.4.1.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li>
							<a href="#5.4.2">5.4.2 Performance with different duration segments</a>
								<ul class="subsubsubchild">
									<li><a href="#">5.4.2.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li><a href="#5.4.3">5.4.3 Analysis of the end-to-end latency</a></li>
						<li><a href="#5.4.4">5.4.4 Discussion</a></li>
					</ul>
				</li>
				<li>
					<a href="#5.5">5.5 Scenario 4: troughs in the available bandwidth</a>
					<ul class="subsubchild">
						<li>
							<a href="#5.5.1">5.5.1 Performance of the adaptation mechanisms</a>
								<ul class="subsubsubchild">
									<li><a href="#5.5.1.1">5.5.1.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li>
							<a href="#5.5.2">5.5.2 Performance with different duration segments</a>
								<ul class="subsubsubchild">
									<li><a href="#5.5.2.1">5.5.2.1 Impact on the metrics</a></li>
								</ul>
						</li>
						<li><a href="#5.5.3">5.5.3 Analysis of the end-to-end latency</a></li>
						<li><a href="#5.5.4">5.5.4 Discussion</a></li>
					</ul>
				</li>
				<li>
					<a href="#5.6">5.6 Effects of packet loss</a>
					<ul class="subsubchild">
						<li><a href="#5.6.1">5.6.1 Impact on the metrics</a></li>
						<li><a href="#5.6.2">5.6.2 Discussion</a></li>
					</ul>
				</li>
				<li>
					<a href="#5.7">5.7 Evaluation with real live events</a>
					<ul class="subsubchild">
						<li><a href="#5.7.1">5.7.1 Impact on the metrics</a></li>
						<li><a href="#5.7.2">5.7.2 Discussion</a></li>
					</ul>
				</li>
			</ul>			
		</li>
	</ul>					

	<ul class="child">
		<li>
			<a href="#6">6 Conclusions</a>
			<ul class="subchild">
				<li><a href="#6.1">6.1 Discussion</a></li>
				<li><a href="#6.2">6.2 Future work</a></li>
			</ul>
		</li>
	</ul>
	
	<ul class="child">
		<li><a href="#bibliography">Bibliography</a></li>
	</ul>
	
	<ul class="child">
		<li>
			<a href="#A">A Demonstration of the client's application</a>
			<ul class="subchild">
				<li><a href="#A.1">A.1 Graph generator</a></li>
				<li><a href="#A.2">A.2 Logging system</a></li>
				<li>
					<a href="#A.3">A.3 Overview of the client’s GUI</a>
					<ul class="subsubchild">
						<li><a href="#A.3.1">A.3.1 Adding media sources</a></li>
						<li><a href="#A.3.1">A.3.2 Importing multiple media sources</a></li>
						<li><a href="#A.3.3">A.3.3 Searching for media sources</a></li>
						<li><a href="#A.3.4">A.3.4 Modifying and deleting media sources</a></li>
						<li><a href="#A.3.5">A.3.5 Opening a media source</a></li>
						<li><a href="#A.3.6">A.3.6 Playback during the streaming session</a></li>
					</ul>
				</li>
			</ul>
		</li>
	</ul>
	
	<ul class="child">
		<li><a href="#B">B FFmpeg capabilities</a></li>
	</ul>
	
	<ul class="child">
		<li><a href="#C">C Integration of FFmpeg libraries using the Android NDK</a></li>
	</ul>
	
	<ul class="child">
		<li><a href="#trivia">Trivia</a></li>
	</ul>
</div>

<h1><a name="acknowledgements"></a>Acknowledgements</h1>

<div class="quoteblock">
	<p class="quote"><em>To Dad.</em></p>
</div>

<p>I am enormously grateful to <em>Gerald Q. Maguire Jr.</em>
for his highly valuable comments and advice, which have contributed immensely
to this master's thesis project. I would like to acknowledge <em>Thorsten
Lohmar</em> for giving me the opportunity to realize this project at Ericsson in
Aachen, Germany. Thorsten has provided me with thoughtful suggestions and
guidance (which can be found throughout this thesis).</p>

<p>I am greatly thankful to my co-workers at Ericsson for their remarkable
contributions and support: <em>Thomas Johansson, Magued Sedra, Thorsten Dudda, Duong 'James' Quoc
Trong, Peng Wang, Burcu Hanta</em>, and <em>Jairo Alfonso Garc&iacute;a Luna</em>.</p>

<p>I would like to thank my friends from Toledo and my colleagues at the
university, especially <em>Urko Serrano Badiola, Sergio Gayoso
Fern&aacute;ndez</em>, and <em>Sergio Floriano S&aacute;nchez</em> from the Royal Institute of
Technology (KTH) and <em>Federico Navarro Rodr&iacute;guez</em> and <em>Ricardo O&#328;a
Mart&iacute;nez-Albelda</em> from the Technical University of Madrid (UPM) for sharing
their magnificent experience, enthusiasm, and liveliness with me. Their friendship is
gratefully acknowledged. Special thanks goes to <em>Reyes Albo
S&aacute;nchez-Bedoya</em> for her innumerable advice during my studies.</p>

<p>Finally, I would like to express my infinite gratitude to my mother and my
brother for their outstanding support in Spain and abroad. And <em>Kathleen
Streit</em>, with love.</p>


<h1><a name="acronyms"></a>List of Acronyms and Abbreviations</h1>

<div class="acronyms">
	<table>
		<tr><td class="acronym">3GPP</td><td>3rd Generation Partnership Project</td></tr>
		<tr><td class="acronym">AAC</td><td>Advanced Audio Coding</td></tr>
		<tr><td class="acronym">AVC</td><td>Advanced Video Coding</td></tr>
		<tr><td class="acronym">BP</td><td>Baseline Profile</td></tr>
		<tr><td class="acronym">CBP</td><td>Constrained Baseline Profile</td></tr>
		<tr><td class="acronym">CDN</td><td>Content Delivery Network</td></tr>
		<tr><td class="acronym">CODEC</td><td>COmpressor-DECompressor</td></tr>
		<tr><td class="acronym">CPU</td><td>Central Processing Unit</td></tr>
		<tr><td class="acronym">DF</td><td>Delivery Format</td></tr>
		<tr><td class="acronym">DOM</td><td>Document Object Model</td></tr>
		<tr><td class="acronym">DSS</td><td>Darwin Streaming Server</td></tr>
		<tr><td class="acronym">DTS</td><td>Decoding Time-Stamp</td></tr>
		<tr><td class="acronym">DVM</td><td>Dalvik Virtual Machine</td></tr>
		<tr><td class="acronym">GOP</td><td>Group Of Pictures</td></tr>
		<tr><td class="acronym">GUI</td><td>Graphical User Interface</td></tr>
		<tr><td class="acronym">HDS</td><td>Adobe's HTTP Dynamic Streaming</td></tr>
		<tr><td class="acronym">HLS</td><td>Apple's HTTP Live Streaming</td></tr>
		<tr><td class="acronym">HTML</td><td>Hypertext Markup Language</td></tr>
		<tr><td class="acronym">HTTP</td><td>HyperText Transfer Protocol</td></tr>
		<tr><td class="acronym">IEC</td><td>International Electrotechnical Commission</td></tr>
		<tr><td class="acronym">IETF</td><td>Internet Engineering Task Force</td></tr>
		<tr><td class="acronym">IPTV</td><td>Internet Protocol Television</td></tr>
		<tr><td class="acronym">ISO</td><td>International Organization for Standardization</td></tr>
		<tr><td class="acronym">ITU</td><td>International Telecommunication Union</td></tr>
		<tr><td class="acronym">JNI</td><td>Java Native Interface</td></tr>
		<tr><td class="acronym">JVM</td><td>Java Virtual Machine</td></tr>
		<tr><td class="acronym">LGPL</td><td>Lesser General Public License</td></tr>
		<tr><td class="acronym">LSS</td><td>Microsoft's Live Smooth Streaming</td></tr>
		<tr><td class="acronym">MEGACO</td><td>Media Gateway Control Protocol</td></tr>
		<tr><td class="acronym">MF</td><td>Manifest File</td></tr>
		<tr><td class="acronym">MIME</td><td>Multipurpose Internet Mail Extensions</td></tr>
		<tr><td class="acronym">MMUSIC</td><td>Multiparty Multimedia Session Control (Working Group)</td></tr>
		<tr><td class="acronym">MS IIS</td><td>Microsoft Internet Information Services</td></tr>
		<tr><td class="acronym">MPD</td><td>Media Presentation Description</td></tr>
		<tr><td class="acronym">MPEG</td><td>Moving Picture Experts Group</td></tr>
		<tr><td class="acronym">MPEG-DASH</td><td>MPEG Dynamic Adaptive Streaming over HTTP</td></tr>
		<tr><td class="acronym">M2TS</td><td>MPEG-2 Transport Stream</td></tr>
		<tr><td class="acronym">MVC</td><td>Multiview Video Coding</td></tr>
		<tr><td class="acronym">NAT</td><td>Network Address Translation</td></tr>
		<tr><td class="acronym">NTP</td><td>Network Time Protocol</td></tr>
		<tr><td class="acronym">OHA</td><td>Open Headset Alliance</td></tr>
		<tr><td class="acronym">OS</td><td>Operating System</td></tr>
		<tr><td class="acronym">PCM</td><td>Pulse-Code Modulation</td></tr>
		<tr><td class="acronym">PIFF</td><td>Protected Interoperable File Format</td></tr>
		<tr><td class="acronym">PTS</td><td>Presentation Time-Stamp</td></tr>
		<tr><td class="acronym">QSS</td><td>QuickTime Streaming Server</td></tr>
		<tr><td class="acronym">RAP</td><td>Random Access Point</td></tr>
		<tr><td class="acronym">RTMP</td><td>Real Time Messaging Protocol</td></tr>
		<tr><td class="acronym">RTP</td><td>Real-time Transport Protocol</td></tr>
		<tr><td class="acronym">RTCP</td><td>RTP Control Protocol</td></tr>
		<tr><td class="acronym">RTSP</td><td>Real Time Streaming Protocol</td></tr>
		<tr><td class="acronym">SAX</td><td>Java's Simple API for XML</td></tr>
		<tr><td class="acronym">SCCP</td><td>Skinny Call Control Protocol</td></tr>
		<tr><td class="acronym">SDK</td><td>Software Development Kit</td></tr>
		<tr><td class="acronym">SIP</td><td>Session Initiation Protocol</td></tr>
		<tr><td class="acronym">SNTP</td><td>Simple Network Time Protocol</td></tr>
		<tr><td class="acronym">SVC</td><td>Scalable Video Coding</td></tr>
		<tr><td class="acronym">TCP</td><td>Transmission Control Protocol</td></tr>
		<tr><td class="acronym">UDP</td><td>User Datagram Protocol</td></tr>
		<tr><td class="acronym">URI</td><td>Uniform Resource Identifier</td></tr>
		<tr><td class="acronym">URL</td><td>Universal Resource Locator</td></tr>
		<tr><td class="acronym">VCEG</td><td>Video Coding Experts Group</td></tr>
		<tr><td class="acronym">VLC</td><td>VideoLan Player</td></tr>
		<tr><td class="acronym">WAN</td><td>Wide Area Network</td></tr>
		<tr><td class="acronym">WAVE</td><td>Waveform Audio File Format</td></tr>
		<tr><td class="acronym">XML</td><td>Extensible Markup Language</td></tr>
	</table>
</div>




<h1><a name="chap:introduction"></a><a name="1"></a>1 Introduction</h1>

<div class="quoteblock">
	<p class="quote"><em>"The important thing is the diversity available on the Web."</em></p>
	<p class="quoteAuthor">- Tim Berners-Lee</p>
</div>

<p>Today streaming is a very popular technology by which multimedia content is
delivered continuously from a server to end-users<a href="#footnote1" name="note1"><sup>1</sup></a>. Streaming methods are constantly being improved since the network capabilities and usage scenarios are quite heterogeneous. The creation of techniques which automatically provide the best possible quality to consumers has become a important
challenge [<a href="#lee">48</a>]. By means of the widely used Hypertext Transfer Protocol
(HTTP) [<a href="#rfc-2616">31</a>] which is the <em>de facto</em> protocol of today's Internet, new streaming
approaches have been developed [<a href="#lee">48</a>, <a href="#smooth">55</a>, <a href="#pantos">59</a>, <a href="#stockhammer">69</a>].</p>

<p>Recent studies [<a href="#cisco">24</a>] have shown the crescent diversity of end-user
devices. Mobile phones have become immensely popular in the recent years, since
they have been significantly enhanced, providing Internet-based services over
wireless and broadband connections. <em>Smartphones</em> offer capabilities
similar to modern computers, as they run more sophisticated operating systems
than regular cellphones (allowing the installation of third-party applications). Figure
<a href="#fig:cisco-traffic">1.1</a> illustrates predictions for the next several
years in terms of network traffic, suggesting that there will be a considerable
increase mobile traffic (estimated to represent the 26.6% of the total network
traffic in 2015).</p>

<div class="figure">
	<a name="fig:cisco-traffic"></a>
	<img src="img/cisco-traffic.png" alt="Network traffic expected for different devices. Laptops and smartphones 
		lead traffic growth."/>
	<div class="caption">
		<span class="captionTitle">Figure 1.1</span> Network traffic expected for different devices. Laptops and smartphones 
		lead traffic growth. Extracted from [<a href="#cisco">24</a>] (published in February 2011).
	</div>
</div>

<p>Video data has unequivocally become the predominant type of content
transferred by mobile applications. As shown in figure <a href="#fig:cisco-type">1.2</a>,
video traffic is expected to grow exponentially in the next several years,
prevailing (66%) over web (20.9%) and peer-to-peer (6.1%) traffic.</p>

<div class="figure">
	<a name="fig:cisco-type"></a>
	<img src="img/cisco-traffic-type.png" alt="Estimate of the type of traffic to/from smartphone."/>
	<div class="caption">
		<span class="captionTitle">Figure 1.2</span> Estimate of the type of traffic to/from smartphone. Extracted
	from [<a href="#cisco">24</a>] (published in February 2011).
	</div>
</div>

<h2><a name="1.1"></a>1.1 The problem and motivation</h2>

<p>The immense variety of end-user devices operating under heterogeneous mobile
networks leads to an interesting challenge: produce dynamic and
automatized adaptation between producers and consumers, to deliver the
best possible quality of content. Multiple constraints are present in the
process of content delivery, such as network rate fluctuations or the client's
own capabilities. For instance, end-users' devices can be limited by display
resolution, maximum video bit-rate, or supported media formats. This master's
thesis project focuses on portable devices and considers these limitations.</p>

<p>Figure <a href="#fig:adaptive">1.3</a> exemplifies the adaptation for similar clients which
experience different limitations in the underlying communication network, hence
the amount of data supplied per unit time to these clients differ. In this
context, <em>adaptive streaming</em> [<a href="#lee">48</a>] represents a family of techniques
which addresses the problem of the difference in the data provided to different
clients. By means of layered media content and adaptation
mechanisms, end-users can perceive the most appropriate level of
quality given their current constraints [<a href="#cicco">23</a>]. The most popular adaptive
techniques will be introduced in the next chapter (section <a href="#sec:adaptive-streaming">2.3</a>).</p>

<div class="figure">
	<a name="fig:adaptive"></a>
	<img src="img/introduction.png" alt="A simplified example of adaptive streaming offered to end-users."/>
	<div class="caption">
		<span class="captionTitle">Figure 1.3</span> A simplified example of adaptive streaming offered to end-users.
	</div>
</div>

<p>In the particular case of live video streaming (that is, non-previously recorded
media) there is still a need to evaluate different adaptive solutions
under a variety of conditions in wireless networks. Reusing existing protocols
to create Content Delivery Networks (CDN) [<a href="#rfc-3466">25</a>] would provide
enormous advantages to those wishing to offer live streaming services, as they
could take advantage of the optimizations that have been made to efficiently
support these protocols and the large investment in the existing
infrastructures.</p>

<p>Multiple operating systems (OSs) have been developed for smartphones. Android
(explained in detail in section <a href="#sec:android">2.10</a>) is an open-sourced code based
mobile OS developed by Google (Android's official logos are depicted in figure 
<a href="#fig:android-logo">1.4</a>). Recent statistics [<a href="#gartner">33</a>] have shown
that Android is the predominant mobile operating system (36% worldwide)
followed by Symbian (27.4%), Apple's iOS (16.8%), RIM (12.9%), and Windows
Mobile (3.6%) (figure <a href="#fig:os-distribution">1.5a</a>). Furthermore, Android is
expected to be deployed in almost the 50% of smartphones sold in 2012,
followed by Apple's iOS (figure <a href="#fig:prediction">1.5b</a>).</p>

<div class="figure">
	<a name="fig:android-logo"></a>
	<img src="img/android-logos.png" alt="Android official logos."/>
	<div class="caption">
		<span class="captionTitle">Figure 1.4</span> Android official logos.
	</div>
</div>

<div class="figure">
	<table class="subfig">
		<tr>
			<td>
				<a name="fig:os-distribution"></a>
				<img src="img/os-distribution.png" alt="May 2011"/>
			</td>
			<td>
				<span class="bigspace"></span>
			</td>
			<td>
				<a name="fig:prediction"></a>
				<img src="img/os-distribution-2012.png" alt="2012 estimation"/>
				
			</td>
		</tr>
		<tr>
			<td>
				<span class="subfig">(a) May 2011</span>
			</td>
			<td>
			</td>
			<td>
				<span class="subfig">(b) 2012 estimation</span>
			</td>
		</tr>
	</table>
	
	<div class="caption">
		<span class="captionTitle">Figure 1.5</span> Worldwide smartphone sales to end users by Operating System in the first quarter of 2011. Extracted from [<a href="#gartner">33</a>] (May 2011).
	</div>
</div>

<p>At the moment there are few adaptive streaming services for Google's Android,
despite Apple Inc. having published and implemented a protocol known as HTTP
Live Streaming (HLS) [<a href="#pantos">59</a>] already supported in Apple's mobile
phones (the well-known family of <em>iPhone</em> devices). Furthermore, Apple-HLS
is in the process of becoming an Internet Engineering Task Force (IETF)
standard. Other parties, such as the ISO/IEC Moving Picture Experts Group
(MPEG), have proposed a standard (that is still in development) for
adaptive streaming over HTTP, known as Dynamic Adaptive Streaming over HTTP
(DASH) [<a href="#stockhammer">69</a>].</p>

<h2><a name="1.2"></a>1.2 Goals</h2>

<p>This master's thesis project is motivated by the following goals:</p>

<ol>
  <li>Proposal and evaluation of different adaptive mechanisms on the
  client's side which are able to converge to the maximum sustainable
  bit-rate. These mechanisms specify the client's application logic, providing
  efficient use of the available bit-rate in the network. Different procedures
  will be considered to optimize the use of available bandwidth and studying
  potential disadvantages.</li>
  
  <li>Evaluation of system aspects in heterogeneous network scenarios
  where network bit-rate, packet delay, packet lost, video quality levels,
  segment durations, and other aspects differ. This leads to an analysis of
  potential benefits of the adaptive mechanisms, since they will diverge in
  terms of performance under different conditions.</li>
</ol>

<p>In order to achieve these goals, the following tasks are defined in this
project:</p>

<ul>
  <li>Design of a service which supports the fundamental aspects of Apple-HLS
  and MPEG-DASH. The system produces different quality video levels and
  segments the media content into small <em>segments</em> which are offered to
  clients.</li>
  
  <li>Implementation of a prototype client as an application for Google's
  Android operating system. Achieving this goal requires that we identify
  what development resources are available for Android and select which of
  these resources we will use. This leads to an extensive study of the
  capabilities of the Android's native media framework, <em>Stagefright</em>,
  focusing on live video content. In particular, to analyze which media formats
  and streaming protocols are natively supported by Stagefright.</li>
  
  <li>Definition of multiple metrics to analyze during the
  evaluation. The evaluation will include efficiency, performance,
  delays, and bandwidth utilization. These metrics have to be defined
  conveniently in order to efficiently compare the adaptive mechanisms.</li>
</ul>

<h2><a name="1.3"></a>1.3 Scope</h2>

<p>This project intends to evaluate the performance of different adaptive
mechanisms under single end-user scenarios. Therefore, the scalability of
the system (i.e., multiple users requesting media content) is not
covered by this master's thesis project.</p>

<p>The network communication in this project is based on HTTP and uses TCP as the
transport protocol, since it provides reliable byte stream delivery and
congestion avoidance mechanisms. The advantages or disadvantages of using other
transport protocols are not considered in this work.</p>

<h2><a name="1.4"></a>1.4 Audience</h2>

<p>Software engineers and Android developers interested in adaptive media content
delivery could benefit from this master's thesis project. In this work, the
most recent adaptive streaming standards (using HTTP as a delivery protocol)
have been considered.</p>

<h2><a name="1.5"></a>1.5 Organization of the thesis</h2>

<p>Chapter <a href="#chap:background">2</a> presents the relevant background,
introducing different streaming techniques and the adaptive protocols which have
been recently published, such as Apple's HTTP Live Streaming, Microsoft's Live
Smooth Streaming, Adobe's HTTP Dynamic Streaming, and MPEG Dynamic Adaptive
Streaming over HTTP. In addition, the capabilities of the Android operating
system are introduced, focusing on media formats, coders/decoders (CODECs), and
adaptive protocols which are supported in Stagefright.</p>
  
<p>Chapter <a href="#chap:related-work">3</a> summarizes the previous work which has
been done in the area of the adaptive streaming, including simulations under
heterogeneous network restrictions, performance of the different
adaptive protocols, and proposals of adaptation mechanisms.</p>
  
<p>Chapter <a href="#chap:design">4</a> explains on detail the proposed system
architecture which has been designed and implemented during this master's
thesis project.</p>
  
<p>Chapter <a href="#chap:evaluation">5</a> covers the overall evaluation performed
for the system architecture explained in chapter <a href="#chap:design">4</a>. This
chapter includes the definition of the metrics utilized, the input and output
parameters, and the results achieved.</p>
  
<p>Chapter <a href="#chap:conclusions">6</a> presents a discussion of the results achieved
in chapter <a href="#chap:evaluation">5</a> and the conclusions. Finally,
the limitations of this master's thesis project are considered and presented as
future work.</p>

<h1><a name="chap:background"></a><a name="2"></a>2 Background</h1>

<div class="quoteblock">
	<p class="quote"><em>"Any sufficiently advanced technology is indistinguishable from magic."</em></p>
	<p class="quoteAuthor">- Arthur C. Clarke</p>
</div>

<p>There are three main methods to deliver multimedia: <em>traditional streaming</em>
(section <a href="#sec:traditional-streaming">2.1</a>), <em>progressive download</em>
(section <a href="#sec:progressive-download">2.2</a>), and <em>adaptive streaming</em>
(section <a href="#sec:adaptive-streaming">2.3</a>).
Section <a href="#sec:http-adaptive-streaming">2.4</a> describes the evolution of adaptive
streaming, using HTTP as a delivery protocol. The most popular
implementations of this technique are explained in detail in the following
subsections: Apple's HTTP Live Streaming in section <a href="#sec:apple-hls">2.4.2</a>,
Microsoft's Live Smooth Streaming in
section <a href="#sec:microsoft-smooth-streaming">2.4.3</a>, Adobe's HTTP Dynamic Streaming in
section <a href="#sec:adobe-hds">2.4.4</a>, and MPEG Dynamic Adaptive
Streaming over HTTP in section <a href="#sec:mpeg-dash">2.4.5</a>. Two
different types of services can be provided: video on-demand or live streaming
(section <a href="#sec:vod-live">2.9</a>).</p>

<p>The most relevant video and audio CODECs are described
in sections <a href="#sec:video-codecs">2.5</a> and <a href="#sec:audio-codecs">2.6</a> respectively,
whereas container formats are described in section <a href="#sec:container-formats">2.7</a>.
Android operating system capabilities are explained in
section <a href="#sec:android">2.10</a>, mainly focusing on the media framework and supported
CODECs.</p>

<p>Finally, a brief comparison of the different streaming approaches is presented
at the end of the chapter (section <a href="#sec:http-comparison">2.11</a>).</p>


<h2><a name="sec:traditional-streaming"></a><a name="2.1"></a>2.1 Traditional streaming</h2>

<p>Traditional streaming [<a href="#lee">48</a>, p. 113-117] requires a stateful protocol which
establishes a session between the service provider and client. In this
technique, media is sent as a series of packets. The Real-Time Transport Protocol
(RTP) together with the Real-Time Streaming Protocol (RTSP) are frequently used
to implement such service.</p>


<h3><a name="sec:rtp"></a><a name="2.1.1"></a>2.1.1 Real-Time Transport Protocol (RTP)</h3>

<p>The Real-Time Transport Protocol (RTP) [<a href="#rfc-3550">65</a>] describes a packetization
scheme for delivering video and audio streams over IP networks. It was developed
by the audio video transport working group of the IETF in 1996.</p>

<p>RTP is an end-to-end, real-time protocol for unicast or multicast network
services. Because RTP operates over UDP it is suitable for multicast
distribution, while all protocols that are built on top of TCP can only be
unicast. For this reason RTP is widely used for distributing media in the
case of Internet Protocol Television (IPTV), as the Internet service provider
can control the amount of multicast traffic that they allow in their network
and they gain quite a lot from the scaling which multicast offers. For a
streaming multimedia service RTP is usually used in conjunction with RTSP, with the audio
and video transmitted as separate RTP streams.</p>

<p>The RTP specification describes two sub-protocols which are the data transfer
protocol (RTP) and the RTP Control Protocol (RTCP) [<a href="#rfc-3550">65</a>, section 6]:</p>

<ol>
	<li>RTP is used for transferring multimedia data utilizing different
	CODECs along with time-stamps and sequence numbers. These
	time-stamps and sequence numbers allow the receiver to detect packet loss and
	perform reordering when necessary and synchronize media streams, among other
	operations.</li>
	
	<li>RTCP specifies the control information for synchronization and quality
	of service parameters that may be sent. This protocol should use a maximum 5%
	of the overall bandwidth.</li>
</ol>

<p>Optionally RTP can be used with a session description protocol or a signalling
protocol such as H.323, the Media Gateway Control Protocol (MEGACO), the Skinny
Call Control Protocol (SCCP), or the Session Initiation Protocol (SIP).</p>

<p>RTP neither provides a mechanism to ensure timely delivery nor guarantees
quality of service or in-order delivery. Additionally, there is no flow control
provided by the protocol itself, rather flow control and congestion avoidance
are up to the application to implement.</p>

<h3><a name="sec:rtsp"></a><a name="2.1.2"></a>2.1.2 Real-Time Streaming Protocol (RTSP)</h3>

<p>The Real-Time Streaming Protocol (RTSP) [<a href="#rfc-2326">66</a>] is a session control
protocol which provides an extensible framework to control delivery
of real-time data. It was developed by the multiparty multimedia session
control working group (MMUSIC) of the IETF in 1998. RTSP is useful for
establishing and controlling media sessions between end points, but it is not
responsible for the transmission of media data. Instead, RTSP relies on
RTP-based delivery mechanisms. In contrast with HTTP<a href="#footnote2" name="note2"><sup>2</sup></a>, RTSP is stateful and both client
and server can issue requests. These requests can be performed in three
different ways: (1) persistent connections used for several request/response
transactions, (2) one connection per request/response transaction or (3) no
connection.</p>

<p>Some popular RTSP implementations are Apple's QuickTime Streaming Server (QSS)
(also its open-sourced version, Apple's Darwin Streaming Server (DSS)) and
RealNetworks' Helix Universal Server.</p>

<h2><a name="sec:progressive-download"></a><a name="2.2"></a>2.2 Progressive download</h2>

<p>Progressive download is a technique to transfer data between server and client
which has become very popular and it is widely used on the Internet.
Progressive download typically can be realized using a regular HTTP
server. Users request multimedia content which is downloaded progressively into
a local buffer. As soon as there is sufficient data the media starts to play.
If the playback rate exceeds the download rate, then playback is delayed until
more data is downloaded.</p>

<p>Progressive download has some disadvantages: (1) wasteful of bandwidth if the
user decides to stop watching the video content, since data has been
transferred and buffered that will not be played, (2) no bit-rate adaptation,
since every client is considered equal in terms of available bandwidth and,
(3) no support for live media sources.</p>


<h2><a name="sec:adaptive-streaming"></a><a name="2.3"></a>2.3 Adaptive streaming</h2>

<p>Adaptive streaming [<a href="#lee">48</a>, p. 141-155] is a technique which detects the
user's available bandwidth and CPU capacity in order to adjust the quality of
the video that is provided to the user, so as to offer the best quality that
can be given to this user in their current circumstance. It requires an encoder
to provide video at multiple bit rates (or that multiple encoders be used) and
can be deployed within a CDN to provide improved scalability. As a result,
users experience streaming media delivery with the highest possible quality.</p>

<p>Techniques to adapt the video source's bit-rate to variable bandwidth can be
classified into three categories: transcoding (section <a href="#sec:transcoding">2.3.1</a>),
scalable encoding (section <a href="#sec:scalable-encoding">2.3.2</a>), and stream switching
(section <a href="#sec:stream-switching">2.3.3</a>).</p>


<h3><a name="sec:transcoding"></a><a name="2.3.1"></a>2.3.1 Transcoding</h3>

<p>By means of transcoding it is possible to convert raw video content
on the fly on the server's side. To match a specific bit-rate we
transcode from one encoding to another. A block diagram of this technique is depicted in
figure <a href="#fig:transcoding">2.1</a>. The main advantage of this approach is the fine
granularity that can be obtained, since streams can be transcoded to the user's
available bandwidth.</p>

<div class="figure">
	<a name="fig:transcoding"></a>
	<img src="img/transcoding.png" alt="Transcoding approach 
		for adaptive streaming."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.1</span> Transcoding approach 
		for adaptive streaming. Adapted from [<a href="#cicco">23</a>].
	</div>
</div>

<p>However, there are some serious disadvantages that are worth pointing out. First
of all, the high cost of transcoding, which requires adapting the raw video
content several times for several requests for different quality. As a result
scalability decreases since transcoding needs to be performed for every
different client available bandwidth. Due to the computational requirements of
a real-time transcoding system, the encoding process is required to be
performed in appropriate servers, in order to be deployed in CDNs.</p>


<h3><a name="sec:scalable-encoding"></a><a name="2.3.2"></a>2.3.2 Scalable encoding</h3>

<p>Using a scalable CODEC standard such as H.264/MPEG-4 AVC (described in detail in
section <a href="#sec:h264">2.5.4</a>), the picture resolution and the frame rate can be
adapted without having to re encode the raw video content [<a href="#h264">42</a>].
This approach tends to reduce processing load, but it is clearly limited to a
set of scalable CODEC formats. A block diagram of this technique is depicted in
figure <a href="#fig:scalable-encoding">2.2</a>.</p>

<div class="figure">
	<a name="fig:scalable-encoding"></a>
	<img src="img/scalable-encoding.png" alt="Scalable encoding 
		approach for adaptive streaming."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.2</span> Scalable encoding 
		approach for adaptive streaming. Adapted from [<a href="#cicco">23</a>].
	</div>
</div>

<p>Nevertheless, deployment into CDNs is complicated in this approach because
specialized servers are required to implement the adaptation logic [<a href="#cicco">23</a>].</p>


<h3><a name="sec:stream-switching"></a><a name="2.3.3"></a>2.3.3 Stream switching</h3>

<p>The stream switching approach encodes the raw video content at several different
increasing bit-rates, generating <em>R</em> versions of the same content, known as
video levels. As shown in figure <a href="#fig:stream-switching">2.3</a>, an algorithm must
dynamically choose the video level which matches the user's available
bandwidth. When changes in the available bandwidth occur, the algorithm simply
switches to different levels to ensure continuous playback.</p>

<div class="figure">
	<a name="fig:stream-switching"></a>
	<img src="img/stream-switching.png" alt="Stream switching 
		approach for adaptive streaming."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.3</span> Stream switching 
		approach for adaptive streaming. Adapted from [<a href="#cicco">23</a>].
	</div>
</div>

<p>The main purpose of this method is to minimize processing costs, since no
further processing is needed once all video levels are generated. In addition,
this approach does not require a specific CODEC format to be implemented, that
is, it is completely CODEC agnostic. In contrast, storage and transmission
requirements must be considered because the same video content is encoded
<em>R</em> times (but at different bit-rates). Note that the quality levels are not
incremental, therefore only one substream has to be requested. The only
disadvantage of this approach is the coarse granularity since there is only a
discrete set of levels. Additionally, if there are no clients for a given rate
there is no need to generate this level; however, this only costs storage space
at the server(s) and not all servers need to store all levels of a stream.</p>

<p>Figure <a href="#fig:stream-switching-time">2.4</a> illustrates the stream switching approach
over time, assuming that all segments have the same duration and the switching
operations are performed after each segment has been played (not partially).
Segments at different video qualities are requested to be played in a sequence.
The number of levels and the duration of the segments are flexible and become
part of the system's design choices.</p>

<div class="figure">
	<a name="fig:stream-switching-time"></a>
	<img src="img/stream-switching-time.png" alt="Stream switching example over time."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.4</span> Stream switching example over time.
	</div>
</div>


<h2><a name="sec:http-adaptive-streaming"></a><a name="2.4"></a>2.4 HTTP-based adaptive streaming</h2>

<p>Recently a new solution for adaptive streaming has been designed, based on the
stream switching technique (explained in section <a href="#sec:stream-switching">2.3.3</a>). It
is an hybrid method which uses HTTP as a delivery protocol instead of defining
a new protocol.</p>

<p>Video and audio sources are cut into short segments of the same
length (typically several seconds). Optionally, segments can be cut along a
video Group of Pictures (explained in section <a href="#sec:frames">2.5.1</a>), thus every
segment starts with a key frame, meaning that segments do not have past/future
dependencies among them. Finally, all segments are encoded in the desired format
and hosted on a HTTP server.</p>

<p>Clients request segments sequentially and download them using HTTP
progressive download. Segments are played in order and since they are
contiguous, the resulting overall playback is smooth. All adaptation logic is
controlled by the client. This means that the client calculates the fetching
time of each segment in order to switch-up or switch-down the bit-rate. A basic
example is depicted in figure <a href="#fig:http-sequence">2.5</a>, where the <em>feedback controller</em>
represents the switching logic applied on the client side. Thicker arrows correspond to
transmission of an actual data segment.</p>

<div class="figure">
	<a name="fig:http-sequence"></a>
	<img src="img/http-sequence-graph.png" alt="Client contract adaptation. Network delays are omitted for
  simplicity."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.5</span> Client contract adaptation. Network delays are omitted for
  simplicity.
	</div>
</div>

<h3><a name="2.4.1"></a>2.4.1 Why HTTP?</h3>

<p>HTTP is widely used in the Internet as a delivery protocol. Because HTTP is so
widely used HTTP-based services avoid NAT and firewall issues. Because (1) the
client initiated the TCP connection from behind the firewall or Network Address
Translation (NAT) or (2) because holes for HTTP have been purposely opened
through the firewall or NAT service. The NAT or firewall will allow the packets
from the HTTP server to be delivered to the client over a TCP connection or
SCTP association (for the rest of this thesis we will assume that TCP is used
as the transport protocol for HTTP). Additionally because HTTP uses TCP it
automatically gets in order reliable byte stream delivery and TCP provides
extensive congestion avoidance mechanisms. HTTP-based services can use the
existing HTTP servers and CDN infrastructures.</p>

<p>Finally, the <em>streaming</em> session is controlled entirely by the client, thus
there is no need for negotiation with the HTTP server, as clients simply open
TCP connections and choose an initial content bit-rate. Then clients switch
among the offered streams depending on their available bandwidth.</p>

<h3><a name="sec:apple-hls"></a><a name="2.4.2"></a>2.4.2 Apple's HTTP Live Streaming (HLS)</h3>

<p>In May 2009 Apple released a HTTP-based streaming media communication protocol
(Apple-HLS) [<a href="#hls-practices">10</a>, <a href="#hls-overview">11</a>, <a href="#andrew">29</a>, <a href="#mcdonald">52</a>, <a href="#pantos">59</a>] to
transmit bounded and unbounded streams of multimedia data. Apple-HLS is based on the
Emblaze Network Media Streaming technology which was released in 1998. According to
this specification, an overall stream is broken into a sequence of small
HTTP-based file downloads, where users can select alternate streams encoded at
different data rates. Because the HTTP clients request the files for
downloading this method works through firewalls and proxy servers (unlike
UDP-based protocols such as RTP which require ports to be opened in the
firewall or require use of an application layer gateway).</p>

<p>Initially, users download an extended M3U playlist which contains several
Uniform Resource Identifiers (URIs) [<a href="#rfc-2396">14</a>] corresponding to media
files, where each file must be a continuation of the encoded stream (unless it
is the first one or there is a discontinuity tag which means that the overall
stream is unbounded). Each individual media file must be formatted as an MPEG-2
transport stream [<a href="#m2ts">43</a>] or a MPEG-2 audio elementary stream.</p>

<p>Listing <a href="#lst:simple-playlist">2.1</a> illustrates a simple example of an extended
M3U playlist where the entire stream consists of three
10-seconds-long media files. Listing <a href="#lst:alternate-playlist">2.1</a> provides a
more complicated example, where there are different available bandwidths and
each entry points to an extended M3U sub-playlist file (depicted in
figure <a href="#fig:stream-alternates">2.6</a>).</p>

<div class="listing">
	<a name="lst:simple-playlist"></a>
	<div class="caption">
		<span class="captionTitle">Listing 2.1</span> Example of an extended M3U playlist which contains three
	10-seconds-long media segments.
	</div>
<pre>
#EXTM3U
#EXT-X-MEDIA-SEQUENCE:0
#EXT-X-TARGETDURATION:10
#EXTINF:10,
http://www.example.com/segment1.ts
#EXTINF:10,
http://www.example.com/segment2.ts
#EXTINF:10,
http://www.example.com/segment3.ts
#EXT-X-ENDLIST
</pre>
</div>

<div class="listing">
	<a name="lst:alternate-playlist"></a>
	<div class="caption">
		<span class="captionTitle">Listing 2.2</span> Example of an extended M3U playlist which contains several
sub-playlists, consequently providing alternate stream at different
qualities.
	</div>
<pre>
#EXTM3U
#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=1280000
http://www.example.com/low.m3u8
#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=2560000
http://www.example.com/mid.m3u8
#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=7680000
http://www.example.com/hi.m3u8
#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=65000,CODECS="mp4a.40.5"
http://www.example.com/audio-only.m3u8
</pre>
</div>

<p>The overall process performed in an Apple-HLS architecture is shown in
figure <a href="#fig:hls">2.7</a>. From the server's side, the
protocol operates as follows: (1) the media content is encoded at different bit-rates to produce
streams which present the same content and duration (but with different
quality), (2) each stream is divided into individual files (segments) with
approximately equal duration, (3) a playlist file is created which contains an
URI for each media file indicating its duration (the playlist can be accesed
through an URL), and (4) further changes to the playlist file must be performed
atomically.</p>

<p>From the client's side, the following actions take place: (1) selection of the
media file which shall be played must be made and (2) periodically reload the
playlist file (unless it is bounded). It is necessary to wait a period of time
before attempting to reload the playlist. The initial amount of time to wait
before re-loading the playlist is set as the duration of the last media file in
the playlist. If the client reloads the playlist file and the playlist has not
changed, then the client waits a period of time proportional to the duration of
the segments before retrying: 0.5 times the duration for the first attempt,
1.5 times the duration for the second and 3.0 times the duration in further
attempts.</p>

<div class="figure">
	<a name="fig:stream-alternates"></a>
	<img src="img/stream-alternates.png" alt="Alternate index files to offer different streams."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.6</span> Alternate index files to offer different streams. Adapted from [<a href="#hls-overview">11</a>].
	</div>
</div>

<div class="figure">
	<a name="fig:hls"></a>
	<img src="img/hls.png" alt="HTTP Live streaming architecture."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.7</span> HTTP Live streaming architecture. Adapted from [<a href="#hls-overview">11</a>].
	</div>
</div>


<h3><a name="sec:microsoft-smooth-streaming"></a><a name="2.4.3"></a>2.4.3 Microsoft's Live Smooth Streaming (LSS)</h3>

<p>In 2009, Microsoft Corporation released its approach [<a href="#iss-smooth">53</a>, <a href="#smooth">55</a>, <a href="#zambelli">74</a>] for adaptive streaming over HTTP.
Microsoft's Live Smooth Streaming (LSS<a href="#footnote3" name="note3"><sup>3</sup></a>) format specification is based
on the ISO Base Media File Format and standardized as the Protected
Interoperable File Format (PIFF) [<a href="#piff">19</a>], whereas the manifest file is
based on the Extensible Markup Language (XML) [<a href="#bray">18</a>] (a simplified example
is shown in listing <a href="#lst:smooth-streaming-manifest">2.3</a>).</p>

<p>Microsoft provides a Smooth Streaming demo<a href="#footnote4" name="note4"><sup>4</sup></a> which requires the
Silverlight plug-in [<a href="#silverlight">54</a>] to be installed. In this online
application, the available bandwidth can be easily adjusted within a very
simple user interface. A network usage graph is dynamically displayed as well
as the adapted video output.</p>

<div class="listing">
	<a name="lst:smooth-streaming-manifest"></a>
	<div class="caption">
		<span class="captionTitle">Listing 2.3</span> Microsoft-LSS manifest sample.
	</div>
<pre>
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;SmoothStreamingMedia MajorVersion="2" MinorVersion="0" Duration="2300000000"
 TimeScale="10000000">
   &lt;Protection>
      &lt;ProtectionHeader SystemID="{9A04F079-9840-4286-AB92E65BE0885F95}">
         &lt;!-- Base 64 Encoded data omitted for clarity -->
      &lt;/ProtectionHeader>
   &lt;/Protection>
   &lt;StreamIndex Type = "video" Chunks = "115" QualityLevels = "2" MaxWidth = "720" 
    MaxHeight = "480" TimeScale="10000000" Name="video" Url ="QualityLevels({bitrate},
    {CustomAttributes})/Fragments(video={start_time})">
      &lt;QualityLevel Index="0" Bitrate="1536000" FourCC="WVC1"
       MaxWidth="720" MaxHeight="480" CodecPrivateData = "...">
         &lt;CustomAttributes>
            &lt;Attribute Name="Compatibility" Value="Desktop" />
         &lt;/CustomAttributes>
      &lt;/QualityLevel>
      &lt;QualityLevel Index="5" Bitrate="307200" FourCC="WVC1"
       MaxWidth="720" MaxHeight="480" CodecPrivateData="...">
         &lt;CustomAttributes>
            &lt;Attribute Name="Compatibility" Value="Handheld" />
         &lt;/CustomAttributes>
      &lt;/QualityLevel>
      &lt;c t ="0" d="19680000" />
      &lt;c n ="1" t="19680000" d="8980000" />
   &lt;/StreamIndex>
&lt;/SmoothStreamingMedia>
</pre>
</div>


<h3><a name="sec:adobe-hds"></a><a name="2.4.4"></a>2.4.4 Adobe's HTTP Dynamic Streaming</h3>

<p>Adobe's HTTP dynamic streaming (HDS) approach enables on-demand and live
streaming and it supports HTTP and Real Time Messaging Protocol
(RTMP) [<a href="#rtmp">4</a>]. It uses different format specifications for media files
(Flash Video or F4V, based on the standard MPEG-4 Part 12) and manifests (Flash
Media Manifest or F4M). In order to deploy Adobe's solution it is necessary to
set up a Flash Media Streaming Server [<a href="#hassoun">37</a>] which is a proprietary and
commercial product. Additionally, users need to install Adobe's Flash Player.</p>


<h3><a name="sec:mpeg-dash"></a><a name="2.4.5"></a>2.4.5 MPEG Dynamic Adaptive Streaming over HTTP (MPEG-DASH)</h3>

<p>MPEG Dynamic Adaptive Streaming over HTTP (MPEG-DASH) is a protocol presented by
a joint working group [<a href="#stockhammer">69</a>] of Third Generation Partnership
Project (3GPP) and MPEG. This protocol has recently been considered to become
an ISO standard [<a href="#ts26234">1</a>, <a href="#ts26244">2</a>]. MPEG-DASH defines a structure similar to
Microsoft-LSS for adaptive streaming supporting on-demand, live, and
time-shifting<a href="#footnote5" name="note5"><sup>5</sup></a> viewing, but it proposes changes in the file formats, defining a XML-based
<strong>manifest</strong> file.</p>

<p>MPEG-DASH introduced the concept of <em>media presentation</em>. A media
presentation is a collection of structured video/audio content:</p>

<ul>
	<li>A media presentation consists of a sequence of one or more
	<em>periods</em> which are consecutive and do not overlap.</li>

	<li>Each period consists of one or more <em>representations</em> of the
	same media content. Periods have an assigned start time which is
	relative to start of the media presentation.</li>

	<li>Each representation<a href="#footnote6" name="note6"><sup>6</sup></a> specifies a video
	quality <em>profile</em> consisting of several parameters such as bandwidth,
	encoding, and resolution. Representations contain one or more segments,
	represented by Universal Resource Locators (URLs).</li>
	
	<li>Segments contain fragments of the actual video content.</li>
</ul>

<p>A Media Presentation Description (MPD) schema is an XML-based file which
contains the whole structure of a media presentation introduced above. A
simplified version is depicted in figure <a href="#fig:mpd-structure">2.8</a>, and
listing <a href="#lst:mpd-structure">2.4</a> provides a concrete example.</p>

<div class="figure">
	<a name="fig:mpd-structure"></a>
	<img src="img/mpd-structure.png" alt="MPD simplified structure."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.8</span> MPD simplified structure. Adapted from [<a href="#stockhammer">69</a>, figure 4].
	</div>
</div>

<div class="listing">
	<a name="lst:mpd-structure"></a>
	<div class="caption">
		<span class="captionTitle">Listing 2.4</span> MPD example. Optional elements and attributes are omitted for
simplicity.
	</div>
<pre>
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;MPD minBufferTime="PT10S">
   &lt;Period start="PT0S">
      &lt;Representation mimeType="video/3gpp; codecs=263, samr" 
       bandwidth="256000" id="256">
         &lt;SegmentInfo duration="PT10S" baseURL="rep1/">
            &lt;InitialisationSegmentURL sourceURL="seg-init.3gp"/>
            &lt;Url sourceURL="seg-1.3gp"/>
            &lt;Url sourceURL="seg-2.3gp"/>
            &lt;Url sourceURL="seg-3.3gp"/>
         &lt;/SegmentInfo>
      &lt;/Representation>
      &lt;Representation mimeType="video/3gpp; codecs=mp4v.20.9" 
       bandwidth="128000" id="128">
         &lt;SegmentInfo duration="PT10S" baseURL="rep2/">
            &lt;InitialisationSegmentURL sourceURL="seg-init.3gp"/>
            &lt;Url sourceURL="seg-1.3gp"/>
            &lt;Url sourceURL="seg-2.3gp"/>
            &lt;Url sourceURL="seg-3.3gp"/>
         &lt;/SegmentInfo>
      &lt;/Representation>
   &lt;/Period>
&lt;/MPD>
</pre>
</div>

<p>The MPEG-DASH protocol specifies the syntax and semantics of the MPD, the
format of segments, and the delivery protocol (HTTP). Fortunately, it permits
flexible configurations to implement different types of streaming services. The
following parameters can be selected flexibly: (1) the size and duration of the
segments (these can be selected individually for each representation), (2)
the number of representations and (3) the profile of each representation
(bit-rate, CODECs, container format, etc).</p>

<p>Regarding the client's behaviour, it can flexibly: (1) decide when and how to
download segments, (2) select appropriate representation, (3) switch
representations and, (4) select the transport of the MPD file, which could also
be retrieved by other means, rather than only through HTTP.</p>

<p>Figure <a href="#fig:mpd-sequence">2.9</a> exemplifies the
communication between server and client in a MPEG DASH streaming service.
First the client retrieves the MPD file and afterwards it sequentially requests
the media segments. In every period a representation level is selected, based on the fetching times and
other parameters determined by the client.</p>

<div class="figure">
	<a name="fig:mpd-sequence"></a>
	<img src="img/mpeg-dash-sequence.png" alt="Client contract adaptation example in MPEG-DASH. Network delays are
  omitted for simplicity."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.9</span> Client contract adaptation example in MPEG-DASH. Network delays are
  omitted for simplicity.
	</div>
</div>


<h2><a name="sec:video-codecs"></a><a name="2.5"></a>2.5 Video CODECs</h2>

<p>This section describes a number of aspects of video coders and decoders that are
relevant to a reader of this thesis.</p>


<h3><a name="sec:frames"></a><a name="2.5.1"></a>2.5.1 Video frames</h3>

<p>Compressed video standards only encode full frame data for certain frames,
known as <em>key frames</em>, <em>intra-frames</em> or simply
<em>I frames</em>. The frames which follow a key frame, <em>predicted</em>
frames or <em>P frames</em>, are encoded considering only the differences
with the preceding frame, resulting in less data being needed to encode these
subsequent frames. Videos whose frame information changes rapidly require more
key frames than a slowly changing visual scene. An example of the relationship
between several frames is shown in figure <a href="#fig:video-frames">2.10</a>.</p>

<div class="figure">
	<a name="fig:video-frames"></a>
	<img src="img/frames.png" alt="Distribution of frames in a video stream."/>
	<div class="caption">
		<span class="captionTitle">Figure 2.10</span> Distribution of frames in a video stream. Every group of pictures is
  constituted by one I frame and several P frames, and optionally
  B frames.
	</div>
</div>

<p>Bidirectional encoding is also possible by means of Bi predictive frames
(<em>B frames</em>). B frames consider both previous and subsequent
frame differences to achieve better compression.</p>

<p>A Group of Pictures (GOP) consists of one I frame followed by several
P frames and optionally, B frames. Lowering the GOP size (key
frame interval) can provide benefit: using more frequent key frames helps to
reduce distortion when streaming in a lossy environment. However, a low GOP size increases the media file size since key
frames contain more bits than predictive frames.</p>


<h3><a name="sec:dts-pts"></a><a name="2.5.2"></a>2.5.2 Decoding and presentation time-stamps</h3>

<p>Decoding Time-stamp (DTS) is used to synchronize streams and control
the rate at which frames are decoded. It is not essential to include a DTS in
all frames, since it can be interpolated by the decoder. In contrast, the
Presentation Time-stamp (PTS) indicates the exact moment when a video frame
has to be presented at the decoder's output. PTS and DTS only differ when
bidirectional coding is used (i.e., when <em>B-frames</em> are used).</p>


<h3><a name="sec:h263"></a><a name="2.5.3"></a>2.5.3 H.263</h3>

<p>H.263 [<a href="#h263">44</a>] is a low-bit-rate video compression standard designed for
videoconferencing, although is widely used in many other applications. It was
developed by the ITU-T Video Coding Experts Group (VCEG) in 1996. H.263 has
been supported in Flash video applications and widely used by Internet on-demand
services such as YouTube or Vimeo.</p>

<p>H.263 bit-rates range from 24 kb/s to 64 kb/s. Video can be encoded and decoded
to this format with the free LGPL-licensed <span class="code">libavcodec</span> library (part of
the FFmpeg project [<a href="#bellard">13</a>]).</p>


<h3><a name="sec:h264"></a><a name="2.5.4"></a>2.5.4 H.264/MPEG-4 AVC</h3>

<p>H.264/MPEG-4 Part 10 [<a href="#h264">42</a>] or Advanced Video Coding (AVC) is the successor
of H.263 and other standards such as MPEG-2 and MPEG-4 Part 2. H.264 is one of
the most commonly used formats for recording, compression, and distribution of
high definition video. H.264 is one of the CODECs supported for Blu ray
discs. H.264 was developed by the ITU T Video Coding Experts Group
together with ISO/IEC MPEG in 2003. It is supported in Adobe's Flash Player and
Microsoft's Silverlight. Therefore, multiple streaming Internet sources such as
Vimeo, YouTube, and the Apple iTunes Store follow the H.264 standard.</p>

<p>H.264 specifies seventeen <em>profiles</em> which are oriented to multiple types
of applications. The Constrained Baseline Profile (CBP) is the most basic
one, followed by the Baseline Profile (BP) and the Main Profile (MP) in
increasing order of complexity. CBP and BP are broadly used in mobile
applications and videoconferencing. Additionally, these are the only H.264
profiles supported by Android's native media framework.
Table <a href="#tab:cbp-bp">2.1</a> summarizes the major differences among these three
profiles.</p>

<div class="table">
	<a name="tab:cbp-bp"></a>
	<div class="caption">
		<span class="captionTitle">Table 2.1</span> Major differences among H.264 Constrained Baseline Profile (CBP),
  Baseline Profile (BP) and Main Profile (MP).
	</div>
	
	<table>
		<tr class="header">
			<td>Feature</td>
			<td>CBP</td>
			<td>BP</td>
			<td>MP</td>
		</tr>
		<tr class="even">
			<td>Android support</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>No</td>
		</tr>
		<tr class="odd">
			<td>Flexible macro-block ordering (FMO)</td>
			<td>No</td>
			<td>Yes</td>
			<td>No</td>
		</tr>
		<tr class="even">
			<td>Arbitrary slice ordering (ASO)</td>
			<td>No</td>
			<td>Yes</td>
			<td>No</td>
		</tr>
		<tr class="odd">
			<td>Redundant slices (RS)</td>
			<td>No</td>
			<td>Yes</td>
			<td>No</td>
		</tr>
		<tr class="even">
			<td>B-frames</td>
			<td>No</td>
			<td>No</td>
			<td>Yes</td>
		</tr>
		<tr class="odd">
			<td>CABAC entropy coding</td>
			<td>No</td>
			<td>No</td>
			<td>Yes</td>
		</tr>
	</table>
</div>

<p>One of the most recent features added to the H.264 standard is Scalable
Video Coding (SVC) [<a href="#h264">42</a>, Annex G]. SVC enables the construction of
bitstreams which contain sub bitstreams, all conforming the standard. In
addition, Multiview Video Coding (MVC) [<a href="#h264">42</a>, Annex H] offers an even more
complex composition of bitstreams, allowing more than one view point for a
video scene<a href="#footnote7" name="note7"><sup>7</sup></a>.</p>


<h3><a name="sec:vp8"></a><a name="2.5.5"></a>2.5.5 VP8</h3>

<p>VP8 is a video compression format originally created by On2, but eventually
released by Google in 2010 after they purchased On2. VP8 was
published with a BSD license, therefore it is considered to be an open
alternative to H.264.</p>

<p>VP8 encoding and decoding can be performed by the <span class="code">libvpx</span> library
[<a href="#webm">70</a>]. Moreover, the FFmpeg team released a <span class="code">ffvp8</span> decoder on
July, 2010.</p>


<h2><a name="sec:audio-codecs"></a><a name="2.6"></a>2.6 Audio CODECs</h2>

<p>This section describes a number of aspects of audio coders and decoders that are
relevant to a reader of this thesis.</p>


<h3><a name="sec:mp3"></a><a name="2.6.1"></a>2.6.1 MP3</h3>
  
<p>MP3 [<a href="#brandenburg">17</a>, <a href="#hacker">36</a>, <a href="#mp3">39</a>] (published as MPEG-1 and MPEG-2 Audio
Layer III) has undoubtedly become in the last decade the <em>de facto</em> audio
CODEC due to its use in multiple media services and digital audio players. MP3
is a patented digital audio encoding format which reduces the amount of data
required since it discards the less audible components to human hearing, i.e.,
it implements a lossy compression algorithm.</p>
  

<h3><a name="sec:aac"></a> <a name="2.6.2"></a>2.6.2 Advanced Audio Coding (AAC)</h3>
 
<p>Advanced Audio Coding (AAC) [<a href="#brandenburg">17</a>] is an ISO/IEC standardized audio
compression format which provides lossy compression encoding. It is supported
in a extensive variety of devices. AAC is part of the MPEG-2 [<a href="#mpeg2-aac">40</a>]
and MPEG-4 [<a href="#mpeg4-aac">41</a>] specifications. AAC was designed to be the successor
of the MP3 format. A later extension defines the High-Efficiency Advanced
Audio Coding (HE AAC).</p>
  
<p>Three default profiles are defined [<a href="#mpeg2-aac">40</a>]: Low Complexity (LC), Main
Profile (MP), and Scalable Sample Rate (SSR). In conjunction with the Perceptual
Noise Substitution and 45 Audio Object Types [<a href="#mpeg4-aac">41</a>], new profiles are
defined, such as the High Efficiency AAC Profile (HE AAC and
HE AAC v2) and the Scalable Audio Profile. The latter utilizes Long Term
Prediction (LTP).</p>
  
  
<h3><a name="sec:vorbis"></a> <a name="2.6.3"></a>2.6.3 Vorbis</h3>

<p>Vorbis [<a href="#xiph">72</a>] is a free and open audio CODEC meant to replace patented and
restricted formats such as MP3 (section <a href="#sec:mp3">2.6.1</a>). Vorbis provides a lossy
compression encoding over a wide range of bit-rates. It has been shown to
perform similar to MP3 [<a href="#carlacci">22</a>].</p>


<h2><a name="sec:container-formats"></a><a name="2.7"></a>2.7 Container formats</h2>

<p>A container is a meta-format which wraps any kind of media data, resulting in a
single file. Containers are used to interleave different data types, for
instance video streams, subtitles, and even meta-data information. A vast
variety of container formats has been developed, presenting different features
and limitations. The most important multimedia containers are briefly
introduced below.</p>

<ul>  
  <li><strong>MP4</strong> (<span class="code">.mp4</span>) is a popular container format defined
  in the MPEG-4 Part 14 standard. It supports almost any kind of media data.
  Typically an MP4 container contains video and audio streams encoded with H.264
  and AAC, respectively.</li>
  
  <li><strong>3GP</strong> (<span class="code">.3gp</span>) [<a href="#ts26244">2</a>] is widely utilized on 3G mobile
  phones. 3GP is defined as an extension of MPEG-4 Part 12. 3GP
  normally stores video streams encoded with either MPEG-4 Part 2, H.263, or H.264 and
  audio streams with either AAC (LC profile) or HE-AAC.</li>
  
  <li><strong>MPEG Transport Stream</strong> (<span class="code">.ts</span>) is defined
  in the MPEG-2 Part 1 standard and generally used in digital television
  broadcast systems.</li>
  
  <li><strong>Ogg</strong> (<span class="code">.ogg</span>) is an open container format
  developed by the Xiph.Org Foundation. Ogg generally combines the
  Theora and Vorbis CODECs.</li>
  
  <li><strong>WebM</strong> [<a href="#webm">70</a>] (<span class="code">.webm</span>) is a recently released open,
  royalty-free container format based on the Matroska container. WebM has gained
  noteworthy popularity, since it has been adopted as one of the most suitable formats for the Web
  content (although HTML 5, the predominant markup language for web pages, is
  defined as being CODEC-agnostic [<a href="#html5">38</a>, section 4.8.6]) due to its
  patent-free and open nature. Consequently, only open-source CODECs are
  recommended: video streams are encoded with VP8 and audio streams with Vorbis
  (these introduced in section <a href="#sec:vp8">2.5.5</a> and section <a href="#sec:vorbis">2.6.3</a>,
  respectively).</li>
</ul>

<p>Table <a href="#tab:container-codec">2.2</a> provides a comparison between the container
formats explained above, in terms of supported audio and video CODECs.</p>

<div class="table">
	<a name="tab:container-codec"></a>
	<div class="caption">
		<span class="captionTitle">Table 2.2</span> Video and audio CODECs supported in several container formats (2011,
  August). Information collected from [<a href="#ts26244">2</a>, <a href="#m2ts">43</a>, <a href="#webm">70</a>, <a href="#xiph">72</a>]. 
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Format</td>
			<td>H.263</td>
			<td>H.264</td>
			<td>MPEG-4</td>
			<td>VP8</td>
			<td>MP3</td>
			<td>AAC</td>
			<td>HE-AAC</td>
			<td>Vorbis</td>
		</tr>
		<tr class="even">
			<td>3GP</td>
			<td>Yes</td> 		
			<td>Yes</td> 		
			<td>Yes</td>
			<td>No</td>
			<td>No</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>No</td>
		</tr>
		<tr class="odd">
			<td>MP4</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>No</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>Yes</td>
		</tr>
		<tr class="even">
			<td>MPEG-TS</td>
			<td>No</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>No</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>No</td>
		</tr>
		<tr class="odd">
			<td>Ogg</td>
			<td>No</td>
			<td>No</td>
			<td>No</td>
			<td>No</td>	
			<td>No</td>	
			<td>No</td>	
			<td>No</td>		
			<td>Yes</td>
		</tr>
		<tr class="even">
			<td>Webm</td>
			<td>No</td>
			<td>No</td>
			<td>No</td>
			<td>Yes</td>
			<td>No</td>
			<td>No</td>
			<td>No</td>
			<td>Yes</td>
		</tr>
	</table>
</div>


<h2><a name="sec:quality-levels"></a><a name="2.8"></a>2.8 Quality video levels</h2>

<p>Video and audio content can be offered at multiple representations
(<em>quality levels</em>) to adequate to different types of end-users. It is a
well-known fact that end-users are affected by a wide variety of restrictions in
terms of network capabilities, screen resolutions, and media formats
supported, among other limitations. The more representations provided on the
server's side, the better <em>granularity</em> characterizes the system, since a
wider variety of alternative versions of media content is served. Nevertheless,
the creation of multiple quality levels incurs a higher cost in terms of
processing time, storage requirements, and CPU consumption. The following
encoding parameters are especially relevant when defining a representation
level:</p>

<ul>
	<li><strong>Video bit-rate (kb/s)</strong> rate of information in the video stream.</li> 
	
	<li><strong>Frame rate (fps)</strong> frequency of <em>frame</em> presentation,
	measured as frames per second.</li>
	
	<li><strong>Audio bit-rate (kb/s)</strong>  rate of information in the audio stream.</li> 
	
	<li><strong>Audio channels</strong>  stereo (2) or mono (1).</li> 
	
	<li><strong>Sampling rate (Hz)</strong>  number of samples per second taken from a signal.</li> 
	
	<li><strong>GOP size</strong>  number of frames which follow a key-frame.</li> 
	
	<li><strong>Resolution (pixels)</strong>  size of a video image (<em>frame</em>).</li> 
</ul>

<p>Except for the GOP size, increasing any of these parameters leads to a
higher quality audio or video output, consequently incurring a larger file
size (requiring more bits to be transmitted).</p>


<h2><a name="sec:vod-live"></a><a name="2.9"></a>2.9 Video on-demand and live streaming</h2>

<p>There are two different ways to use streaming techniques. In the first one,
video on-demand, users request media files which have been previously recorded
and compressed and are stored on a server. Today this technique has become
very popular, with YouTube being the most popular website offering on-demand
streaming. The alternative is live streaming which enables an unbounded
transmission where media is generated, compressed, and delivered on the
fly. In the case of live streaming there may or not may be a concurrent
recording (which could be transmitted later on-demand).</p>

<p>Both streaming techniques may offer the user basic video control functions such
as pause, stop, and rewind. Additionally, for on-demand streaming there may be the
possibility of issuing a fast-forward command. Note that fast forward is only
possible when the media files are stored, thus the future content is
known. Of course it is also possible for the system to implement the possibility
of a fast-forward command if the user has paused the playback, but this will be
limited to moving forward to the recently generated portion of the content.</p>


<h2><a name="sec:android"></a><a name="2.10"></a>2.10 Google's Android operating system</h2>

<p>Android is an operating system specially designed for mobile devices. It is
mainly developed and supported by Google Inc., although other members of the
Open Handset Alliance (OHA) have collaborated in its development and release.
Table <a href="#tab:android-version">2.3</a> reviews Android's version history.</p>

<div class="table">
	<a name="tab:android-version"></a>
	<div class="caption">
		<span class="captionTitle">Table 2.3</span> Google's Android version history. 
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Version</td>
			<td>Codename</td>
			<td>Release date</td>
			<td>Linux kernel version</td>
		</tr>
		<tr class="even">
			<td>1.0</td>
			<td>None</td>
			<td>23 September 2008</td>
			<td>Unknown</td>
		</tr>
		<tr>
			<td>1.1</td>
			<td>None</td>
			<td>9 February 2009</td>
			<td>Unknown</td>
		</tr>
		<tr>
			<td>1.5</td>
			<td>Cupcake</td>
			<td>30 April 2009</td>
			<td>2.6.27</td>
			</tr>
		<tr>
			<td>1.6</td>
			<td>Donut</td>
			<td>15 September 2009</td>
			<td>2.6.29</td>
		</tr>
		<tr>
			<td>2.0/2.1</td>
			<td>Eclair</td>
			<td>26 October 2009</td>
			<td>2.6.29</td>
		</tr>
		<tr>
			<td>2.2</td>
			<td>Froyo</td>
			<td>20 May 2010</td>
			<td>2.6.32</td>
		</tr>
		<tr>
			<td>2.3</td>
			<td>Gingerbread</td>
			<td>6 December 2010</td>
			<td>2.6.35</td>
		</tr>
		<tr>
			<td>2.4</td>
			<td>Ice Cream Sandwich</td>
			<td>Not released</td>
			<td>Unknown</td>
		</tr>
		<tr>
			<td>3.0</td>
			<td>Honeycomb</td>
			<td>22 February 2011</td>
			<td>2.6.36</td>
		</tr>
		<tr>
			<td>3.2</td>
			<td>Honeycomb</td>
			<td>15 July 2011</td>
			<td>2.6.36</td>
		</tr>
   </table>
</div>

<p>Android is based on a modified version of the Linux kernel and its applications
are normally developed in the Java programming language<a href="#footnote8" name="note8"><sup>8</sup></a>. However, Android has not adopted the
official Java Virtual Machine (JVM), meaning that Java Byte code can not be
directly executed. Instead, applications run on the Dalvik Virtual Machine
(DVM), a JVM-based virtual machine specifically designed for Android. DVM is
optimized for mobile devices, which generally have CPU performance and memory
limitations. In addition, DVM makes more efficient use of battery power.</p>

<p>Applications are usually released via the Android Market, Google's
official online store. Nevertheless, publication of the applications is not
restricted, allowing installation from any other source.
Figure <a href="#fig:android-distribution">2.11</a> shows the current distribution of Android
versions based on the operating system of the devices that have recently
accessed the Android Market. As shown, Android's newer versions (the 3.x branch)
are only slowly being adopted, for example <em>Honeycomb</em> still represents
less than 1% of the overall of Android devices, while <em>Froyo</em> the
predominate version (running on almost 60% of the devices that access the
Android Market).</p>

<div class="figure">
	<a name="fig:android-distribution"></a>
	<img src="img/android-distribution.png" alt="Distribution of Android platform versions (as of July 2011)"/>
	<div class="caption">
		<span class="captionTitle">Figure 2.11</span> Distribution of Android platform versions (as of July 2011).
  Extracted from [<a href="#android-distribution">7</a>].
	</div>
</div>

<h3><a name="2.10.1"></a>2.10.1 Media formats supported on Android</h3>

<p>Android supports several multimedia formats and CODECs [<a href="#every">28</a>, p. 195-250],
including H.263 and H.264. Table <a href="#tab:android-video-formats">2.4</a> and
Table <a href="#tab:android-audio-formats">2.5</a> summarize respectively the video and
audio CODECs and container formats that are supported. For media playback, only
the decoding capabilities are relevant (encoding is typically used for recording
purposes).</p>

<div class="table">
	<a name="tab:android-video-formats"></a>
	<div class="caption">
		<span class="captionTitle">Table 2.4</span> Android supported video CODECs and container formats. Extracted
  from [<a href="#android-dev">6</a>].
	</div>
	
	<table class="center">
		<tr class="header">
			<td>CODEC</td><td>Encoding</td><td>Decoding</td><td>Container format</td>
		</tr>
		<tr>
			<td>H.263</td><td>Yes</td><td>Yes</td><td>3GPP (<span class="code">.3gp</span>) and MPEG-4 (<span class="code">.mp4</span>)</td>
		</tr>
		<tr>
			<td>H.264</td><td>No (supported <br/>from 3.0 onwards)</td><td>Yes</td><td>3GPP (<span class="code">.3gp</span>) and MPEG-4 (<span class="code">.mp4</span>).<br/>Only Baseline Profile (BP)</td>
		</tr>
		<tr>
			<td>MPEG-4</td><td>No</td><td>Yes</td><td>3GPP (<span class="code">.3gp</span>)</td>
		</tr>
		<tr>
			<td>VP8</td><td>No</td><td>No (supported from<br/>2.3.3 onwards) </td><td> WebM (<span class="code">.webm</span>)</td>
		</tr>
	</table>
</div>

<div class="table">
	<a name="tab:android-audio-formats"></a>
	<div class="caption">
		<span class="captionTitle">Table 2.5</span> Android supported audio CODECs and container formats. Extracted
  from [<a href="#android-dev">6</a>].
	</div>
	
	<table class="center">
		<tr class="header">
			<td>CODEC</td><td>Encoding</td><td>Decoding</td><td>Container format</td>
		</tr>
		<tr>
			<td>AAC LC/LTP</td><td>Yes</td><td>Yes</td><td>3GPP (<span class="code">.3gp</span>) and MPEG-4 (<span class="code">.mp4</span>, <span class="code">.m4a</span>)</td>
		</tr>
		<tr>
			<td>HE-AAC v1</td><td>No</td><td>Yes</td><td>3GPP (<span class="code">.3gp</span>) and MPEG-4 (<span class="code">.mp4</span>, <span class="code">.m4a</span>)</td>
		</tr>
		<tr>
			<td>HE-AAC v2</td><td>No</td><td>Yes</td><td>3GPP (<span class="code">.3gp</span>) and MPEG-4 (<span class="code">.mp4</span>, <span class="code">.m4a</span>)</td>
		</tr>
		<tr>
			<td>MP3</td><td>No</td><td>Yes</td><td>MP3 (<span class="code">.mp3</span>). Mono and stereo 8-320 kb/s constant (CBR) <br/> or variable bit-rate (VBR)</td>
		</tr>
		<tr>
			<td>Vorbis</td><td>No</td><td>Yes</td><td>Ogg (<span class="code">.ogg</span>)</td>
		</tr>
		<tr>
			<td>PCM/WAVE</td><td>No</td><td>Yes</td><td>WAVE (<span class="code">.wav</span>)</td>
		</tr>
	</table>
</div>


<h3><a name="sec:protocols-android"></a><a name="2.10.2"></a>2.10.2 Adaptive protocols over HTTP supported on Android</h3>

<p>Android's media framework natively supports streaming over RTP and RTSP.
Unfortunately, the majority of Android versions do not support any of the
adaptive protocols over HTTP mentioned earlier. Only Honeycomb features
Apple-HLS natively. During the development of this master's thesis project
there was no media player for Android supporting the recent MPEG DASH
standard. This section explores the existing compatibilities with regard to Apple-HLS,
Microsoft-LSS, and Adobe-HDS.</p>

<h4><a name="2.10.2.1"></a>2.10.2.1 Apple-HLS support</h4>

<p>At the moment there are a few implementations of Apple-HLS for Android:</p>

<ul>  
  <li><strong>NexPlayer<sup>TM</sup></strong> was released in September 2010
  by Nextreaming Corp. They claim to support Apple's adaptive streaming
  approach. Unfortunately, neither the application nor detailed features are publicly
  available at their website.</li>
  
  <li><strong>VPlayer</strong> is a commercial video player available from the
  Android Market<a href="#footnote9" name="note9"><sup>9</sup></a>.
  Unfortunately, VPlayer is not compatible with all Android devices.</li>
  
  <li><strong>Daroon Player</strong> is a free video player developed by
  Daroonsoft and offered from the Android market<a href="#footnote10" name="note10"><sup>10</sup></a>. 
  It supports a wide variety of media formats and streaming protocols,
  including RTSP and Apple-HLS.</li>
</ul>

<h4><a name="2.10.2.2"></a>2.10.2.2 Microsoft-LSS support</h4>

<p>Microsoft's adaptive streaming approach for Android is not available yet
officially, although Microsoft has indicated that they soon plan to support it
through a Silverlight<a href="#footnote11" name="note11"><sup>11</sup></a>
browser plug-in soon. However, the open-source implementation of Silverlight
for Unix-based operating systems (<em>Moonlight</em>), has been experimentally
ported to Android<a href="#footnote12" name="note12"><sup>12</sup></a>.</p>

<h4><a name="2.10.2.3"></a>2.10.2.3 Adobe-HDS support</h4>

<p>The Adobe Flash 10.1 plug-in for browsers is available<a href="#footnote13" name="note13"><sup>13</sup></a> for Android 2.2, although
it is only compatible with a limited variety of
Android devices<a href="#footnote14" name="note14"><sup>14</sup></a>. The plug-in
supports RTP and RTSP streaming, HTML progressive download, Adobe's Flash
Streaming, and Adobe-HDS.</p>


<h2><a name="sec:http-comparison"></a><a name="2.11"></a>2.11 Comparison among the different HTTP-based adaptive solutions</h2>

<p>Table <a href="#tab:http-comparison">2.6</a> summarizes
the main features of the most relevant HTTP-based adaptive streaming solutions:
Microsoft-LSS, Apple-HLS, and MPEG-DASH.</p>

<div class="table">
	<a name="tab:http-comparison"></a>
	<div class="caption">
		<span class="captionTitle">Table 2.5</span> Comparison among Microsoft-LSS, Apple-HLS, and MPEG-DASH. Extracted
  from [<a href="ts26234">1</a>, <a href="#smooth">55</a>, <a href="#pantos">59</a>].
	</div>
	
	<table>
		<tr class="header">
			<td>Feature</td><td>Microsoft-LSS</td><td>Apple-HLS</td><td>MPEG-DASH</td>
		</tr>
		<tr>
			<td>Specification</td><td>Proprietary</td><td>Proprietary</td><td>Standard</td>
		</tr>
		<tr>
			<td>Video on demand</td><td>Yes</td><td>Yes</td><td>Yes</td>
		</tr>
		<tr>
			<td>Live</td><td>Yes</td><td>Yes</td><td>Yes</td>
		</tr>
		<tr>
			<td>Delivery protocol</td><td>HTTP</td><td>HTTP</td><td>HTTP</td>
		</tr>
		<tr>
			<td>Origin server</td><td>MS IIS</td><td>HTTP</td><td>HTTP</td>
		</tr>
		<tr>
			<td>Media container</td><td>MP4</td><td>MPEG-TS</td><td>3GP or MP4</td>
		</tr>
		<tr>
			<td>Supported video CODECs</td><td>Agnostic</td><td>H.264</td><td>Agnostic</td>
		</tr>
		<tr>
			<td>Recommended segment duration (s)</td><td>2</td><td>10</td><td>Flexible</td>
		</tr>
		<tr>
			<td>End-to-end latency (s)<br/>(variable, depending on the size of segments)</td><td>> 1.5</td><td>30</td><td>> 2</td>
		</tr>
		<tr>
			<td>File type on server</td><td>Contiguous</td><td>Fragmented</td><td>Both</td>
		</tr>
	</table>
</div>

<p>In order to implement a functional live streaming service for Android, all the
limitations of the operating system must be considered, as well as the
possibility of deploying a compatible server. We explicitly considered the
following:</p>

<ul>
	<li>Adobe's and Microsoft's solutions are proprietary and both require
	specialized servers. Such approaches increase cost and decrease the openness
	of the resulting service.</li>

	<li>Apple-HLS intends to be a IETF standard, but its specification
	(regarding CODEC and container format of segments) is strict enough to
	consider a straightforward implementation for Android. Although H.264 is a
	fully supported CODEC on Android, MPEG-TS as a container format is only
	included in Android version 3.0 (codenamed Honeycomb) and onwards. Therefore, a
	file conversion is required to support Apple-HLS.</li>

	<li>MPEG-DASH is an emerging adaptive HTTP streaming standard which is
	flexible enough to be implemented in devices with Android built-in.</li>
</ul>


<h1><a name="chap:related-work"></a><a name="3"></a>3 Related work</h1>

<div class="quoteblock">
	<p class="quote"><em>"If you wish to make an apple pie from scratch, you must
first invent the universe."</em></p>
	<p class="quoteAuthor">- Carl Sagan</p>
</div>

<p>Extensive work has been carried out in the area of adaptive streaming over HTTP
(i.e., using HTTP as a delivery protocol). Multiple rate adaptation mechanisms
have been proposed and experiments have been performed under different
network conditions. An extensive evaluation of adaptive streaming, including
live sources under heterogeneous network rates, has been carried out
in [<a href="#eklof">26</a>], although using RTP and RTSP as delivery protocols.</p>

<p>In [<a href="#safiqul">62</a>] the media segmentation procedure has been utilized to provide
a HTTP streaming server with dynamic advertisement splicing. Unfortunately, the
evaluation only included experiments under homogeneous bit-rate conditions,
therefore no rate-adaptation was performed in either server or client.</p>

<p>The fundamental capabilities of the 3GPP's MPEG-DASH standard have been
demonstrated in [<a href="#stockhammer">69</a>], pointing out the most significant properties
of the media presentation descriptor (MPD or simply <em>manifest</em> file).
Long-session experiments for both on-demand and live video content were
performed, featuring advertisement insertion. An experimental comparison between
Apple's HLS and MPEG-DASH over an HSPA network has been carried out in
[<a href="#siraj">67</a>], although only on-demand content was considered.</p>

<p>The benefits of the Scalable Video Coding (SVC) (an extension
of H.264/MPEG-4 AVC [<a href="#h264">42</a>, Annex G]) in a MPEG-DASH environment are
demonstrated in [<a href="#sanchez">64</a>]. Media content is divided into SVC layers and
time intervals. By means of this H.264 extension, storage requirements and
congestion at the origin server are claimed to be reduced.
SVC in conjunction with Multiple Descriptor Coding (MDC) were tested over a
peer-to-peer (P2P) video on-demand system in [<a href="#abboud">3</a>]. An initial
adaptation algorithm is suggested, based on the client's display resolution,
bandwidth, and processing power. During playback, a progressive quality
adaptation is carried out, monitoring the buffer state and analyzing the change
of download throughput during the buffering process.</p>

<p>In [<a href="#muller">57</a>] a MPEG-DASH prototype is presented as a plug-in for the
VideoLan player 1.2.0 (VLC). A novel rate adaptation algorithm for MPEG-DASH was
proposed in [<a href="#liu">49</a>], using a smoothed throughput measurement (based on the
segment fetch time) as the fundamental metric. Therefore, the algorithm can be
implemented at the application layer since it does not consider TCP's
round-trip time (RTT). Upon detecting that the media bit-rate does not match
the current end-to-end network capacity, an mechanism for conservative
up-switching and aggressive down-switching of representations is invoked.</p>

<p>A pre-fetching approach for user-generated content video is presented in
[<a href="#khemmarat">45</a>]. It predicts a set of videos which are likely to be
watched in the near future and downloads them before they are requested. The
benefits of the pre-fetching scheme are compared with a traditional caching
scheme are demonstrated in a number of different network scenarios.</p>

<p>An intensive experiment on rate-adaptation mechanisms
of adaptive streaming is presented in [<a href="#akhshabi">5</a>]. Three different players
(OSMF, Microsoft Smooth Streaming, and
Netflix) are evaluated in a broad variety of scenarios (both on-demand and
live) with both persistent and short-term changes in the network's available
bandwidth and shared bottleneck links. J. Yao, et al. [<a href="#yao">73</a>] carried out an
empirical evaluation of HTTP adaptive streaming under vehicular mobility.</p>

<p>An experimental analysis of HTTP-based request-response streams compared to
classical TCP streaming is presented in [<a href="#kuschnig">46</a>]. It is claimed that the
HTTP streams are able to scale with the available bandwidth by increasing the
chunk size or the number of concurrent streams.</p>

<p>A Quality Adaptation Controller (QAC) for live
adaptive video streaming which employs feedback control theory is proposed in
[<a href="#cicco">23</a>]. Experiments with greedy TCP connections are performed
over the Akamai High Definition Video Server (AHDVS), considering bandwidth
variations and different streams which share a network bottleneck.</p>

<p>Evensen, et. al. [<a href="#evensen">27</a>] present a client scheduler that distributes
requests for video over multiple heterogeneous interfaces simultaneously. Segments
are divided into smaller sub-segments. They experimented with on-demand and
quasi-live streaming. Evaluations have been performed over three different types
of streaming: on-demand (assuming infinite buffer, only limited
by network bandwidth), live streaming with buffering (the whole video is not
available when streaming starts), and live streaming without buffering.
The last scenario considers <em>liveness</em> as the most important
metric, thus segments are skipped if the stream lags too far behind the
broadcast.</p>

<p>An elaborated comparison between Apple's HLS on iPhone and RTP on Android 1.6 is
presented in [<a href="#ransburg">61</a>]. In particular, the impact of packet delay and
packet loss are evaluated with respect to the start-up delay and playback, as
well as TCP traffic fairness.</p>

<p>From previous work in the area of adaptive streaming we can deduce
that there is still a lack of evaluation on mobile devices, especially those
using the most recent standards (such as MPEG-DASH, introduced in
section <a href="#sec:mpeg-dash">2.4.5</a>) for the particular case of live content sources. This
master's thesis aims to fill the gap by deploying a full service for mobile
devices, providing an extensive evaluation (over a set of heterogeneous network
scenarios similar to the experiments carried out in [<a href="#akhshabi">5</a>]) with
different adaptation mechanisms (also described as feedback controllers). These
mechanisms are substantially based on the algorithms proposed in
[<a href="#cicco">23</a>, <a href="#liu">49</a>, <a href="#siraj">67</a>], although some enhancements have been made,
specifically: (1) a mechanism to discard segments upon abrupt reduction of the network's available
bit-rate, and (2) a procedure to lower the selected media quality on the
client's side in case of a buffer underflow.</p>


<h1><a name="chap:design"></a><a name="4"></a>4 Design and implementation</h1>

<div class="quoteblock">
	<p class="quote"><em>"Simplicity is the prerequisite for reliability"</em></p>
	<p class="quoteAuthor">- Edsger W. Dijkstra</p>
</div>

<p>This chapter explains each of the elements of the overall
system (depicted in figure <a href="#fig:system-architecture">4.1</a>). The most important
entities are the server and the client which are explained in
section <a href="#sec:server">4.3</a> and section <a href="#sec:client">4.4</a>, respectively.
Communication between these two entities flows over HTTP. The advantages of
using HTTP were described in chapter <a href="#chap:background">2</a>.
Synchronization of the client and server are described in
section <a href="#sec:synchronization">4.2</a>.</p>

<div class="figure">
	<a name="fig:system-architecture"></a>
	<img src="img/system-architecture.png" alt="System architecture."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.1</span> System architecture.
	</div>
</div>

<p>Two types of servers have been deployed depending on the nature of content: one
of the servers provides video <em>on-demand</em>
(section <a href="#sec:on-demand-server">4.3.1</a>) and the other offers <em>live</em> video
(section <a href="#sec:live-server">4.3.2</a>). As, explained in chapter <a href="#chap:background">2</a>,
the media content needs to be encoded and segmented to satisfy the
specifications of the MPEG-DASH standard and Apple-HLS (see
section <a href="#sec:mpeg-dash">2.4.5</a> and section <a href="#sec:apple-hls">2.4.2</a> respectively). This
procedure is represented by the <em>content preparation</em> module, which is
characterized in section <a href="#sec:content-preparation">4.1</a>.</p>

<p>In reality, network traffic conditions are susceptible to change. A
<em>network emulator</em> is described in section <a href="#sec:network-emulator">4.5</a>. This
network emulator enables controlled experiments to be performed with different
bit-rates, various delays, and different packet loss rates.</p>


<h2><a name="sec:content-preparation"></a><a name="4.1"></a>4.1 Content preparation</h2>

<p>Figure <a href="#fig:content-preparation">4.2</a> depicts the modules which multiplex the
input media content into different quality streams followed by a segmentation
procedure. The transcoder part and the selection of the <em>R</em> representations
are explained in section <a href="#sec:transcoder-module">4.1.1</a>, followed by the
segmentation, combiner, and indexing parts in
sections <a href="#sec:segmenter-combiner-module">4.1.2</a> and <a href="#sec:indexing-module">4.1.3</a>,
respectively. The overall output will be pushed to the HTTP servers as is
explained in sections <a href="#sec:on-demand-server">4.3.1</a> and <a href="#sec:live-server">4.3.2</a>.</p>

<div class="figure">
	<a name="fig:content-preparation"></a>
	<img src="img/content-preparation.png" alt="Modules for content preparation."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.2</span> Modules for content preparation. A media file is indicated as
  input. <em>R</em> different representations are generated, producing <em>n</em>
  segments for each original segment. An index file is also produced as output.
	</div>
</div>


<h3><a name="sec:transcoder-module"></a><a name="4.1.1"></a>4.1.1 Transcoder module</h3>

<p>The transcoder module is responsible for generating different quality
levels, as described in section <a href="#sec:quality-levels">2.8</a>. This module receives
a media file as input (containing a video and audio stream, at least one of them is
required to be present), then produces from the audio/video stream several files
encoded at different bit-rates. Audio and video are combined using the MP4
container format. This module is implemented as a <span class="code">BASH</span> script and relies
on the FFmpeg [<a href="#bellard">13</a>] and x264 [<a href="#x264">71</a>]
libraries<a href="#footnote15" name="note15"><sup>15</sup></a>. Listing <a href="#lst:ffmpeg-command">4.1</a> and
listing <a href="#lst:x264-command">4.2</a> illustrate the use of the <span class="code">ffmpeg</span> command
and the x264 parameters applied, in order to satisfy the H.264 Baseline Profile
(introduced in the CODECs section).</p>

<div class="listing">
	<a name="lst:ffmpeg-command"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.1</span> FFmpeg command line used in the transcoder module. Note the
fixed parameters defined in the first line: frame rate, resolution, aspect
ratio, and GOP size.
	</div>
<pre>
ffmpeg -i $INPUT -y -r 25 -s 480x320 -aspect 3:2 -g 25 \
-acodec libfaac -ab $ABITRATE -ac $CHANNELS -ar $SAMPLE_RATE \
-vcodec libx264 $X264_PARAMS -b $VBITRATE -bufsize $VBITRATE -maxrate $VBITRATE \
-async 10 -threads 0 -f $FILE_FORMAT -t $CLIP_DURATION $OUTPUT
</pre>
</div>

<div class="listing">
	<a name="lst:x264-command"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.2</span> Parameters used for H.264 encoding at Baseline Profile. Note
that the <em>coder</em> attribute is set to 0 to restrict to H.264 Baseline
Profile.
	</div>
<pre>
X264_PARAMS=-coder 0 -flags +loop+mv4 -cmp 256 -subq 7 -trellis 1 -refs 5 -bf 0 -wpredp 0
-partitions +parti4x4+parti8x8+partp4x4+partp8x8+partb8x8 -flags2 -wpred-dct8x8
-me_range 16 -g 25 -keyint_min 25 -sc_threshold 40 -i_qfactor 0.71 -qmin 10
-qmax 51 -qdiff 
</pre>
</div>


<h3><a name="sec:segmenter-combiner-module"></a><a name="4.1.2"></a>4.1.2 Segmenter and combiner modules</h3>

<p>The segmenter module receives a set of media files encoded at different
bit-rates and splits them into several segments (with similar features to
those described in [<a href="#mcdonald-segmenter">51</a>]). In addition, an
<em>initialization segment</em> is also generated as an output. The
initialization segment provides the meta-data<a href="#footnote16" name="note16"><sup>16</sup></a> which
describes the media content, without including any media data. Furthermore, it
supplies the timing information (specifically the DTS and PTS, as defined in
section <a href="#sec:dts-pts">2.5.2</a>) of every segment.</p>

<p>This module reads the different parts or <em>boxes</em> of the container format
and separates the file into several pieces of approximately the same duration
(this duration is passed in as an input parameter). It attempts GOP alignment
between all the input files, that is, <em>segments</em> always start with a
key-frame<a href="#footnote17" name="note17"><sup>17</sup></a> and the <em>breaking point</em> is
the same for all representations.</p>

<p>Several tools can be used to analyze the structure of a media file. In
particular, <span class="code">MP4box</span><a href="#footnote18" name="note18"><sup>18</sup></a> is able to list all the
elements of a container format in a NHML file (an XML-based type for
multiplexing purposes), indicating which samples are a Random Access Points
(RAPs) and which are not. Listing <a href="#lst:track1-nhml">4.3</a> shows a sample NHML
output from <span class="code">MP4box</span>.</p>

<div class="listing">
	<a name="lst:track1-nhml"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.3</span> NHML example file produced by MP4box.
	</div>
<pre>
&lt;?xml version="1.0" encoding="UTF-8" ?>
&lt;NHNTStream version="1.0" timeScale="25" streamType="4" objectTypeIndication="33"
 specificInfoFile="..." width="480" height="320" trackID="1"
 baseMediaFile="..." >
   &lt;NHNTSample DTS="0" dataLength="1082" isRAP="yes" />
   &lt;NHNTSample DTS="1" dataLength="11" />
   ...
   &lt;NHNTSample DTS="25" dataLength="413" isRAP="yes" />
   ...
   &lt;NHNTSample DTS="4644" dataLength="413" isRAP="yes" />
   ...
&lt;/NHNTStream>
</pre>
</div>

<p>Sequentially, the combiner module produces segments that can be played on
Stagefright. To enable this it transforms all the chunks into self-contained
files taking the header information from the initialization segment.
Since the live stream will consist of several self-contained segments, there is
no need to modify the DTS or PTS.</p>


<h3><a name="sec:indexing-module"></a><a name="4.1.3"></a>4.1.3 Indexing module</h3>

<p>An index of all the segments generated in the previous steps must be pushed to
the HTTP servers. This module inspects the segments that have been produced and
generates an ordered list (MPD) which satisfies the MPEG-DASH
standard guidelines. Two different types of MPDs are created:</p>

<ul class="description">
	<li><strong>Standard form</strong> provides a full file, containing all the
	representation levels and a list of available segments for each one (i.e., all
	URIs).</li>
	
	<li><strong>Template form</strong> provides a shorter file. It uses the
	<span class="code">Urltemplate</span> tag, indicating the index bounds (<span class="code">startIndex</span> and
	<span class="code">endIndex</span>). This type of manifest is especially useful in the case of live
	content, since fewer modifications must be done for every MPD update.</li>
</ul>


<h2><a name="sec:synchronization"></a><a name="4.2"></a>4.2 Synchronization between server and client</h2>

<p>In the particular case of live video content, it is useful if both server
and client have the same sense of time. <em>Synchronization</em> in this context
means that provider and consumer sides communicate with an external time server
to set their clocks to the same accurate time base. Compared to a
non-synchronized scheme, clients do not need to make so many queries
to the server (<em>HTTP requests</em>) since the clients knows in advance when new
content will be available.</p>

<p>Synchronization is achieved by means of the Simple Network Time Protocol (SNTP)
[<a href="#sntp-rfc4330">56</a>], which is based on the Network Time Protocol (NTP).
Fortunately, many NTP public servers are freely available on the Internet.
The NTP Pool project<a href="#footnote19" name="note19"><sup>19</sup></a> has been selected for this purpose because it
provides a pool of free NTP servers operating on a reasonable-use basis. Indeed,
the implementation of this prototype follows the recommendations of
[<a href="sntp-rfc4330">56</a>, sec. 10] to perform a fair use of the time servers, thus
periodic requests are never performed more frequently than every 30 seconds.
Figure <a href="#fig:ntp-pool">4.3a</a> depicts the relation between client, server, and
the NTP pool.</p>

<div class="figure">
	<a name="fig:ntp-pool"></a>
	<img src="img/ntp-pool.png" alt="Communication among client, server and NTP server pool."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.3</span> (a) Communication among client, server and NTP server pool. (b) A
  simple SNTP request.
	</div>
</div>

<p>Figure <a href="#fig:ntp-packet">4.4</a> depicts the header of a NTP packet. There are three
fields needed for the simplest synchronization: time-stamp of client's request
(<span class="code">Originate Timestamp</span> field), time-stamp of the client's request arrival at
the time server (<span class="code">Receive Timestamp</span> field), and a time-stamp of
when the server's response was transmitted (<span class="code">Transmit Timestamp</span>). The
rest of header fields (such as <span class="code">Poll</span>, <span class="code">Stratum</span>, and <span class="code">Precision</span>)
and are not considered here for simplicity (for further details
see [<a href="#sntp-rfc4330">56</a>]).</p>

<div class="figure">
	<a name="fig:ntp-packet"></a>
	<img src="img/ntp-packet.png" alt="NTP packet header."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.4</span> NTP packet header. Relevant fields for the synchronization procedure
  are shown highlighted.
	</div>
</div>

<p>The synchronization procedure (shown in figure <a href="#fig:ntp-pool">43b</a>) is
performed in this prototype as follows:</p>

<ol>
	<li>The NTP server receives the request and adds the time when the request
	was received (<em>t<sub>1</sub></em>) to the <span class="code">Receive Timestamp</span> header field.</li>
	
	<li>A response is sent with a new time-stamp (<em>t<sub>2</sub></em>) indicated in the
	<span class="code">Transmit Timestamp</span> header field.</li>
	
	<li>The client receives the response and computes the offset (<em>T<sub>offset</sub></em>)
	that should be applied to its local time.</li>
</ol>

<p>The offset (<em>T<sub>offset</sub></em>) (between the machine's local time and the NTP time)
and round-trip-time (RTT) (path delay between the client and NTP server) can be
determined as:</p>

<div class="equation">
	<table>
		<tr>
			<td>
				T<sub>offset</sub> = 
			</td>
			<td class="top3">
				(t<sub>1</sub> <span class="code">-</span> t<sub>0</sub> ) + (t<sub>2</sub> <span class="code">-</span> t<sub>3</sub> )
				<div class="hrcomp"><hr/></div>
				2
			</td>
			<td>
				<span class="equationnum">(4.1)</span>
			</td>
		</tr>
	</table>
</div>

<div class="equation">
	<table>
		<tr>
			<td>
				RTT = (t<sub>3</sub> <span class="code">-</span> t<sub>0</sub> ) <span class="code">-</span> (t<sub>2</sub> <span class="code">-</span> t<sub>1</sub> )
				<span class="equationnum">(4.2)</span>
			</td>
		</tr>
	</table>
</div>

<p>Both HTTP client and HTTP server will add their respective offset to
their local time (note that offset can be a negative quantity). In particular,
the HTTP client's request time becomes <em>t<sub>0</sub> + T<sub>offset</sub></em>. For the HTTP
client's operations that need an absolute time reference, the offset is simply added to
the result of a Java method <span class="code">System.currentTimeMillis()</span> invocation.</p>


<h2><a name="sec:server"></a><a name="4.3"></a>4.3 HTTP Servers</h2>

<p>Two types of HTTP servers have been deployed in this architecture. The first
server is suitable only for video on-demand, whereas the second server serves
content from live sources. Each of these servers is briefly explained in
the following sections.</p>


<h3><a name="sec:on-demand-server"></a><a name="4.3.1"></a>4.3.1 On-demand server</h3>

<p>An Apache [<a href="#laurie">47</a>] HTTP server acts as video on-demand server. Apache has
been selected because it is robust and easy to deploy on Gnu/Linux machines.
The purpose of this server in our architecture is simple: this HTTP server
provides a list of <em>manifest</em> files which contain URLs for the segments
generated for every representation. Table <a href="#tab:mime-added">4.1</a> lists the
Multipurpose Internet Mail Extensions (MIME) [<a href="#rfc-2045">32</a>] types that needed
to be added to the Apache configuration. These types are added to the Apache's
configuration file (<span class="code">httpd.conf</span>).</p>

<div class="table">
	<a name="tab:mime-added"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.1</span> Additional MIME types needed for the Apache HTTP server.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Type</td><td>MIME type</td><td>File extension</td>
		</tr>
		<tr><td>DASH manifest</td><td>video/vnd.3gpp.mpd</td><td><span class="code">.3gm</span></td></tr>
		<tr><td>DASH video segment</td><td>audio/vnd.3gpp.segment</td><td><span class="code">.3gs</span></td></tr>
		<tr><td>Apple-HLS playlist</td><td>application/x-mpegURL</td><td><span class="code">.m3u8</span></td></tr>
		<tr><td>Apple-HLS video segment</td><td>video/MP2T</td><td><span class="code">.ts</span></td></tr>
	</table>
</div>


<h3><a name="sec:live-server"></a><a name="4.3.2"></a>4.3.2 Live server</h3>

<p>We have decided to use Twisted [<a href="#fettig">30</a>] is an event-driven networking
engine written in Python [<a href="#goerzen">35</a>], licensed under the MIT
license<a href="#footnote20" name="note20"><sup>20</sup></a>, because it supports
a wide variety of protocols and it contains multiple resources to deploy a
simple web server. The live server is based on a content-loop server developed
previously at Ericsson GmbH. The content-loop server has been modified to
satisfy the requirements of the system architecture proposed in this chapter.</p>

<p>HTTP responses are generated by an abstract class which extends Twisted's
<span class="code">Resource</span> type. A simplification of this class is shown in
listing <a href="#lst:dashresource">4.4</a>.</p>

<div class="listing">
	<a name="lst:dashresource"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.4</span> DashResource abstract class.
	</div>
<pre>
class DashResource(resource.Resource):

   def returnSuccess(self, request, content, contentType):
      ... // Set HTTP headers
      request.setHeader('Content-Length', "%d" % len(content))
      request.setResponseCode(http.OK)
      request.write(content)
      request.finish()

   def returnFailed(self, request, error):
      request.setResponseCode(404)
      request.write(error)
      request.finish()
</pre>
</div>

<p>The live server receives all media segments and manifest files that have
previously been generated (as explained in
section <a href="#sec:content-preparation">4.1</a>) and prepares a live source. In our
prototype, live content is provided by looping several clips and
numbering all segments by means of the mathematical <em>modulo</em> function to
produce an unbounded stream of content. Segments requested with a index greater
than the available segments are automatically pointed to an existing segments
modulo the total number of segments, thus providing an infinite loop of video
content. Segments are numbered with an arbitrary length integer. The behaviour
of the server is summarized as follows:</p>

<ol>
	<li>Server starts. The server inspects all the representations and segments
	in order to generate the first <em>manifest</em> file. This MPD has the attribute
	<span class="code">type</span> set to <span class="code">Live</span>, indicating that the availability of segments is
	limited (in time - i.e., that they have to be requested within a bounded
	period of time) and susceptible to change (i.e., asking for a given segment
	number at a later time might result in a different segment of media content).</li>
	
	<li>A set of segments is offered according to the <em>available shifting
	time</em> or <em>window</em>, indicated in the server configuration. The server
	calculates when the next update will occur, this primarily depends on the
	segments' duration. When the duration of one segment has elapsed, a new update
	is executed.</li>
	
	<li>If a HTTP request is received from a client, the <em>overloaded</em>
	Twisted method <span class="code">render(self, request)</span> is invoked. Different situations
	will arise depending on the client's request and server's state:
		
		<ol>
			<li>If the client is requesting the <strong>MPD file</strong>, then the server
			simply responses with the last updated XML content in this file.</li>
			
			<li>If the client is requesting a <strong>media segment</strong> numbered with an
			index <em>i</em> (as depicted in figure <a href="#fig:loop-behaviour">4.5</a>). The server
			checks whether the segment <em>i</em> belongs to the set of available segments:
				<ul>
					<li>If this segment is within the available shifting time, then the server
					performs the modulo operation and replies with a satisfactory HTTP response
					(<span class="code">code 200</span>), starting the transmission of the segment.</li>
					
					<li>Otherwise, the server replies with a HTTP unsatisfactory response
					(<span class="code">code 404</span>) and a simple message. There are two possible situations in
					which situation may occur: the client requests the segment too soon
					(segment not available yet) or the segment was requested too late
					(segment no longer available).</li>
				</ul>
			</li>
		</ol>
	</li>
</ol>
			
<div class="figure">
	<a name="fig:loop-behaviour"></a>
	<img src="img/loop-behaviour.png" alt="Characterization of the Live server."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.5</span> Characterization of the Live server. <em>T<sub>shift</sub></em> represents the
  indexes of segments within the available shifting time, <em>n</em> the number of
  segments for one representation.
	</div>
</div>


<h2><a name="sec:client"></a><a name="4.4"></a>4.4 Client</h2>

<p>In this architecture, an Android application acts as the client. Android
cellphones have sufficient capabilities to provide video playback and
perform communication over HTTP.</p>


<h3><a name="sec:main-features"></a><a name="4.4.1"></a>4.4.1 Features</h3>

<p>The client developed in this master's thesis project has the following
features:</p>

<ul>
  <li>Media adaptation by means of several bit-rate algorithms
  (section <a href="#sec:algorithms">4.4.2</a>). The optimal representation level is selected
  depending on the network's restrictions/performance.</li>
  
  <li>Supports the MPEG-DASH protocol, as summarized in section <a href="#sec:mpeg-dash">2.4.5</a>. In particular, this client is
	compatible with the <em>manifest</em> files (MPD) that the server produces.</li>
	
	<li>Minimal support for Apple-HLS (section <a href="#sec:apple-hls">2.4.2</a>), more
	specifically it is compatible with Apple's extended playlists (<span class="code">.m3u8</span>)
	and conversion of the MPEG-TS container format to the 3GP/MP4 formats supported on Android (a
	complete list of Android supported CODECs and media formats were listed in
	table <a href="#tab:android-video-formats">2.4</a>).</li>
	
	<li>Handling of HTTP connections occurs as background tasks, thus preventing
	interruptions of the media player.</li>
	
	<li>Database management of content sources, as indicated by the URL of
	MPDs or extended M3U playlists.</li>
	
	<li>Automated search of manifest files.</li>
	
	<li>A rich Graphical User Interface (GUI) is provided. During playback, the
	client displays dynamic plots which provide accurate information about the actual bandwidth
	utilization or when segments are downloaded. A full-screen mode is
	automatically launched when the cellphone is turned to a landscape orientation.</li>
</ul>


<h3><a name="sec:algorithms"></a><a name="4.4.2"></a>4.4.2 Adaptation mechanisms</h3>

<p>The adaptation mechanisms proposed in this master's thesis project follow three
requirements:</p>

<ol>
	<li>Playback shall not be stopped (i.e. buffer underflow should be avoided).</li>
	
	<li>Optimal use of network resources, selecting the highest possible bit-rate
	level while meeting requirement 1.</li>
	
	<li>Switching to the appropriate quality level should be performed as rapidly
	as possible.</li>
</ol>

<p>If any of these requirements is not satisfied, this indicate an
erroneous use of the available bandwidth. If the first requirement is not
met, this indicates that the choice of representation level selected was
overestimated. As a result, the time it will take to download the next segment
will be longer than the segment's own duration, leading to playback
interruptions if the representation level is not reduced. Not fulfilling the
second requirement indicates that the representation level has been
underestimated. In this case, the user of this client will not experience the
best possible quality - however, they will be able to watch/listen to the
content at less than the highest possible quality. The third requirement
involves a design choice: when switching events may occur. In our
implementation, the adaptation mechanism is always invoked right after segments
are downloaded (buffered). Therefore, all the proposed mechanisms are equally
fast, but provide different criteria for the appropriate quality level.</p>

<p>Three adaptation mechanisms has been proposed: <em>aggressive</em> adaptation,
<em>conservative</em> adaptation, and <em>mean</em> adaptation. Details of
these three mechanisms are explained in the following subsections.</p>


<h4><a name="sec:aggressive"></a><a name="4.4.2.1"></a>4.4.2.1 Aggressive adaptive mechanism</h4>

<p>The aggressive mechanism is defined in algorithm <a href="#alg:aggressive">1</a>. This
mechanism has the following characteristics:</p>

<ul>
  <li>The starting quality level is the level with the lowest bandwidth
  requirement.</li>

  <li>Selection of the lowest representation level occurs when the playlist
  (<em>buffer</em>) is empty.</li>
  
  <li>Multiple step switching, i.e., the selected quality level can be
  adjusted drastically up and down upon termination of the algorithm, since the
  switching operations are contained in a <span class="code">while</span> loop.</li>
</ul>

<p>The aggressive mechanism determines the optimal quality level considering
<em>only</em> the last throughput measurement <em><span class="symbol">t</span></em>. The selected quality level
is increased when the last throughput measurement is greater than the current
representation's bit-rate. Otherwise, the quality level is decreased.</p>

<div class="algorithm">
	<a name="alg:aggressive"></a>
	<div class="caption">
		<span class="captionTitle">Algorithm 1</span> Aggressive adaptive algorithm.
	</div>
<pre>
<strong>Data:</strong> Last throughput measurement <span class="symbol">t</span>, playlist P, 
ordered array of representations r with size |r| = R, 
current representation index r<sub>curr</sub>

<strong>begin</strong>
	<strong>if</strong> |P| = 0 <strong>then</strong>
		Switch-down to minimum: r<sub>curr</sub> &larr; 0
	<strong>else</strong>
		<strong>if</strong> <span class="symbol">t</span> &gt; r[r<sub>curr</sub>] <strong>then</strong>
			<strong>while</strong> r[r<sub>curr</sub>+1] &lt; <span class="symbol">t</span> <strong>and</strong> r<sub>curr</sub> &lt; R &ndash; 1 <strong>do</strong>
				Switch-up one level: r<sub>curr</sub> &larr; r<sub>curr</sub> + 1
			<strong>end</strong>
		<strong>else</strong>
			<strong>while</strong> r[r<sub>curr</sub> &ndash; 1] &gt; <span class="symbol">t</span> <strong>and</strong> r<sub>curr</sub> &gt; 0 <strong>do</strong>
				Switch-down one level: r<sub>curr</sub> &larr; r<sub>curr</sub> &ndash; 1
			<strong>end</strong>
		<strong>end</strong>
	<strong>end</strong>
<strong>end</strong>
</pre>
</div>

<p>This mechanism is referred to as aggressive because it provides a 
rapid change in behaviour in its response to bandwidth fluctuations.
Nonetheless, problems may arise with this algorithm due to short-term bit-rate
peaks. Since it will try to switch level, the increased bandwidth
might not be available for all of the next segment's download, hence the client
will not have time to download the new segment. The expected advantages and disadvantages of
the aggressive mechanism are:</p>

<ul>
  <li><strong>Advantage</strong> best utilization of available bandwidth.</li>
  
  <li><strong>Disadvantage</strong> high sensitivity to changes in available bandwidth. This
  could lead to selection of too high quality level (due to bit-rate peaks).</li>
</ul>
	

<h4><a name="sec:conservative"></a><a name="4.4.2.2"></a>4.4.2.2 Conservative adaptive mechanism</h4>
	
<p>The conservative mechanism (specified in algorithm <a href="#alg:conservative">2</a>) is
based on the aggressive mechanism (described in section <a href="#sec:aggressive">4.4.2.1</a>).
However, it enhances the selection of the quality level by adding a
<em>sensitivity</em> parameter which is applied to the last measured
throughput <em><span class="symbol">t</span></em>. Consequently, the client becomes <em>less</em> sensitive to
the available network bit-rate, resulting in a more <em>conservative</em>
selection of the representation level. In this mechanisms, a sensitivity of
the 70% is applied (note that a sensitivity of 100% will produce the
same behaviour as the aggressive mechanism). As a result, the expected
advantages and disadvantages of the conservative mechanism are:</p>

<ul>
  <li><strong>Advantage</strong> this algorithm avoids pauses while providing a
  continuous playback experience, since the selected quality level is
  systematically slightly below the optimal.</li>
  
  <li><strong>Disadvantage</strong> by underestimating the available
  network bit-rate, this algorithm leads to a non-optimal bandwidth
  utilization and less than the highest possible quality.</li>
</ul>

<div class="algorithm">
	<a name="alg:conservative"></a>
	<div class="caption">
		<span class="captionTitle">Algorithm 2</span> Conservative adaptive algorithm..
	</div>
<pre>
<strong>Data:</strong> Last throughput measurement <span class="symbol">t</span>, playlist P, 
ordered array of representations r with size |r| = R, 
current representation index r<sub>curr</sub>

<strong>begin</strong>
	Sensitivity &larr; 0.7
	<strong>if</strong> |P| = 0 <strong>then</strong>
		Switch-down to minimum: r<sub>curr</sub> &larr; 0
	<strong>else</strong>
		<span class="symbol">t</span>' &larr; <span class="symbol">t</span> &times; Sensitivity
		
		<strong>if</strong> <span class="symbol">t</span>' &gt; r[r<sub>curr</sub>] <strong>then</strong>
			<strong>while</strong> r[r<sub>curr</sub>+1] &lt; <span class="symbol">t</span>' <strong>and</strong> r<sub>curr</sub> &lt; R &ndash; 1 <strong>do</strong>
				Switch-up one level: r<sub>curr</sub> &larr; r<sub>curr</sub> + 1
			<strong>end</strong>
		<strong>else</strong>
			<strong>while</strong> r[r<sub>curr</sub> &ndash; 1] &gt; <span class="symbol">t</span>' <strong>and</strong> r<sub>curr</sub> &gt; 0 <strong>do</strong>
				Switch-down one level: r<sub>curr</sub> &larr; r<sub>curr</sub> &ndash; 1
			<strong>end</strong>
		<strong>end</strong>
	<strong>end</strong>
<strong>end</strong>
</pre>
</div>


<h4><a name="sec:mean"></a><a name="4.4.2.3"></a>4.4.2.3 Mean adaptive mechanism</h4>

<p>The mean mechanism is built upon the aggressive mechanism
(described in section <a href="#sec:aggressive">4.4.2.1</a>). Using this mechanism the optimal
quality level decision is based upon the arithmetic mean<a href="#footnote21" name="note21"><sup>21</sup></a> of the last three throughput measurements (see algorithm <a href="#alg:mean">3</a>). The overall behaviour is similar to the adaptive mechanism proposed in [<a href="#siraj">67</a>] where the last five measurements were considered. In the mean mechanism the throughput average is calculated based on the last <strong>three</strong> measures (<em><span class="symbol">t</span><sub>1</sub></em>, <em><span class="symbol">t</span><sub>2</sub></em>, and <em><span class="symbol">t</span><sub>3</sub></em>) In addition, a high sensitivity parameter is applied to the throughput average. The expected advantages and disadvantages of the mean mechanism are:</p>

<ul>
  <li><strong>Advantage</strong> better utilization of bandwidth compared to
  the aggressive and the conservative mechanism in the long-term. Reduced
  sensitivity to bit-rate peaks and troughs.</li>
  
  <li><strong>Disadvantage</strong> longer reaction time when there is a large bit-rate
  variation. Selection of the appropriate quality level might be
  performed in several switching steps.</li>
</ul>

<div class="algorithm">
	<a name="alg:mean"></a>
	<div class="caption">
		<span class="captionTitle">Algorithm 3</span> Mean adaptive algorithm.
	</div>
<pre>
<strong>Data</strong>: Last 3 throughput measurements <span class="symbol">t</span><sub>1</sub>, <span class="symbol">t</span><sub>2</sub>, and <span class="symbol">t</span><sub>3</sub>, playlist P, 
ordered array of representations r with size |r| = R, current representation index r<sub>curr</sub>

<strong>begin</strong>
	Sensitivity &larr; 0.95
	<span class="symbol">t</span><sub>mean</sub> &larr; (<span class="symbol">t</span><sub>1</sub> + <span class="symbol">t</span><sub>2</sub> + <span class="symbol">t</span><sub>3</sub>)/3
	
	<strong>if</strong> |P| = 0 <strong>then</strong>
		Switch-down to minimum: r<sub>curr</sub> &larr; 0
	<strong>else</strong>
		<span class="symbol">t</span><sub>mean</sub>' &larr; <span class="symbol">t</span><sub>mean</sub> &times; Sensitivity
		
		<strong>if</strong> <span class="symbol">t</span><sub>mean</sub>' &gt; r[r<sub>curr</sub>] <strong>then</strong>
			<strong>while</strong> r[r<sub>curr</sub>+1] &lt; <span class="symbol">t</span><sub>mean</sub>' <strong>and</strong> r<sub>curr</sub> &lt; R - 1 <strong>do</strong>
				Switch-up one level: r<sub>curr</sub> &larr; r<sub>curr</sub> + 1
			<strong>end</strong>
		<strong>else</strong>
			<strong>while</strong> r[r<sub>curr</sub>-1] &gt; <span class="symbol">t</span><sub>mean</sub>' <strong>and</strong> r<sub>curr</sub> > 0 <strong>do</strong>
				Switch-down one level: r<sub>curr</sub> &larr; r<sub>curr</sub> - 1
			<strong>end</strong>
		<strong>end</strong>
	<strong>end</strong>
<strong>end</strong>
</pre>
</div>

<h3><a name="4.4.3"></a>4.4.3 Module characterization</h3>

<p>Figure <a href="#fig:player-overview">4.6</a> illustrates the modules which constitute the
client's application. A dashed line separates the prototype from the
cellphone's external resources, such as the available memory (<em>external
storage</em>) and the <em>user interface</em>. The user interface represents the
user's interaction with the device's buttons and (where available)
touch-screen.</p>

<div class="figure">
	<a name="fig:player-overview"></a>
	<img src="img/player-overview.png" alt="Overview of the client's application modules."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.6</span> Overview of the client's application modules. The
  dashed line separates the device's <em>hardware</em> resources from
  the client's application (<em>software</em>).
	</div>
</div>

<p>The client's functionality can be summarized as follows. The <em>player
module</em> (described in section <a href="#sec:player-module">4.4.4</a>) starts the application
and manages the video controller and graphical resources, in particular, the
Android surface where the video is displayed. The <em>parser module</em>
(described in section <a href="#sec:parser">4.4.5</a>) is launched and it transforms the index
file or <em>manifest file</em> into several <em>representation playlists</em>, each
one corresponding to a determined quality level. If the parsing procedure is
successfull, this module periodically checks for manifest updates as
a background task.</p>

<p>Next the <em>segmeter-downloader module</em>
(described in section <a href="#sec:segment-downloader-module">4.4.6</a>) starts to request the
media segments over HTTP using persistent connections. A query is sent to the
<em>rate-adaptation module</em> (described in
section <a href="#sec:rate-logic-module">4.4.7</a>) after each download. The rate-adaptation
module is responsible for selecting the most appropriate quality level
depending on the network conditions. Consequently, the <em>transcoder module</em>
performs a media conversion when necessary, as was explained in
section <a href="#sec:transcoder-module">4.4.1</a>.</p>

<p>Segments successfully downloaded into the buffer are added to a primary
<em>playlist</em> (described in section <a href="#sec:playlist">4.4.6</a>), which enumerates the
received pieces of content. Changes in the playlist will be constantly monitored
by the player module.</p>

<p>The timer module (described in section <a href="#sec:timer-module">4.4.9</a>) calculates the
timing of all the events which take place in the system. This information
is essential for the evaluation described in the next chapter.</p>

<h4><a name="4.4.3.1"></a>4.4.3.1 Activities</h4>

<p>In Android terminology, an <em>activity</em> is an application component that
provides a graphical interface, listening to the user's interaction. Activities
are analogous to <em>windows</em> in typical computer applications as they
provide graphical components (such as text or buttons) and can be
opened or closed in a specific order.</p>

<p>Activities are controlled by several listeners: <span class="code">onCreate()</span> is the most
important method, as this method is invoked at the beginning of the activity.
The remaining listeners (<span class="code">onResume()</span>, <span class="code">onStop()</span>, <span class="code">onPause()</span>,
<span class="code">onRestart()</span>, and <span class="code">onDestroy()</span>) have been adapted to satisfy the
desired behaviour of the application<a href="#footnote22" name="note22"><sup>22</sup></a>. In
particular these methods:</p>

<ul>
  	<li>Stop the background tasks (<em>threads</em>) when the user
exits the application.</li>

	<li>Handle the device's <em>orientation</em> changes, i.e., when the user turns
	the cellphone more than 90 degrees. Two orientations are defined in Android:
	<em>landscape</em> (horizontal) and <em>portrait</em> (vertical), as illustrated in
	figure <a href="#fig:activity-orientation">4.7</a>.</li>
</ul>

<div class="figure">
	<a name="fig:activity-orientation"></a>
	<img src="img/activity-orientation.png" alt="Activity orientation."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.7</span> Activity orientation.
	</div>
</div>

<p>Three activities have been designed in this prototype, see
figure <a href="#fig:activity-plan">4.8</a>. The first activity, depicted on the left of the
figure, displays a list of manifest files. The user can easily add, modify and
remove entries using the GUI components (Android's <span class="code">contextMenu</span>). When an
element of the list is selected, the second activity is started. This step is
only used for our evaluation, since it selects the proposed adaptive algorithms
(these algorithms will be introduced in section <a href="#sec:rate-logic-module">4.4.7</a>).
The last activity handles the actual media playback, displaying both the video
and dynamic plots on the screen. A demonstration of the GUI can be found in the
appendix <a href="#app:gui">A</a>.</p>

<p>All activities have to be described in the
<span class="code">AndroidManifest.xml</span> file, as shown in the simplified in
listing <a href="#lst:android-manifest">4.5</a>. Lines 5-8 indicate the first activity to be
launched when the application is started (<span class="code">ContentSelection</span> activity). In
addition, the Android OS permissions required for the application need to be
specified. In our case these permissions are:</p>

<ul>
  <li><span class="code">WRITE_EXTERNAL_STORAGE</span>: to write into the MicroSD card
  (the location of the <em>buffer</em> in this prototype).</li>
  
  <li> <span class="code">INTERNET</span>: to open a HTTP communication with the servers, described
  in section <a href="#sec:server">4.3</a>.</li>
</ul>

<div class="listing">
	<a name="lst:android-manifest"></a>
	<div class="caption">
		<span class="captionTitle">Listing 2.2</span> Simplified version of the Android manifest XML-file.
	</div>
<pre>
&lt;?xml version="1.0" encoding="utf-8"?>
&lt;manifest xmlns:android="http://schemas.android.com/apk/res/android">
   &lt;application>
      &lt;activity android:name="ContentSelection">
         &lt;intent-filter>
            &lt;action android:name="android.intent.action.MAIN"/>
            &lt;category android:name="android.intent.category.LAUNCHER"/>
         &lt;/intent-filter>
      &lt;/activity>
      &lt;activity android:name="RateAlgorithmSelection"/>
      &lt;activity android:name="Player"/>
   &lt;/application>
   &lt;uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE"/>
   &lt;uses-permission android:name="android.permission.INTERNET"/>
&lt;/manifest>
</pre>
</div>

<div class="figure">
	<a name="fig:activity-plan"></a>
	<img src="img/activity-plan.png" alt="Activities of the client application."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.8</span> Activities of the client application.
	</div>
</div>


<h3><a name="sec:player-module"></a><a name="4.4.4"></a>4.4.4 Player module</h3>

<p>The player module is the essential and main component of the application. It
manages the playback of the media segments, displaying the video stream on the
screen. Figure <a href="#fig:player-flow">4.9</a> depicts the set of actions performed in
this module. Initially the module creates two background tasks with
the following purposes: (1) periodically parse the <em>manifest</em> file (to be
performed by the parsing module, explained in detail in
section <a href="#sec:parser">4.4.5</a>) and (2) download the media fragments (performed by the
segment-downloader module, described in section <a href="#sec:segment-downloader-module">4.4.6</a>).</p>

<div class="figure">
	<a name="fig:player-flow"></a>
	<img src="img/player-flow.png" alt="Sequence of events produced in the player module."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.9</span> Sequence of events produced in the player module.
	</div>
</div>

<p>The player module examines a generated <em>playlist</em> of buffered segments
(which is regularly updated by the segment-downloader module). The player
continues this processing until the main activity is closed.</p>


<h4><a name="sec:video-surfaces"></a><a name="4.4.4.1"></a>4.4.4.1 Video surface management</h4>

<p>A video surface is the main element of a video player. This surface
is represented as a <span class="code">SurfaceView</span> element in the Android framework. The
media files will be played (i.e., displayed) on this surface. The process of
binding a video surface with to instance of a Android <span class="code">MediaPlayer</span> object
takes place in four steps (as depicted in figure <a href="#fig:surface-view-binding">4.10</a>):</p>

<ol>
	<li>The surface is created in an Activity and its surface holder is
	created. The type of the surfaceHolder must be
	<span class="code">SURFACE_HOLDER_PUSH_BUFFERS</span> in order to play audio and video,
	otherwise video will not be shown on the screen.</li>
	
	<li>This holder starts three listeners:
		<ol>
			<li><span class="code">surfaceCreated</span>: triggered when the surface is ready.</li>
			
			<li><span class="code">surfaceChanged</span>: detects surface changes, for instance a change in
			its size (which could result in a change in video resolution).</li>
			
			<li><span class="code">surfaceDestroy</span>: launched when the activity which holds the
			surface ends.</li>
		</ol>
	</li> 
	
	<li>When <span class="code">surfaceCreated()</span> is invoked, the holder can be bound to an
	instance of a <span class="code">MediaPlayer</span> object. At this point it is possible to start
	loading a media file. Listing <a href="#lst:surface-created">4.11</a> shows how the first
	segment will be prepared in a new thread.</li>
	
	<li>Binding between the surface and the Android media player is done.</li>
</ol>

<div class="figure">
	<a name="fig:surface-view-binding"></a>
	<img src="img/surface-view-binding.png" alt="Surface view binding."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.10</span> Surface view binding.
	</div>
</div>

<div class="listing">
	<a name="lst:surface-created"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.6</span> Listener launched when video surface is ready.
	</div>
<pre>
public void surfaceCreated(SurfaceHolder holder) {

   mediaPlayer.setDisplay(holder); // SurfaceHolder binding
   new Thread(new Runnable() { // Start segment handling as background task

      @Override
      public void run() {
         Looper.prepare();
         playHandler = new Handler();
         playHandler.post(nextSegment);
         Looper.loop();
      }
   }).start();
}
</pre>
</div>

<p>Different techniques were studied in order to load different video segments
concurrently. By means of creating more than one instance of the
<span class="code">MediaPlayer</span> class, it may be possible to prepare several video
segments at the same time. However, this approach is not suitable because of the
<em>unique binding</em> condition, i.e., only one instance of <span class="code">MediaPlayer</span>
can be attached to a <span class="code">surfaceHolder</span> (as depicted in
figure <a href="#fig:binding-problem">4.11</a>). The Java method <span class="code">setDisplay()</span> can only be
invoked once, and further calls are ignored. This makes it necessary to utilize
another surface video for every <span class="code">MediaPlayer</span>. Unfortunately, since
<span class="code">SurfaceView</span> is a heavy object and it consumes a significant amount of
resources, this approach is not efficient. Therefore, in our implementation
several instances of <span class="code">MediaPlayer</span> are created but only one instance is
attached to a <span class="code">SurfaceView</span>.</p>

<div class="figure">
	<a name="fig:binding-problem"></a>
	<img src="img/binding-problem.png" alt="Binding problem."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.11</span> Binding problem.
	</div>
</div>

<h4><a name="4.4.4.2"></a>4.4.4.2 Implementation</h4>

<p>The listeners of the <em>Player activity</em> and the <span class="code">MediaPlayer</span>
class constitute the essential elements of this implementation.
Listing <a href="#lst:onCreate">4.7</a> shows the most significant lines of the
<span class="code">onCreate()</span> Java method, here the <span class="code">surfaceHolder</span> and
<span class="code">MediaPlayer</span> objects are instantiated.</p>

<div class="listing">
	<a name="lst:onCreate"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.7</span> Fragment of the activity's initialization method
<span class="code">onCreate()</span>.
	</div>
<pre>
surfaceHolder = surfaceView.getHolder(); // Create surfaceHolder and set listeners
surfaceHolder.addCallback(this);
surfaceHolder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);

mediaPlayer = new MediaPlayer(); // Create media player and set listeners
mediaPlayer.setOnPreparedListener(this);
mediaPlayer.setOnCompletionListener(this);
mediaPlayer.setOnErrorListener(this);
mediaPlayer.setScreenOnWhilePlaying(true);
</pre>
</div>

<p>Listing <a href="#lst:handleNextSegment">4.8</a> shows the <span class="code">handleNextSegment()</span>
method. It is launched as a background task once the <span class="code">surfaceView</span> resource
is ready. This task manages the playback of media segments, by checking whether
there are entries in the playlist. When there are new entries, the next
appropriate media segment is load asynchronously, as shown in
listing <a href="#lst:setNextDataSource">4.9</a>. There is one restriction imposed by the
Android specification (as depicted in the state diagram of
figure <a href="#fig:android-mp-states">4.12</a>): the <span class="code">MediaPlayer</span> object used by the
activity must be restarted for every segment, since <span class="code">setDataSource()</span> can
only be invoked after <span class="code">reset()</span>.</p>

<div class="listing">
	<a name="lst:handleNextSegment"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.8</span> Procedure to handle the next media segment to be played.
	</div>
<pre>
private void handleNextSegment() {

   while (!playList.isReadyToPlay()) {} // Waiting for segments on playlist
   
   ... // Update UI elements (loading wheel)
   ... //Update segment pointers
   lastSegmentPath = currentSegmentPath;
   currentSegmentPath = playList.getNext(); // Read next entry

   if (currentSegmentPath != null) {
      setNextDataSource(currentSegmentPath); // Start segment load procedure
   } else {
      if (buffer.get404Errors() > MAX_404ERRORS)
         closeMedia("Many segments missing");
   }
}
</pre>
</div>

<div class="figure">
	<a name="fig:android-mp-states"></a>
	<img src="img/android-media-player.png" alt="State diagram of Android's media player."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.12</span> State diagram of Android's media player. Note that
  <span class="code">setDataSource()</span> can only be called from the <em>Idle</em> status.
  Figure adapted from [<a href="#android-ref-mediaplayer">8</a>].
	</div>
</div>

<div class="listing">
	<a name="lst:setNextDataSource"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.9</span> Setting the next data source of the <span class="code">MediaPlayer</span>
instance.
	</div>
<pre>
private void setNextDataSource(String nextFilePath) {
   
   try {
      mediaPlayer.reset();
      mediaPlayer.setDataSource(nextFilePath);
      mediaPlayer.prepareAsync(); // Prepare segment asynchronously
   
   } catch (Exception e) {
      playHandler.post(nextSegment); // If errors: continue with next segment
   }
}
</pre>
</div>

<p>Once segments are successfully loaded, the <span class="code">onPrepared()</span> listener is
triggered, as illustrated in listing <a href="#lst:onPrepared">4.10</a>. The player proceeds
to play the segment as soon as possible, subsequently launching (according to
the <span class="code">DELETION_TIMEOUT</span> parameter<a href="#footnote23" name="note23"><sup>23</sup></a>) a background cleaning task. This task
removes old entries in the representation lists and deletes already played segments.</p>

<div class="listing">
	<a name="lst:onPrepared"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.10</span> Listener triggered when the media segment is loaded.
	</div>
<pre>
public void onPrepared(MediaPlayer mp) {
   
   if (playedSegments == 0) Timer.startPlayback(); // Timing info
   mp.start();
   new Thread(new Runnable() { // Update dynamic plots on screen in a new thread

      @Override
      public void run() {
         cleaningHandler.postDelayed(new Runnable() {

            @Override
            public void run() {
               ... // Perform RepresentationLists cleaning
               ... // Remove played segments from external storage
            }
         }, DELETION_TIMEOUT);
      }
   }).start();
}
</pre>
</div>

<p>A completion listener (<span class="code">onCompletion()</span>) is invoked when segments have
reached their end. Consequently, the next segment is
immediately prepared in order to minimize interruptions in the media during
playback. The code to do this is shown in listing <a href="#lst:on-completion">4.11</a>.</p>

<div class="listing">
	<a name="lst:on-completion"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.11</span> Listener triggered after playback completion.
	</div>
<pre>
public void onCompletion(MediaPlayer mp) {
   ... // Logging
   playedSegments++;
   playHandler.post(nextSegment); // Launch background task
   ... // Update UI’s buffer bar
}
</pre>
</div>

<p>Upon termination of the activity, background tasks and resources are released
(using the method <span class="code">closeMedia()</span> as shown in the simplified listing <a href="#lst:closeMedia">4.12</a>).</p>

<div class="listing">
	<a name="lst:closeMedia"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.12</span> Termination of background tasks.
	</div>
<pre>
public void closeMedia() {
   
   if (mediaPlayer != null) mediaPlayer.release();
   if (buffer != null) buffer.stop(); // Stop buffering background task
   if (parser != null){ // Stop manifest updater task
      ...
      if (isLive())
      if (mpdHandler != null) mpdHandler.getLooper().quit();
   }
   finish(); // Terminate activity
}
</pre>
</div>

<h3><a name="sec:parser"></a><a name="4.4.5"></a>4.4.5 Parser module</h3>

<p>This module parses a file which follows either the MPEG-DASH standard or Apple's
<span class="code">m3u8</span> extended playlists. After completion, a list of available segments
is generated for every representation, ordered by bandwidth (a basic
example was illustrated in table <a href="#tab:representation-lists">4.2</a>).
In addition, two parsing <em>modes</em> are defined:</p>

<ul>
	<li><strong>Strict mode</strong> the parser checks that the manifest file follows
	all specifications, in particular mandatory attributes.</li>
	
	<li><strong>Non-strict mode</strong> the parser operates on a best effort basis. If
	there are some attributes missing, it utilizes with default values.</li>
</ul>

<p>The player is responsible for calling the parser module at the start of
the player's execution. If the manifest file is available and it
satisfies the supported standards implemented in this prototype, an initial set
of parameters is defined: number of representations, number of segments, type of
content (on-demand or live) and segment duration, among other parameters (a
full list of the supported parameters is given in the following section).</p>

<div class="table">
	<a name="tab:representation-lists"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.2</span> Sample set of representation lists, assuming three quality levels
(denoted by <em>bw1</em>, <em>bw2</em>, and <em>bw3</em>) and 10s-long
segments.
	</div>
	
	<table class="noborder">
		<tr>
			<td>
				<table>
					<tr class="header">
						<td>Time</td><td>Segment URL</td>
					</tr>
					<tr><td>0.00</td><td>.../bw1/1.mp4</td></tr>
					<tr><td>10.00</td><td>.../bw1/2.mp4</td></tr>
					<tr><td>20.00</td><td>.../bw1/3.mp4</td></tr>
				</table>
			</td>
			<td>
					<table>
						<tr class="header">
							<td>Time</td><td>Segment URL</td>
						</tr>
						<tr><td>0.00</td><td>.../bw2/1.mp4</td></tr>
						<tr><td>10.00</td><td>.../bw2/2.mp4</td></tr>
						<tr><td>20.00</td><td>.../bw2/3.mp4</td></tr>
					</table>
			</td>
			<td>
					<table>
						<tr class="header">
							<td>Time</td><td>Segment URL</td>
						</tr>
						<tr><td>0.00</td><td>.../bw3/1.mp4</td></tr>
						<tr><td>10.00</td><td>.../bw3/2.mp4</td></tr>
						<tr><td>20.00</td><td>.../bw3/3.mp4</td></tr>
					</table>
			</td>
		</tr>
	</table>
</div>

<p>In the case of <em>live</em> content, this module is executed quasi-periodically
(assuming that the segments are the same length). The manifest file is parsed
again and the lists of segments are updated. The parsing procedure is aborted if
the manifest file has not changed<a href="#footnote24" name="note24"><sup>24</sup></a>.</p>

<h4><a name="4.4.5.1"></a>4.4.5.1 DOM and SAX</h4>

<p>Java's Simple API for XML (SAX) and the Document Object Model (DOM) are two
widely used parsing methodologies. In this design, SAX has been selected as the
parsing technology for this module<a href="#footnote25" name="note25"><sup>25</sup></a>. SAX demonstrates better
capabilities for the type of files handled in the overall application.
Advantages of SAX compared to DOM are:</p>

<ul>
	<li>SAX parsing can be stopped at any line, at any time.</li>
	<li>SAX is better for large files because it consumes less memory than DOM.</li>
	<li>SAX takes less time than DOM to read the document.</li>
	<li>SAX is read only, whereas DOM can produce changes in the file.</li>
	<li>SAX parses sequentially, DOM can go backwards to the parent nodes.</li>
</ul>

<p>In summary, SAX provides a faster method to parse XML files than DOM. Since
these files basically consist of a set of parameters and list of segments, it can be read
sequentially and there is no need for the client to modify any field. In
fact, the server is responsible for providing new updates of the manifest file
and is the <em>only</em> entity that updates this file.</p>

<h4><a name="4.4.5.2"></a>4.4.5.2 Implementation</h4>

<p>Listing <a href="#lst:dash-parser-constructor">4.13</a> shows the Java constructor,
which makes use of the <span class="code">SAXParser</span>, <span class="code">SAXParserFactory</span>, and
<span class="code">XMLReader</span> classes included in the Android framework. The main method is
<span class="code">parse()</span> (see listing <a href="#lst:parse-dash">4.14</a>) which will be invoked just
once for on-demand services and periodically for live content. A
<span class="code">XMLHandler</span> private class (listing <a href="#lst:user-xml-handler">4.15</a>) reads all
XML-tags and their attributes to generate Java objects and lists of segments
for different representations. Tables <a href="#tab:mpd">4.3</a> to <a href="#tab:url-template">4.8</a>
present the attributes of the MPD [<a href="#ts26234">1</a>] supported in this
implementation.</p>

<div class="listing">
	<a name="lst:dash-parser-constructor"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.13</span> DASH parser constructor.
	</div>
<pre>
public DASHParser(String manifestURL, Mode mode) throws IOException, 
 UnvalidManifestException {

   ... // Init SAX variables
   try {
      SAXParser = factory.newSAXParser();
      XMLReader = SAXParser.getXMLReader();
      userXMLHandler = new UserXMLHandler();
      XMLReader.setContentHandler(userXMLHandler);
   } catch (...) {} // Exception handling
      initTempDirectory(); // Create temporal directory to save segments
}
</pre>
</div>

<div class="listing">
	<a name="lst:parse-dash"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.14</span> Java method to parse an MPD file.
	</div>
<pre>
public void parse() throws IOException, SAXException,
 UnvalidManifestException {

   XMLReader.parse(parsingUrl);
   if (!isValidManifest())
      throw new UnvalidManifestException("Manifest is not valid");
   sortSegmentLists();
   this.segmentLists = getSegmentLists();
}
</pre>
</div>

<div class="listing">
	<a name="lst:user-xml-handler"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.15</span> XMLHandler private class, it overrides SAX methods to parse
	supported MPD tags. It parses attributes and transforms them into Java objects
	and lists of segments.
	</div>
<pre>
private class UserXMLHandler extends DefaultHandler {

   ... // Override methods: startDocument(), endDocument() and endElement()
   
   @Override
   public void startElement(String uri, String localName, String qName,
    Attributes attributes) throws SAXException {
      
      /* Detect all supported tags and transform to Java objects */
      createMPD(attributes); // MPD tag
      createProgramInformation(attributes); //ProgramInformation tag
      createPeriod(attributes); // Period tag
      createSegmentInfoDefault(attributes); // SegmentInfo tag
      createRepresentation(attributes); // Representation tag
      createSegmentInfo(attributes); // SegmentInfo tag
      createURL(attributes); // URL tag
      createUrlTemplate(attributes); // URLTemplate tag
   }
}
</pre>
</div>

<div class="table">
	<a name="tab:mpd"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.3</span> Supported attributes for the <span class="code">MPD</span> tag in MPEG-DASH.
	</div>
	
	<table>
		<tr class="header"><td>Attribute</td><td>Definition</td></tr>
		
		<tr><td>Type</td><td><em>Optional, on-demand by default</em>. Type of the media presentation. On-demand and live types are defined</td></tr>
   
		<tr><td>Base URL</td><td><em>Optional</em>. Base URL on MPD level</td></tr>
	   
		<tr><td>Minimum update period</td><td><em>Mandatory</em>. Minimum period the MPD
		is updated on the server.</td></tr>
		
		<tr><td>Minimum buffering time</td><td><em>Mandatory</em>. Minimum amount of
		initially buffered media that is needed to ensure smooth playback.</td></tr>
		
		<tr><td>Media presentation duration</td><td><em>Optional</em>. Duration of the
		entire media presentation.</td></tr>
		
		<tr><td>Availability start time</td><td><em>Mandatory for live, optional for on-demand</em>.
		Start time of the first period of the media presentation in UTC format.</td></tr>
	   
		<tr><td>Available shifting time</td><td><em>Optional</em>. Duration of the time shifting
		buffer that is available for a Live presentation. If it is present for
		on-demand services, the client should ignore this attribute.</td></tr>
	</table>
</div>

<div class="table">
	<a name="tab:period"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.4</span> Supported attributes for the <span class="code">Period</span> tag in MPEG-DASH.
	</div>
	
	<table>
		<tr class="header"><td>Attribute</td><td>Definition</td></tr>
		<tr><td>Start</td><td><em>Optional</em>. Accurate start time of the period relative to the <em>availability start time</em> of the media Presentation.</td></tr>
	
		<tr><td>Identifier</td><td><em>Optional</em>. Unique identifier for this period within the media Presentation.</td></tr>
	
		<tr><td>Default segment information</td><td><em>Optional</em>. Default Segment information about Segment durations and, optionally, URL construction.</td></tr>
	</table>
</div>

<div class="table">
	<a name="tab:representation"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.5</span>	Supported attributes for the <span class="code">Representation</span> tag in MPEG-DASH.
	</div>
	
	<table>
		<tr class="header"><td>Attribute</td><td>Definition</td></tr>
		<tr><td>Identifier</td><td><em>Mandatory</em>. Unique
	identifier for this representation within the period.</td></tr>
	
	<tr><td>Bandwidth</td><td><em>Mandatory</em>. Minimum bandwidth of a
	hypothetical constant bit-rate channel in bits per second over which the
	representation can be delivered such that a client, after buffering for
	exactly the <em>minimum buffering time</em> can be assured of having enough data
	for continuous playback.</td></tr>
	
	<tr><td>MIME type</td><td><em>Mandatory</em>. MIME type of the
	initialization segment, if present. If not, it provides the MIME type of the
	first media segment. This MIME type includes the CODEC parameters for all
	media types, including profile and level information where applicable.</td></tr>
	</table>
</div>

<div class="table">
	<a name="tab:segment-info"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.6</span> Supported attributes for the <span class="code">SegmentInfo</span> tag in
  MPEG-DASH.
	</div>
	
	<table>
		<tr class="header"><td>Attribute</td><td>Definition</td></tr>
	<tr><td>Base URL</td><td><em>Optional</em>. Base URL on representation level.</td></tr>
	
	<tr><td>Segment duration</td><td><em>Mandatory if duration is not
	specified on period level</em>. Constant approximate segment
	duration. All segments within this <em>segment information</em> element have the
	same duration unless it is the last segment within the period, which could be
	significantly shorter. If this attribute is not present, the value of this
	attribute is derived to be equal to the value of the duration attribute on
	period level, if present.</td></tr>
	
	<tr><td>Start index</td><td><em>Optional, 1 by default</em>. Index of the
	first accessible media segment in this representation.</td></tr>
		
	<tr><td>URL template</td><td><em>Optional</em>. The presence of this element
	specifies that a template construction process for media segments is applied.
	The element must include attributes to generate a segment list for the
	representation associated with this element.</td></tr>
	
	</table>
</div>

<div class="table">
	<a name="tab:url"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.7</span> Supported attributes for the <span class="code">URL</span> tag in MPEG-DASH.
	</div>
	
	<table>
		<tr class="header"><td>Attribute</td><td>Definition</td></tr>

  	<tr><td>Source URL</td><td><em>Optional</em>. URL of the media segment. If
	not present, then any <em>base URL</em> is mapped to the <em>sourceURL</em>
	attribute and the <em>range</em> attribute should be present.</td></tr>

	</table>
</div>

<div class="table">
	<a name="tab:url-template"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.8</span> Supported attributes for the <span class="code">URLTemplate</span> tag in MPEG-DASH.
	</div>
	
	<table>
		<tr class="header"><td>Attribute</td><td>Definition</td></tr>
		
    <tr><td>Source URL</td><td><em>Optional</em>. The source string providing the
	template. If the template is not present, the <em>id</em> attribute on
	representation level provides the necessary information to construct the
	template.</td></tr>

	<tr><td>End index</td><td><em>Optional</em>. Index of the
	last accessible media segment in this representation.</td></tr>

	</table>
</div>

<p>The lexical representation of all <span class="code">duration</span> attributes follows the W3C
ISO 8601 Date and Time Formats syntax [<a href="#biron">15</a>, section 3.2.6] <em>"P nY
nM nD T nHnM nS"</em>, where <em>nY</em> represents the number of years, <em>nM</em>
the number of months, <em>nD</em> the number of days, <em>T</em> is the date/time
separator, <em>nH</em> the number of hours, <em>nM</em> the number of minutes and
<em>nS</em> the number of seconds (decimal digits supported).</p>

<p>Additionally, extended M3U playlists can be parsed (see
listing <a href="#lst:parse-apple">4.16</a>). Table <a href="#tab:extm3u">4.9</a> and
table <a href="#tab:stream-inf">4.10</a> show the tags of the Apple's extended M3U
playlists [<a href="#pantos">59</a>] which are supported in the prototype.</p>

<div class="table">
	<a name="tab:extm3u"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.9</span> Supported tags for extended M3U playlists.
	</div>
	
	<table>
		<tr class="header"><td>Tag</td><td>Definition</td></tr>

   	<tr><td><span class="code">#EXTM3U</span></td><td><em>Mandatory</em>. All
	playlists files must start with this tag. If not, the client must not attempt
	to use the playlist.</td></tr>
	
	<tr><td><span class="code">#EXT-X-VERSION</span></td><td><em>Optional</em>. It
	specifies the protocol version. The client checks if it supports the version.
	if not, it must not attempt to use the playlist file.</td></tr>

	<tr><td><span class="code">#EXT-X-TARGETDURATION</span></td><td><em>Mandatory</em>. It
	specifies the maximum media file duration. The <span class="code">EXTINF</span> duration of each
	media file in the playlist file must be less than or equal to the target
	duration.</td></tr>

	<tr><td><span class="code">#EXT-X-MEDIA-SEQUENCE</span></td><td><em>Optional</em>. It
	indicates the sequence number of the first URI that appears in a playlist
	file. This tag can only appear once.</td></tr>

	<tr><td><span class="code">#EXTINF</span></td><td><em>Conditionally mandatory</em>.
	It is a marker which describes the media file identified by the URI that
	follows it. Each media file URI must be preceded by this tag.</td></tr>

	<tr><td><span class="code">#EXT-X-STREAM-INF</span></td><td><em>Optional</em>. It
	indicates that the next URI in the playlist file identifies another playlist
	file (sub-playlist).</td></tr>

	</table>
</div>

<div class="table">
	<a name="tab:stream-inf"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.10</span> Supported attributes for the <span class="code">EXT-X-STREAM-INF</span> tag.
	</div>
	
	<table>
		<tr class="header"><td>Attribute</td><td>Definition</td></tr>
   
	<tr><td><span class="code">BANDWIDTH</span></td><td><em>Mandatory</em>. Decimal integer of bits per second. It
	must be an upper bound of the overall bit-rate of each media file, including
	container overhead.</td></tr>

	<tr><td><span class="code">PROGRAM-ID</span></td><td><em>Optional</em>. Decimal integer that uniquely identifies
	a particular presentation within the scope of the playlist file. A playlist file may
	contain multiple <span class="code">EXT-X-STREAM-INF</span> tags with the same <span class="code">PROGRAM-ID</span>
	to identify different encodings of the same presentation. These variant
	playlists could contain additional <span class="code">EXT-X- STREAM-INF</span> tags.</td></tr>
	
	<tr><td><span class="code">CODECS</span></td><td><em>Optional</em>. Quoted string containing a comma-separated
	list of formats, where each format specifies a media sample type that is
	present in a media file in the playlist file. Valid format identifiers are
	those in the ISO File Format Name Space [<a href="#rfc-4281">34</a>].</td></tr>

	<tr><td><span class="code">RESOLUTION</span></td><td><em>Optional</em>. Decimal value describing the approximate
	encoded horizontal and vertical resolution of video within the stream.</td></tr>
	</table>
</div>

<div class="listing">
	<a name="lst:parse-apple"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.16</span> Java method to parse an extended M3U playlist
	(<span class="code">.m3u8</span>).
	</div>
<pre>
public void parse() throws MalformedURLException, IOException,
 InvalidPlaylistException, UnmodifiedMPDException {

   String filename = downloadManifest(new URL(playlist));
   FileReader reader = new FileReader(filename);
   Scanner scanner = new Scanner(reader);
   ...
   while (scanner.hasNextLine()) {
      ... // Analysis of each extended tag
   }
   sortSegmentLists();
}
</pre>
</div>

<h3><a name="sec:segment-downloader-module"></a><a name="sec:playlist"></a><a name="sec:re-download"></a><a name="4.4.6"></a>4.4.6 Segment-downloader module</h3>

<p>The segment-downloader module is responsible for opening HTTP connections to
fetch the available segments. It sends HTTP requests and waits for the
corresponding reply, checking the HTTP headers and response codes in the reply.
Once the connection is opened, the received byte stream is transferred to a
<em>buffer</em> to store the media files (in this prototype, the cellphone's
MicroSD card &ndash; a so-called external storage device &ndash; acts as the
buffer<a href="#footnote26" name="note26"><sup>26</sup></a>).
Although the Android system natively supports HTTP progressive streaming, it is
necessary to pre-download the segments in order to avoid pauses once
playback has started. Immediately after a new segment is stored in the buffer,
this module performs two actions:</p>

<ol>
	<li>It converts the media file into a supported format, if necessary. This
	occurs when Apple-HLS segments are contained in the MPEG-TS
	(<span class="code">.ts</span>) format, as they cannot be directly played by Stagefright.
	This task will be performed by the <em>transcoder module</em>, further explained
	in section <a href="#sec:transcoder-module">4.1.1</a>.</li>
	
	<li>It tests the media file using a background (<em>fake</em>) player, which
	is not bound to any <span class="code">SurfaceView</span>. This fake player simply prepares the
	segment and triggers the <span class="code">onPrepared()</span> and <span class="code">onError()</span> listeners. If
	there is an error preparing a particular segment, then the module will try to
	download this segment one more time. If an error persists, the software will
	skip this segment. If the buffer is not empty, the next segment is played.
	Otherwise, a <em>buffering animated wheel</em> is shown to the user to indicate
	that the playback has paused.</li>
</ol>

<p>Video fragments that are successfully downloaded are added to a
<em>playlist</em>, as proposed in [<a href="#stockhammer">69</a>]. This list is represented as
a table with two columns: estimated (relative) playback time and filename
(<em>path</em>) to the media segments. A simple example is shown in
table <a href="#tab:playlist-example">4.11</a>. The timing column is calculated based upon
the information provided by the parsing module.</p>

<div class="table">
	<a name="tab:playlist-example"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.11</span> Playlist example.
	</div>

	<table>
		<tr class="header">
			<td>Time</td><td>Segment URL</td>
		</tr>
		<tr><td>0.00</td><td>/media/sdcard/tmp/...segment1.mp4</td></tr>
		<tr><td>10.00</td><td>/media/sdcard/tmp/...segment2.mp4</td></tr>
		<tr><td>20.00</td><td>/media/sdcard/tmp/...segment3.mp4</td></tr>
	</table>
</div>

<p>This module also provides a segment <em>re-download mechanism</em>. Segments whose
download time increases greatly, must be immediately discarded. This is one of
the indicators of an inadequate representation level and the
<em>rate-adaptation module</em> must be notified, (the information that will be
shared between the segment-downloader and the rate-adaptation modules is
depicted in figure <a href="#fig:buffer-logic-communication">4.13</a>). Equation <a href="#eq:timeout">4.3</a>
indicates the maximum download time or <em>timeout</em> (<em>T<sub>timeout</sub></em>) where
<em>S<sub>d</sub></em> represents the duration of the segment. In this prototype, the timeout
value is fixed at 80% of the segment duration, which provides enough tolerance
to enable a segment to be downloaded and potentially be downloaded a second time
at a lower bit-rate.</p>

<div class="equation">
	<a name="eq:timeout"></a>
	<table>
	<tr>
	  <td>
		T<sub>timeout</sub> =  80 &times; S<sub>d</sub> <span class="equationnum">(4.3)</span>
	  </td>
	</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:buffer-logic-communication"></a>
	<img src="img/buffer-logic-communication.png" alt="Communication between the segment-downloader and the rate-adaptation
  modules."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.13</span> Communication between the segment-downloader and the rate-adaptation
  modules. Dashed lines represent requests whereas normal lines represent
  notifications.
	</div>
</div>

<p>A client's <em>throughput</em> metric (<em><span class="symbol">t</span></em>, measured in bits per second) has
been defined to inform the rate-adaptation module right after every segment is
downloaded of the recently experienced HTTP connection throughput. Throughput
at the client's side is calculated as:</p>

<div class="equation">
	<a name="eq:measured-bandwidth"></a>
	<table>
		<tr>
			<td>    
				<span class="symbol">t</span> = 
			</td>
			<td>
				S<sub>i</sub>
				<div class="hrcomp"><hr/></div>
				T<sub>i</sub>
			</td>
			<td>
				<span class="equationnum">(4.4)</span>
			</td>
		</tr>
	</table>
</div>

<p>Where <em>S<sub>i</sub></em> is the size in bits of the segment <em>i</em> and <em>T<sub>i</sub></em> is the time
it took to download it, measured in seconds.</p>

<h4><a name="4.4.6.1"></a>4.4.6.1 Implementation</h4>

<p>The segment-downloader module is implemented as a background task (i.e., a
secondary thread executing as an infinite loop), as shown in the
listing <a href="#lst:buffer-run">4.17</a>. First, a call to <span class="code">fillBuffer()</span> in the
<span class="code">rateLogic</span> (rate adaptation module) determines whether a new segment
should be downloaded to the buffer. If so, the destination URL will
be requested by means of the <span class="code">getNextSegment()</span> method. Two situations could
arise: (1) there is already a new segment available to download or (2) there
is not, in which case the loop continues in case of <em>live</em> content or ends in
case of video <em>on-demand</em>. Note that this module only downloads segments
given a URL. The decisions about the appropriate representation level are
managed in the <em>rate-adaptation module</em>.</p>

<div class="listing">
	<a name="lst:buffer-run"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.17</span> Main method of the segment-downloader module.
	</div>
<pre>
public void run() {

   while (running) {
      try {
         if (rateLogic.fillBuffer()) {
            URL url = rateLogic.getNextSegment();
            if (url != null) {
               try {
                  downloadSegment(url); // Fetch next segment
                  rateLogic.addEstimatedTime(); // Notify rate-adaptation module
                  storedSegments++;
                  playList.isReadyToPlay(); // Update playlist status
                  bufferBarUpdate();
               } catch (DownloadAgainException e) { ...
               } finally {
                  rateLogic.adapt(); // Perform adaptation in the rate-adaptation module
               }
            } else {
               if (playList.isLive()) continue; // Infinite loop for live content
               else break;
            }
         }
      } catch (...) {} // Catch exceptions
   }
   playList.setEnd();
}
</pre>
</div>

<p>If there is a new segment available, then this module calls the
<span class="code">downloadSegment()</span> method. The first step is the initialization
of the HTTP connection, which is shown in
listing <a href="#lst:http-connection">4.18</a>. The code must check that the HTTP responses
have correct headers and a valid response code. If there is an error, then the
code attempts for  a limited number of times (modeled as <span class="code">MAX_ATTEMPS</span> and
set to five in this implementation) to download the segment again - until a
valid HTTP code is returned.</p>

<div class="listing">
	<a name="lst:http-connection"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.18</span> Opening a valid HTTP connection.
	</div>
<pre>
while (attempt &lt; MAX_ATTEMPTS) {
   connection = (HttpURLConnection) url.openConnection();
   connection.setConnectTimeout(TIMEOUT);
   connection.setDoInput(true);
   connection.connect();

   int responseCode = connection.getResponseCode(); // Obtain HTTP response code
   if (responseCode == HttpURLConnection.HTTP_OK)
      break;
   if (responseCode == HttpURLConnection.HTTP_NOT_FOUND)
      throw new IOException("HTTP 404 not found");
   attempt++;
   connection.disconnect();
}
if (connection.getResponseCode() != HttpURLConnection.HTTP_OK)
   return; // Exit if HTTP response code is not HTTP-200
... // Read the Content-Length header field
connection = (HttpURLConnection) url.openConnection();
</pre>
</div>

<p>Listing <a href="#lst:download-segment">4.19</a> illustrates the management of the flow of
bytes. The byte stream obtained from the HTTP connection (produced by an
instance of <span class="code">BufferedInputStream</span>) is redirected to the external storage (instance of
<span class="code">FileOutputStream</span>). In order to provide maximum timing accuracy, timing
measurements are performed at the beginning of the byte transfers. A
<em>timeout</em> (as explained above) is defined in this implementation as
<span class="code">MAX_ALLOWED_TIME</span> and is used as the criteria of when to <strong>discard</strong>
and re-download segments that could not be retrieved in time.</p>

<div class="listing">
	<a name="lst:download-segment"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.19</span> Procedure to download a new media segment.
	</div>
<pre>
private void downloadSegment(URL url) throws ... {
   String filename = tempPath + getFileName(url.toString());
   if (!new File(filename).exists()) {
      ... // Init HTTP connection
      bis = new BufferedInputStream(connection.getInputStream(), BYTE_BLOCK);
      fos = new FileOutputStream(filename);
      ... // Start timer

      while ((count = bis.read(data, 0, data.length)) != -1) {
         ... // Stop downloading if player is closed
         if (Timer.getByteFlowTime() > MAX_ALLOWED_TIME) {
            if (rateLogic.shouldDownloadAgain()) {
               ... // Calculate availBW and notify Logic module
               throw new DownloadAgainException(...);
            }
         }
		 fos.write(data, 0, count);
         bytes += count;
      }
      ... // Close streams

      try {
         availBW = (int) ((totalBytes * 8 * 1000) / Timer.endByteFlow());
         rateLogic.setAvailBandwidth(availBW);
      } catch (ArithmeticException e){}

      if (Player.isM3U8) // Perform MPEG-TS conversion if necessary
         filename = Transcoder.convertToMp4(filename);

      try {
         new FakePlayer(filename); // Test downloaded segment in FakePlayer
      } catch (DownloadAgainException e) {
         rateLogic.retryOnError();
         throw new DownloadAgainException(e.getMessage());
      }
      playList.add(filename); // Add segment to the playlist
   }
}
</pre>
</div>

<p>Once the media file has been correctly downloaded, the segment-downloader module
provides a new bandwidth measurement (equation <a href="#eq:measured-bandwidth">4.4</a>) to
the rate-adaptation module. Notification is also performed when discarding
segments, which forces the selection of the appropriate quality level before
fetching another piece of content. Finally, the segment's path is added to the
playlist, which is implemented as a Java <span class="code">List&lt;MediaSegment></span>, where
<span class="code">MediaSegment</span> objects consist of a estimated playback time-stamp and a file
path.</p>


<h3><a name="sec:rate-logic-module"></a><a name="4.4.7"></a>4.4.7 Rate adaptation module</h3>

<p>This module can be considered the core of the adaptation logic. It receives
information about the estimated playback time and the measured bandwidth from
the segment-downloader module (as explained in
section <a href="#sec:segment-downloader-module">4.4.6</a>). In addition, the rate-adaptation
module has access to the different playlists generated by the parsing module
(see section <a href="#sec:parser">4.4.5</a>). Based upon all of this information the
rate-adaptation module runs the adaptation algorithm to decide which
representation level is the optimal one. In this module, the aggressive,
conservative, and mean algorithm (proposed in section <a href="#sec:algorithms">4.4.2</a>) have
been implemented.</p>

<h4><a name="4.4.7.1"></a>4.4.7.1 Implementation</h4>

<p>In terms of Java programming, the rate-adaptation module is a submodule of the
segment-downloader, since they execute in the same thread. The algorithms
proposed in the previous sections rely on actions that increase or decrease the
representation level. Hence, four Java methods (listing <a href="#lst:switching">4.20</a>)
have been developed to provide this functionality. <span class="code">switchUp()</span> and
<span class="code">switchDown()</span> receive a parameter indicating the number of level switching
steps. The implementation of the switch-up method is shown in
listing <a href="#lst:switch-up">4.21</a>.</p>

<div class="listing">
	<a name="lst:switching"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.20</span> Java methods used to switch the representation level.
	</div>
<pre>
private void switchUp(int levels);
private void switchDown(int levels);
private void switchMaxUp();
private void switchMinDown();
</pre>
</div>

<div class="listing">
	<a name="lst:switch-up"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.21</span> Switch-up method. Increase the selected representation
	level. The number of steps to increase is passed as a parameter.
	</div>
<pre>
private void switchUp(int levels) {

   while (levels > 0 &amp;&amp; !isMaxBandwidth) {
   try {
      /* Try to access the next representation list */
      segmentLists.get(currentId + levels);
      /* Success, update the pointers */
      currentId += levels;
      currentList = segmentLists.get(currentId);
      setBandwidths();
      break;
   } catch (IndexOutOfBoundsException e) {}
      levels--;
   }
}
</pre>
</div>

<p>The process of adaptation is performed after the download of every segment. The
segment-downloader module calls the <span class="code">adapt()</span> method within the
rate-adaptation module. Listing <a href="#lst:adapt">4.22</a> shows a simplified version of
the Java code, only the <em>adaptive (aggressive)</em> algorithm is shown in
this example. The remainder of the implementation follows the algorithms.</p>

<div class="listing">
	<a name="lst:adapt"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.22</span> Adaptation Java method. It selects the appropriate
representation level for each algorithm.
	</div>
<pre>
public void adapt() {
   switch (logic) { // Apply rate algorithm according to adaptive profile

      case PROGRESSIVE:
         ... // Null adaptation, testing purposes

      case ADAPTIVE_CONSERVATIVE:
         ... // Implementation of the conservative adaptive algorithm

      case ADAPTIVE_CONSERVATIVE:
         if (playList.isEmpty()) {
            switchMinDown();
         } else {
            int throughput = (int) (measuredThroughput * SENSITIVITY);
            if (throughput > currentBW) {
               while (nextBW &lt; throughput &amp;&amp; !isMaxBandwidth)
                  switchUp(1);
            } else {
               do {
                  switchDown(1);
               } while (prevBW > throughput &amp;&amp; !isMinBandwidth);
            }
         }
         break;

      case ADAPTIVE_MEAN:
         ... // Implementation of the mean adaptive algorithm
   }
   ... // Prevent re-download a segment at the same quality twice
}
</pre>
</div>

<h3><a name="sec:transcoder"></a><a name="4.4.8"></a>4.4.8 Transcoder module</h3>

<p>Media segments are fetched and allocated in the Android device's external
storage, as was previously explained in
section <a href="#sec:segment-downloader-module">4.4.6</a>. In case of the MPEG-DASH standard,
media formats produced by the HTTP servers are fully supported in Stagefright,
making the segments suitable for playback. However, the Apple-HLS specification
contemplated only MPEG-TS as a media container format. Unfortunately, this
container format is not natively supported in Stagefright, hence a transcoder
is utilized to perform the necessary conversion.</p>

<p>The transcoder module solves the compatibility problem by providing additional
processing for Apple's HLS media content. Fortunately, most Apple-HLS sources
contains streams encoded with H.264 (baseline profile) and AAC, thus the
transcoder only needs to change the MPEG-TS container
into one of the Android's supported formats (listed in
table <a href="#tab:android-video-formats">2.4</a>).
Figure <a href="#fig:container-conversion">4.14</a> illustrates the necessary transformation
for compatibility. Since this procedure consumes a considerable
amount of CPU cycles<a href="#footnote27" name="note27"><sup>27</sup></a>, it <strong>must</strong> be performed within a bounded period of time to avoid
interrupting the playback (see experiments in section <a href="#sec:live-tv">5.7</a>). As an
absolute maximum upperbound, the <em>overall execution</em> of the operations
performed in the buffering and transcoder modules may never take longer that
the previous segment's duration, as expressed in
equation <a href="#eq:transcoder-maximum-general">4.5</a>.</p>

<div class="equation">
	<a name="eq:transcoder-maximum-general"></a>
	<table>
	<tr>
	  <td>
		T(i)<sub>download</sub> + T(i)<sub>conversion</sub> &lt; T(i<span class="code">-</span>1)<sub>duration</sub>
		<span class="equationnum">(4.5)</span>
	  </td>
	</tr>
	</table>
</div>

<p>Assuming that all segments are of the same length,
equation <a href="#eq:transcoder-maximum-general">4.5</a> can be expressed as:</p>

<div class="equation">
	<table>
	<tr>
	  <td>
		T<sub>download</sub> + T<sub>conversion</sub> &lt; T<sub>duration</sub>
		<span class="equationnum">(4.6)</span>
	  </td>
	</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:container-conversion"></a>
	<img src="img/ts-to-mp4-conversion.png" alt="Media container conversion performed in the transcoder module."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.14</span> Media container conversion performed in the transcoder module. It
  provides compatibility with Apple-HLS.
	</div>
</div>

<h4><a name="4.4.8.1"></a>4.4.8.1 Implementation</h4>

<p>This module makes use of the FFmpeg audio and video libraries. Since they are
written in the C and C++ programming languages, a binding is needed to invoke
the proper functions from Android's Java standard code. This binding is achieved
by means of the Java Native Interface (JNI). Fortunately a Native Development Kit
(NDK)<a href="#footnote28" name="note28"><sup>28</sup></a> is offered at the official Android
developers site, which provides several tools to link Java code to pieces of
native code. The basic Android application model does not change, since the NDK
works in combination with the Android's SDK (introduced
in section <a href="#sec:android">2.10</a>). The integration of the FFmpeg libraries can be
found in the appendix <a href="#app:integration">C</a>.</p>

<p>Listing <a href="#lst:jni-exception">4.23</a> describes how exceptions can be thrown from the
native code to the Java activities to notify them of an exception in the native code.</p>

<div class="listing">
	<a name="lst:jni-exception"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.23</span> C function which throws a Java exception via JNI.
	</div>
<pre>
int jniThrowException(JNIEnv* env, const char* className, const char* msg) {
   
   jclass exceptionClass = env->FindClass(className);
   if (exceptionClass == NULL)
      return -1;
   env->ThrowNew(exceptionClass, msg)
   return 0;
}
</pre>
</div>

<p>A set of native functions have been defined in a Java <span class="code">FFmpeg</span> class: the
most important of these are shown in listing <a href="#lst:natives">4.24</a>. These methods
provide a basic interface to prepare the conversion. The first two
functions, <span class="code">native_avcodec_register_all()</span> and
<span class="code">native_av_register_all()</span>, initialize all the media formats and CODECs
that have been enabled when FFmpeg was compiled. Input and output file names
are set with <span class="code">native_av_setInputFile()</span> and
<span class="code">native_av_setOutputFile()</span>, respectively. The output CODECs are set by
calling the <span class="code">native_av_setVideoCodec()</span> and the
<span class="code">native_av_setAudioCodec()</span> functions. In the FFmpeg notation, media
streams can remain unaltered if <span class="code">copy</span> is indicated as the video and audio
CODECs, in this case changes will be made to the container format, which is the
main purpose of this module in our current prototype.</p>

<div class="listing">
	<a name="lst:natives"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.24</span> A particular set of native functions defined in the
<span class="code">FFmpeg</span> class.
	</div>
<pre>
/* Codec initialization */
private native void native_avcodec_register_all();
private native void native_av_register_all();
private native void native_av_init() throws RuntimeException;

/* Input parameters */
private native FFmpegAVFormatContext native_av_setInputFile(String filePath)
 throws IOException;
private native FFmpegAVFormatContext native_av_setOutputFile(String filePath)
 throws IOException;
private native void native_av_setVideoCodec(String codec);
private native void native_av_setAudioCodec(String codec);
private native void native_av_parse_options(String[] args)
 throws RuntimeException;

/* Main conversion method */
private native void native_av_convert()
 throws RuntimeException;

/* Release resources */
private native int native_av_release(int code);
</pre>
</div>

<p>The initialization procedure is specified in listing <a href="#lst:ffmpeg-init">4.25</a>,
where the input options are allocated. Input parameters are parsed in the
<span class="code">FFmpeg_parseOptions()</span> native function, which has been simplified in
listing <a href="#lst:parse-options">4.26</a>.</p>

<div class="listing">
	<a name="lst:ffmpeg-init"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.25</span> Initialization native function.
	</div>
<pre>
static void FFmpeg_init(JNIEnv *env, jobject obj) {

   sObject = (*env)->NewGlobalRef(env, obj);
   jclass clazz = (*env)->GetObjectClass(env, obj);
   int i=0;
   
   for(i=0; i&lt;AVMEDIA_TYPE_NB; i++){
      avcodec_opts[i]= avcodec_alloc_context2(i);
   }
   avformat_opts = avformat_alloc_context();
   sws_opts = sws_getContext(16, 16, 0, 16, 16, 0, sws_flags, NULL, NULL, NULL);
}
</pre>
</div>

<div class="listing">
	<a name="lst:parse-options"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.26</span> Parsing native function to obtain conversion parameters.
	</div>
<pre>
static void FFmpeg_parseOptions(JNIEnv *env, jobject obj, jobjectArray args) {

   ... // Init variables
   if (args != NULL) {
      argc = (*env)->GetArrayLength(env, args);
      argv = (char **) malloc(sizeof(char *) * argc);
      for(i=0;i&lt;argc;i++) {
         jstring str = (jstring)(*env)->GetObjectArrayElement(env, args, i);
         argv[i] = (char *)(*env)->GetStringUTFChars(env, str, NULL);
      }
   }
   parse_options(argc, argv, options, opt_output_file); // Parse options

   /* Check input and output files */
   if(nb_output_files &lt;= 0 &amp;&amp; nb_input_files == 0)
      jniThrowException(env, ...);
   if (nb_output_files &lt;= 0)
      jniThrowException(env, ...);
   if (nb_input_files == 0)
      jniThrowException(env, ...);
}
</pre>
</div>

<p>Finally, listing <a href="#lst:convert">4.27</a> illustrates <span class="code">FFmpeg_transcode()</span>, where
the fundamental FFmpeg function <span class="code">av_transcode()</span> is invoked. Calling this
function implies that all input parameters have been properly set without
throwing any <span class="code">JniException</span>.</p>

<div class="listing">
	<a name="lst:convert"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.27</span> Convert native function.
	</div>
<pre>
static void FFmpeg_transcode(JNIEnv *env, jobject obj, jlong outputFile,
 jlong inputFile) {

   ...
   if (av_transcode(output_files, nb_output_files, input_files, nb_input_files,
    stream_maps, nb_stream_maps) &lt; 0)
      jniThrowException(env, ...);
}
</pre>
</div>

<p>The Java method <span class="code">convertTo()</span> (simplified in listing <a href="#lst:java-convert">4.28</a>)
wraps all the native functions previously explained, providing a simple
<em>shortcut</em> to utilize the FFmpeg libraries from any Android activity. This
procedure is depicted in figure <a href="#fig:ffmpeg-convert">4.15</a>. The whole operation
can be simplified into three simple steps:</p>

<div class="listing">
	<a name="lst:java-convert"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.28</span> Java conversion method.
	</div>
<pre>
private static String convertTo(String inputFile, String extension)
 throws FFmpegException, RuntimeException, IOException {

   ffmpeg = new FFmpeg(); // Prepare conversion parameters
   ffmpeg.setVideoCodec("copy"); // Only conversion of the container format
   ffmpeg.setAudioCodec("copy");
   ffmpeg.init(inputFile, outputFile);
   ffmpeg.convert(); // Start format conversion
   return outputFile;
}
</pre>
</div>

<ol>
	<li>The Java <span class="code">convertTo()</span> method is invoked after downloading a segment
	in the segment-downloader module (as was explained in
	section <a href="#sec:segment-downloader-module">4.4.6</a>). The function <span class="code">convertTo()</span>
	internally calls the native function <span class="code">native_av_convert()</span>.</li>
	
	<li>JNI launches the corresponding C function (<span class="code">FFmpeg_transcode()</span>,
	listing <a href="#lst:convert">4.27</a>) which makes use of the FFmpeg libraries.</li>
	
	<li>The FFmpeg <span class="code">av_transcode()</span> function is started. There are two
	possible outcomes:
		
		<ul>	
			<li>Conversion was successfully performed, thus a new file has been
			created in the Android device's external storage and this file should be
			properly added to the <em>playlist</em>.</li>
			
			<li> A failure occurred in some point in the conversion procedure
			(<span class="code">av_transcode()</span> returned -1). In this case a <span class="code">JniException</span> is
			thrown up to the Java layer, which is viewed as a <span class="code">RuntimeException</span>.</li>
		</ul>
	</li>
</ol>

<div class="figure">
	<a name="fig:ffmpeg-convert"></a>
	<img src="img/ffmpeg-convert.png" alt="FFmpeg conversion interface."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.15</span> FFmpeg conversion interface.
	</div>
</div>

<h3><a name="sec:timer-module"></a><a name="4.4.9"></a>4.4.9 Timer module</h3>

<p>The timer module provides accurate timing information about multiple events that
take place in the client application. The following measurements are
essential for our evaluation of the system:</p>

<ul>
	<li><strong>Start-up delay</strong> time it takes to start the playback.</li>
	
	<li><strong>Download delay</strong> time required to download a single media
	segment.</li>
	
	<li><strong>Absolute playback time</strong> starting playback time of a single
	segment, expressed in absolute time (based upon being synchronized with a NTP
	server).</li>
	
	<li><strong>Load</strong> time it takes to prepare the visualization of a single
	segment to be played until it is shown on the screen.</li>
	
	<li><strong>Parsing delay</strong> time it takes to receive and parse the whole
	playlist since it is requested.</li>
	
	<li><strong>Conversion delay</strong> time spend in transcoding or conversion
	operations (currently this is only applicable for Apple-HLS, as described in
	section <a href="#sec:transcoder-module">4.1.1</a>).</li>
	
	<li><strong>Inactivity</strong> interval of time when the client is inactive, i.e., when all
	available segments have been fetched.</li>
	
	<li><strong>Pause</strong> time the player spends stopped during playback because
	the buffer is empty.</li>
	
	<li><strong>Session lifetime</strong> total duration of the session, i.e., the time since the
	system was started until it is shutdown.</li>
</ul>

<p>The timer module is synchronized with a pool of NTP servers, as introduced in
section <a href="#sec:synchronization">4.2</a>. Table <a href="#tab:ntp-settings">4.12</a> summarizes the
most significant SNTP parameters used in this implementation.</p>

<div class="table">
	<a name="tab:ntp-settings"></a>
	<div class="caption">
		<span class="captionTitle">Table 4.12</span> NTP settings.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>NTP server</td><td>NTP port</td><td>Version</td><td>Mode</td><td>NTP timeout (s)</td><td>NTP update (s)</td>	
		</tr>
		<tr>
			<td><span class="code">europe.pool.ntp.org</span></td><td>123</td><td>3</td><td>3</td><td>15</td><td>30</td>
		</tr>
	</table>
</div>

<h4><a name="4.4.9.1"></a>4.4.9.1 Implementation</h4>

<p>A <span class="code">sntpClient</span> class has been created to request timing information from an
NTP server. A NTP <em>datagram</em> is sent to the server and port specified in
table <a href="#tab:ntp-settings">4.12</a>, indicating the current system time in the
<span class="code">OriginateTimeStamp</span> header field. Calculation of the <em>offset</em> time
between the cellphone's internal clock and the NTP server's clock is shown in
listing <a href="#lst:request-ntp-time">4.29</a>.</p>


<div class="listing">
	<a name="lst:request-ntp-time"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.29</span> Request NTP time.
	</div>
<pre>
private static boolean requestNTPTime() {

   if (ntpClient.requestTime(NTP_SERVER, NTP_TIMEOUT)) {
      /* Calculate offset */
      offsetNTP = ntpClient.getNtpTime() + SystemClock.elapsedRealtime()
         - ntpClient.getNtpTimeReference()
         - System.currentTimeMillis();
      return true;
   }
   return false;
}
</pre>
</div>

<p>The first NTP request is sent upon initialization of this module. Consequently,
a background task is started to make future requests to the NTP servers (see
listing <a href="#lst:ntp-init">4.30</a>). This module offers two types of timing
metrics:</p>

<ul>
  <li><strong>Absolute time</strong> calculated as the sum of the system's internal
  time and the NTP received offset.</li>
  
  <li><strong>Time intervals</strong> based strictly on the system's
  internal clock, computed as the difference between time-stamps.</li>
</ul>

<div class="listing">
	<a name="lst:ntp-init"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.30</span> NTP initialization.
	</div>
<pre>
public static void init() {
   
   time = System.currentTimeMillis(); // System’s local clock
   ... // Request NTP time and calculate offset
   ... // Set all counters to zero

   new Thread(new Runnable() { // Init NTP update in a background task

      @Override
      public void run() {
         Looper.prepare();
         ntpHandler = new Handler();
         ntpHandler.postDelayed(NTPUpdate, NTP_UPDATE);
         Looper.loop();
      }
   }).start();
}
</pre>
</div>

<h2><a name="sec:network-emulator"></a><a name="4.5"></a>4.5 Network emulator</h2>

<p>A prototype of the media player presented in section <a href="#sec:client">4.4</a> has been
developed to provide network adaptation. Rate adaptation implies that the
client <em>reacts</em> in a reasonable time to changes in the network conditions.</p>

<h3><a name="4.5.1"></a>4.5.1 Emulator requisites</h3>

<p>In order to evaluate the capabilities of the application (as will be done in
chapter <a href="#chap:evaluation">5</a>), a network simulator was interposed between the
client and the server to control the characteristics of the HTTP traffic. This
emulator must do at least the following: (1) perform bandwidth limitation
(bidirectional, and optionally, unidirectional), (2) produce packet loss with a
given probability or error rate, and (3) induce appropriate packet delays.</p>

<p><em>Dummynet</em> [<a href="#dummynet-planetlab">20</a>, <a href="#dummynet-revisited">21</a>] has been selected
as the evaluation tool since it satisfies the above requirements and it is
easily integrated in the system<a href="#footnote29" name="note29"><sup>29</sup></a>. Dummynet's is briefly explained in the following
section, along with the functions that will be used during the evaluation (see
the <em>network scenarios</em> defined in section <a href="#sec:network-scenarios">5.1.7</a>).
There are multiple alternative emulation tools available, but they present some
integration difficulties:</p>

<ul>
	<li><strong>WANem</strong><a href="#footnote30" name="note30"><sup>30</sup></a> is a Wide Area Network (WAN) emulator,
	which provides several tools to perform experiments in a Local Area Network
	(LAN) environment. WANem can be used to emulate different WAN conditions such
	as packet loss, packet re-ordering, or jitter. However, WANem has two
	disadvantages for integration in our test environment: (a) WANem is intended to
	be deployed with <em>ethernet</em> interfaces, but the communication between
	client and server is performed over a wireless interface) and (b) unfortunately,
	WANem is only offered as a Gnu/Linux (Knoppix) live distribution, which makes
	scheduled scripting more difficult, since it requires a separate machine
	running the emulator.</li>
	
	<li><strong>Trickle</strong><a href="#footnote31" name="note31"><sup>31</sup></a> is a Gnu/Linux
	bandwidth shaper which limits traffic over a socket. The <span class="code">trickle</span> command
	simply precedes any other command in order to limit bandwidth in this
	application. An example of this would be: <span class="code">trickle [parameters]
	application</span>. Since both on-demand and live server (described in
	sections <a href="#sec:on-demand-server">4.3.1</a> and section <a href="#sec:live-server">4.3.2</a>) have
	been deployed as Gnu/Linux <em>daemons</em>, <span class="code">trickle</span> cannot be easily
	invoked several times during the servers' execution<a href="#footnote32" name="note32"><sup>32</sup></a>. Trickle does not
	support modification of the emulation parameters on-the-fly, therefore servers
	will have to be restarted for every variation in the emulation scenario. Since
	the live server requires a considerable<a href="#footnote33" name="note33"><sup>33</sup></a> time to inspect all segments during its
	initialization, it should not be restarted for
	every variation in the intended emulation.</li>
</ul>
	

<h3><a name="sec:dummynet"></a>	<a name="4.5.2"></a>4.5.2 Dummynet</h3>

<p>Dummynet is the network emulator deployed for our evaluation. For simplicity,
Dummynet was installed on the same Gnu/Linux machine as the HTTP server.
Dummynet intercepts the packets within the protocol stack, as shown in
figure <a href="#fig:dummynet-stack">4.16</a>. These packets are selected according to
different rules, which have been previously specified by means of the <span class="code">ipfw</span>
shell command.</p>

<div class="figure">
	<a name="fig:dummynet-stack"></a>
	<img src="img/dummynet-stack.png" alt="Dummynet introduces one or more pipes and queues in the protocol
  stack."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.16</span> Dummynet introduces one or more pipes and queues in the protocol
  stack. Packets are intercepted and delayed according to the set of network
  rules. Adapted from [<a href="#dummynet-revisited">23</a>, figure 3].
	</div>
</div>

<p>The intercepted packets are passed through one or more queues and pipes
(as depicted in figure <a href="#fig:dummynet-pipe">4.17</a>) which emulate propagation
delays, limitations of available bandwidth, and packet loss. Pipes behave as
fixed-bandwidth channels and queues can be weight-assigned.</p>

<p>Network rules can be easily defined. These rules establish a
<em>networking profile</em>. Several profiles were proposed for the experiments
and measurements described in the next chapter. For example,
listing <a href="#lst:dummynet-pipe">4.31</a> shows how to define a simple pipe rule
that restricts the available bandwidth to 2 Mb/s and introduces a packet delay
of 1000 ms. Listing <a href="#lst:dummynet-pipe-complex">4.32</a> shows a more
complex example, where both <em>outcoming</em> and <em>incoming</em> traffic have
different bit-rates, packet delay, and packet loss specifications.</p>

<div class="figure">
	<a name="fig:dummynet-pipe"></a>
	<img src="img/dummynet-pipe.png" alt="Pipes and queues, the basic elements of Dummynet."/>
	<div class="caption">
		<span class="captionTitle">Figure 4.17</span> Pipes and queues, the basic elements of Dummynet. Adapted from
  [<a href="#dummynet-revisited">23</a>, figure 3].
	</div>
</div>

<div class="listing">
	<a name="lst:dummynet-pipe"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.31</span> A simple pipe created in Dummynet. The pipe
bidirectionally limits the network traffic to 2 Mb/s and induces a packet delay
of 1000 ms.
	</div>
<pre>
# Define bandwidth and delay of the emulated link
ipfw pipe 1 config bw 2Mbit/s delay 1000ms
# Pass all traffic through the emulator
ipfw add pipe 1 ip from any to any
</pre>
</div>

<div class="listing">
	<a name="lst:dummynet-pipe-complex"></a>
	<div class="caption">
		<span class="captionTitle">Listing 4.32</span> A more complete example of pipes in Dummynet. Incoming
	network traffic is limited to 512 kb/s with a delay of 32 ms and 1%
	probability of error; whereas the outcoming traffic is limited to 256 kb/s,
	100 ms of packet delay, and 5% of packet loss.
	</div>
<pre>
# Define pipes for incoming and outcoming network traffic
ipfw add pipe 3 out
ipfw add pipe 4 in
# Pipes configuration
ipfw pipe 3 config bw 512Kbit/s delay 32ms plr 0.01
ipfw pipe 4 config bw 256Kbit/s delay 100ms plr 0.05
</pre>
</div>

<h1><a name="chap:evaluation"></a><a name="5"></a>5 Evaluation</h1>

<div class="quoteblock">
	<p class="quote"><em>"Premature optimization is the root of all evil."</em></p>
	<p class="quoteAuthor">- Donald Knuth</p>
</div>

<p>In this chapter a number of potential experiments are proposed to
evaluate the performance of the adaptive mechanisms presented in
section <a href="#sec:rate-logic-module">4.4.7</a>.</p>

<p>The evaluation environment is introduced in
section <a href="#sec:experimental-environment">5.1</a>, including the segmentation schemas
(defined in section <a href="#sec:segmentation-schemas">5.1.3</a>), the input/output
characterization (section <a href="#sec:input-output">5.1.5</a>), the metrics proposed
(section <a href="#sec:metrics">5.1.6</a>), and the network scenarios
emulated (section <a href="#sec:network-scenarios">5.1.7</a>). Results of the experiments are
described in section <a href="#sec:scenario1">5.2</a> to section <a href="#sec:live-tv">5.7</a>.</p>


<h2><a name="sec:experimental-environment"></a><a name="5.1"></a>5.1 Experimental environment</h2>

<p>Figure <a href="#fig:evaluation-environment">5.1</a>
illustrates the evaluation environment. Three components of the architecture
have been provided with input parameters:</p>

<ol>
  <li>The client's application provides several rate mechanisms:
  the <em>aggressive</em>, <em>conservative</em>, and <em>mean</em> algorithms (as
  introduced in section <a href="#sec:rate-logic-module">4.4.7</a>).</li>
  
  <li>A set of scenarios (explained in
  detail in section <a href="#sec:network-scenarios">5.1.7</a>) are emulated in the underlying network
  between server and client.</li>
  
  <li>The preparation of the content follows different
  <em>segmentation schemas</em> (section <a href="#sec:segmentation-schemas">5.1.3</a>). In
  particular, schemas might differ in media formats, CODECs, and duration of
  the segments.</li>  
</ol>

<div class="figure">
	<a name="fig:evaluation-environment"></a>
	<img src="img/evaluation-environment.png" alt="Evaluation environment with three different parametrized components."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.1</span> Evaluation environment with three different parametrized components
  (shown in gray).
	</div>
</div>

<p>Additionally, a real-world scenario has been included in this
evaluation. This scenario exploits the fact that several TV channels following
the Apple-HLS standard are freely available on the Internet (see
section <a href="#sec:live-tv">5.7</a>).</p>


<h3><a name="sec:devices"></a><a name="5.1.1"></a>5.1.1 Experimental devices</h3>

<p>Table <a href="#tab:devices">5.1</a> summarizes the specifications of the
devices utilized on the experiments. The same machine (a standalone netbook)
acts as HTTP server and prepares the content (as explained in
section <a href="#sec:content-preparation">4.1</a> and section <a href="#sec:server">4.3</a>). The client
is a mid-range smartphone running Android 2.2.1.</p>

<div class="table">
	<a name="tab:devices"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.1</span> Specifications of devices employed in the experiments. Extracted
   from [<a href="#s5830">63</a>].
	</div>
	
	<table>
		<tr class="header">
			<td></td><td>Client</td><td>Server</td>
		</tr>
		
		<tr><td>Name</td><td>Samsung Galaxy Ace S5830</td><td>Acer Aspire One D150</td></tr>

		<tr><td>CPU</td><td>800 MHz ARM 11 processor</td><td>1.60 GHz Intel Atom N270</td></tr> 
		
		<tr><td>Memory</td><td>RAM: 256 MB<br/>Internal:
   158 MB<br/>External: 2GB microSD</td><td>RAM: 1 GB DDR3 @ 667 MHz<br/>Storage: 160 GB SATA HDD</td></tr>
   
		<tr><td>OS</td><td>Android v2.2.1 (Froyo)</td><td>Kubuntu Gnu/Linux 2.6.36</td></tr>
   
		<tr><td>Network</td><td>2G: GSM 850/900/1800/1900<br/>3G: HSDPA 900/2100<br/>WLAN: Wi-Fi 802.11 b/g/n</td><td>WLAN: Wi-Fi 802.11 b/g/n</td></tr>
   
		<tr><td>Display</td><td>TFT HVGA touchscreen. 480 &times; 320 px</td><td>- </td></tr>
		
	</table>
</div>


<h3><a name="sec:content-source"></a><a name="5.1.2"></a>5.1.2 Content source</h3>

<p>The 3D animation film <em>Sintel</em> [<a href="#sintel">16</a>] has been selected for this
evaluation, since is free-distributable under the Creative Commons Attribution
(CC-A) license. Sintel is a 15-minute movie produced by the Blender Foundation
and created entirely with open source software. It was released at the
Netherlands Film Festival in September 2010.</p>


<h3><a name="sec:segmentation-schemas"></a><a name="5.1.3"></a>5.1.3 Segmentation schemas</h3>

<p>The media content which is pushed to the HTTP servers can be offered in multiple
ways. Our evaluation focus on two essential aspects: the number of quality
levels (<em>R</em>) and the duration of the segments (<em>S<sub>d</sub></em>).
Table <a href="#tab:segmentation-schemas">5.2</a> defines the different segmentation
schemas used in our evaluation.</p>

<p>Each schema provides a different media segment duration <em>S<sub>d</sub></em>, from 5 s to
15 s. The <em>reference time</em> has been set to 10 s, as this duration was
recommended by Zambelli [<a href="#zambelli">74</a>] (see also
table <a href="#tab:http-comparison">2.6</a>). For
simplicity, a homogeneous but sufficient [<a href="#liu">49</a>, <a href="#longtail">50</a>] number of
representations has been provided in all schemas. Selection of the different
levels of quality is explained in section <a href="#sec:selection-levels">5.1.4</a>.</p>

<div class="table">
	<a name="tab:segmentation-schemas"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.2</span> Segmentation schemas.
	</div>
	
	<table class="center">
		<tr class="header">
			<td></td><td>Schema 1</td><td>Schema 2</td><td>Schema 3</td><td>Schema 4</td>
		</tr>
		<tr>
			<td><em>S<sub>d</sub></em> (s)</td><td>5</td><td>8</td><td>10</td><td>15</td>
		</tr>
		<tr>
			<td><em>R</em></td><td>10</td><td>10</td><td>10</td><td>10</td>
		</tr>
	</table>
</div>

<h3><a name="sec:selection-levels"></a><a name="5.1.4"></a>5.1.4 Selection of media quality levels</h3>

<p>Table <a href="#tab:android-encoding-recommendation">5.3</a> illustrates the
Android's official encoding recommendations for low and high quality media
content. For the sake of simplicity, in this evaluation the <em>frame rate</em>,
<em>audio channels</em>, <em>sampling rate</em>, <em>GOP size</em><a href="#footnote34" name="note34"><sup>34</sup></a>, and
<em>resolution</em> are set to typical values (as shown in
table <a href="#tab:fixed-parameters">5.4</a>), whereas video and audio bit-rates are
part of our design choices. Both audio and video bit-rates are increased in
every media representation that we have conducted experiments with.</p>

<div class="table">
	<a name="tab:android-encoding-recommendation"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.3</span> Official Android's encoding recommendations for low and high quality
  video. Extracted from the Android's developer site [<a href="#android-dev">6</a>].
	</div>
	
	<table>
		<tr class="header">
			<td>Parameter</td><td>Low quality video</td><td>High quality video</td>
		</tr>
		<tr>
			<td>Video resolution (px)</td><td>176 &times; 144</td><td>480 &times; 360</td>
		</tr>
		<tr>
			<td>Frame rate (fps)</td><td>12</td><td>30</td>
		</tr>
		<tr>
			<td>Video bit-rate (kb/s)</td><td>56</td><td>500</td>
		</tr>
		<tr>
			<td> Audio CODEC</td><td>AAC-LC</td><td>AAC-LC</td>
		</tr>
		<tr>
			<td>Audio channels</td><td>1 (mono)</td><td>2 (stereo)</td>
		</tr>
		<tr>
			<td>Audio bit-rate (kb/s)</td><td>24</td><td>128</td>
		</tr>
	</table>
</div>

<div class="table">
	<a name="tab:fixed-parameters"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.4</span> Set of fixed parameters used in all representations.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Container format</td><td>CODECs</td><td>Frame rate (fps)</td><td>GOP size</td><td>Resolution (px)</td>
		</tr>
		<tr>
			<td>MP4 (<span class="code">.mp4</span>)</td><td>H.264 BP + AAC</td><td>25</td><td>25</td><td>480 &times; 320</td>
		</tr>
	</table>
</div>

<p>Table <a href="#tab:representations">5.5</a> defines the quality levels which will be
provided to the HTTP servers, with increasing bit-rates from 80 kb/s to
2 Mb/s. In particular, these representations have been selected to supply a
higher density of levels for lower bit-rates, since this prototype is intended
for mobile networks that may have more limited bandwidth. Therefore, the
difference in rates between levels is not uniform. This selection of media
bit-rates leads to potentially large changes in quality levels by the rate
adaptation algorithms.</p>

<div class="table">
	<a name="tab:representations"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.5</span> Set of media representation levels generated on the streaming
 server.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Level (<em>r<sub>i</sub></em>)</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td>
		</tr>
		<tr>
			<td>Video bit-rate (kb/s)</td><td>56</td><td>70</td><td>100</td><td>150</td><td>200</td><td>400</td><td>700</td><td>900</td><td>1400</td><td>1900</td>
		</tr>
		<tr>
			<td>Audio bit-rate (kb/s)</td><td>24</td><td>24</td><td>44</td><td>64</td><td>94</td><td>94</td><td>94</td><td>94</td><td>94</td><td>94</td>
		</tr>
		<tr>
			<td>Audio channels</td><td>1</td><td>1</td><td>1</td><td>1</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td>
		</tr>
		<tr>
			<td>Sampling rate (kHz)</td><td>22.05</td><td>22.05</td><td>22.05</td><td>22.05</td><td>44.1</td><td>44.1</td><td>44.1</td><td>44.1</td><td>44.1</td><td>44.1</td>
		</tr>
		<tr>
			<td>Avg. bit-rate (kb/s)</td><td>80</td><td>100</td><td>150</td><td>200</td><td>300</td><td>500</td><td>800</td><td>1000</td><td>1500</td><td>2000 </td>
		</tr>
	</table>
</div>

<p>As an example of the video streams, figure <a href="#fig:frame-samples">5.2</a> shows a set
of frames from the same clip encoded at different video/audio bit-rates. For
simplicity, only four quality levels are represented: 80 kb/s (first
representation), 150 kb/s, 800 kb/s, and 2000 kb/s (last representation). Higher
rates assign more bits per image, resulting in a better quality.</p>

<div class="figure">
	<a name="fig:frame-samples"></a>
	<table class="center">
		<tr>
			<td><img src="img/output-80.jpg" alt="80 kb/s"/></td><td><img src="img/output-150.jpg" alt="150 kb/s"/></td>
		</tr>
		<tr>
			<td><strong>(a)</strong> 80 kb/s</td><td><strong>(b)</strong> 150 kb/s</td>
		</tr>
		<tr>
			<td><img src="img/output-800.jpg" alt="800 kb/s"/></td><td><img src="img/output-2000.jpg" alt="2000 kb/s"/></td>
		</tr>
		<tr>
			<td><strong>(c)</strong> 800 kb/s</td><td><strong>(d)</strong> 2000 kb/s</td>
		</tr>
	</table>
	
	<div class="caption">
		<span class="captionTitle">Figure 5.2</span> Frame samples from <em>Sintel</em> encoded at different bit-rates.
	</div>
</div>

<h3><a name="sec:input-output"></a><a name="5.1.5"></a>5.1.5 Input and output characterization</h3>

<p>Table <a href="#tab:input-parameters">5.6</a> and table <a href="#tab:output-parameters">5.7</a>
summarize, respectively, the input and output parameters that have been
considered in our evaluation.</p>

<div class="table">
	<a name="tab:input-parameters"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.6</span> Input parameters.
	</div>
	
	<table>
		<tr class="header">
			<td>Notation</td><td>Unit</td><td>Definition</td>
		</tr>
		<tr>
			<td><em>T</em></td><td>s</td><td>Total session time</td>
		</tr>
		<tr>
			<td><em>T<sub>shift</sub></em></td><td>s</td><td>Available shifting time on server</td>
		</tr>
		<tr>
			<td><em>T<sub>min-buf</sub></em></td><td>s</td><td>Minimum buffering time, specified on server</td>
		</tr>
		<tr>
			<td><em>S<sub>d</sub></em></td><td>s</td><td>Segment duration</td>
		</tr>
		<tr>
			<td><em>S</em></td><td>segments</td><td>Total number of segments in session. Calculated as <em>S = T / S<sub>d</sub></em></td>
		</tr>
		<tr>
			<td><em>T<sub>timeout</sub></em></td><td>s</td><td>Segment's downloading timeout. <em>T<sub>timeout</sub> = S<sub>d</sub> &times; 80%</em> </td>
		</tr>
		<tr>
			<td><em>R</em></td><td>levels</td><td>Total number of representations offered on the
   server</td>
		</tr>
		<tr>
			<td><em>r<sub>i</sub></em></td><td>b/s</td><td>Ordered representation levels by bit-rates, where <em>1 &lt; i &lt;
   R</em>.<br/>The last representation, <em>r<sub>R</sub></em> represents the highest quality level
   offered by the server</td>
		</tr>
		<tr>
			<td><em>bw<sub>avail</sub>(t)</em></td><td>b/s</td><td>Available bandwidth emulated during the session</td>
		</tr>
		<tr>
			<td><em><span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span></em></td><td>b/s</td><td>Maximum available bandwidth</td>
		</tr>
		<tr>
			<td><em>p<sub>e</sub></em></td><td>-</td><td>Probability of error</td>
		</tr>
	</table>
</div>

<div class="table">
	<a name="tab:output-parameters"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.7</span> Output parameters.
	</div>
	
	<table>
		<tr class="header">
			<td>Notation</td><td>Unit</td><td>Definition</td>
		</tr>
		<tr>
			<td><em>T<sub>buf</sub></em></td><td>s</td><td>Total buffering time, i.e., total time the playback is
   interrupted due to buffer underrun during the session</td>
		</tr>
		<tr>
			<td><em>T<sub>start-up</sub></em></td><td>s</td><td>Start-up time</td>
		</tr>
		<tr>
			<td><em>T<sub>inactive</sub></em></td><td>s</td><td>Total inactive time, i.e., the client is not downloading
   segments</td>
		</tr>
		<tr>
			<td><em>S<sub>missed</sub></em></td><td>segments</td><td>Total number of missed segments due
   to <span class="code">HTTP-404</span> responses</td>
		</tr>
		<tr>
			<td><em>S<sub>retry</sub></em></td><td>segments</td><td>Total number of retried segments, i.e.,
   segments which are discarded to be re-downloaded at another quality level</td>
		</tr>
		<tr>
			<td><em><span class="symbol">t</span>(t)</em></td><td>b/s</td><td>Throughput, i.e., measured link bit-rate on client's side</td>
		</tr>
		<tr>
			<td><em>b(t)</em></td><td>b/s</td><td>Media bit-rate selected on client's side</td>
		</tr>
		<tr>
			<td><em>s<sub>client</sub>(t)</em></td><td>s</td><td>Segments' actual playback on client (absolute
   time-stamps synchronized with NTP)</td>
		</tr>
		<tr>
			<td><em>s<sub>server</sub>(t)</em></td><td>s</td><td>Server's segment availability (absolute time-stamps
   synchronized with NTP)</td>
		</tr>
		<tr>
			<td><em>c<sub>state</sub>(t)</em></td><td>-</td><td>Client's state function</td>
		</tr>
	</table>
</div>

<p>The <strong>available bandwidth</strong> (<em>bw<sub>avail</sub>(t)</em>) represents the bit-rate
offered at the server's side over the session time as emulated by Dummynet. The
available bandwidth is a function defined from <em>t = 0</em> s to <em>t = T</em>.</p>
  
<p>The <strong>maximum available bandwidth</strong> (<em><span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span></em>) is
defined as the available bandwidth at the highest representation bit-rate
offered by the server (<em>r<sub>R</sub></em>). In this evaluation, truncation is produced at
<em>r<sub>R</sub> = 2</em> Mb/s (corresponding to representation number 10 in
table <a href="#tab:representations">5.5</a>). The maximum available bandwidth is a function
defined from <em>t = 0</em> s to <em>t = T</em>. Figure <a href="#fig:bandwidth-functions">5.3</a> depicts
an example of the <em>bw<sub>avail</sub>(t)</em> and the <em><span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span></em> functions.</p>

<div class="equation">
	<a name="eq:maximum-bandwidth"></a>
	<table>
		<tr>
			<td>
				<em><span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span> = bw<sub>avail</sub>(t) <span class="bigspace">0 &lt; <span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span> &lt; r<sub>R</sub></span></em>
				<span class="equationnum">(5.1)</span>
			</td>
		</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:bandwidth-functions"></a>
	<img src="img/max-available-bandwidth.png" alt="Maximum
	available bandwidth function."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.3</span> Example of a available bandwidth function (<em>bw<sub>avail</sub>(t)</em>) and the
	representation levels depicted in the left hand figure. The maximum
	available bandwidth function (<em><span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span></em>) is depicted on the
	right hand figure, truncated at the maximum representation level (<em>r<sub>R</sub> =
	2</em> Mb/s).
	</div>
</div>
  
<p>The <strong>throughput</strong> function (<em><span class="symbol">t</span>(t)</em>) represents the measured link
bit-rate on the client's side. The throughput is a stepwise function computed
over the individual throughput of every segment (<em><span class="symbol">t</span><sub>i</sub></em>). This is calculated as:</p>
  
<div class="equation">
	<table>
		<tr>
			<td>
				<em><span class="symbol">t</span><sub>i</sub> = </em>
			</td>
			<td>
				<em>S<sub>i</sub></em>
				<div class="hrcomp"><hr/></div>
				<em>T<sub>i</sub></em>
			</td>
		</tr>
	</table>
</div>  
  
<p>Where <em>S<sub>i</sub></em> is the size in bits of the segment <em>i</em> and <em>T<sub>i</sub></em> the time
the client spends downloading the segment <em>i</em>. The throughput function is
constructed as follows:</p>

<div class="equation">
	<table>
		<tr>
			<td>
				<span class="symbol">t</span>(t) = 
			</td>
			<td class="leftborder">&nbsp;</td>
			<td>
				<table>
					<tr>
						<td>
							0
						</td>
						<td>
							<span class="bigspace">0 &lt; t &lt; t<sub>b1</sub></span>
						</td>
					</tr>
					<tr>
						<td>
							<table>
								<tr>
									<td>
										<em><span class="symbol">t</span><sub>1</sub> = </em>
									</td>
									<td>
										s<sub>1</sub>
										<div class="hrcomp"><hr/></div>
										t<sub>b1</sub> <span class="symbol">-</span> t<sub>a1</sub>
									</td>
								</tr>
							</table>
						</td>
						<td>
							<span class="bigspace">t<sub>b1</sub> &lt; t &lt; t<sub>b2</sub></span>
						</td>
					</tr>
					<tr>
						<td>
							<table>
								<tr>
									<td>
										<em><span class="symbol">t</span><sub>1</sub> = </em>
									</td>
									<td>
										s<sub>1</sub>
										<div class="hrcomp"><hr/></div>
										t<sub>b2</sub> <span class="symbol">-</span> t<sub>a2</sub>
									</td>
								</tr>
							</table>
						</td>
						<td>
							<span class="bigspace">t<sub>b2</sub> &lt; t &lt; t<sub>b3</sub></span>
						</td>
					</tr>
					<tr>
						<td>
							...
						</td>
						<td>
							<span class="bigspace">...</span>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>
</div>

<p>Where <em>s<sub>1</sub>, s<sub>2</sub>, ...</em> are the sizes of the downloaded segments and <em>t<sub>a1</sub>,
t<sub>a2</sub>, ...</em> and <em>t<sub>b1</sub>, t<sub>b2</sub> ...</em> are respectively the starting and
ending time-stamps which determine the downloading time for each segment.</p>
	
<p>The <strong>selected media bit-rate</strong> (<em>b(t)</em>) is a stepwise function which
records the representation level selected by the client for the segments that
have been <em>successfully</em> downloaded. Note that, the representation level
of the segments that are discarded (to be re-downloaded at a lower quality
level) are not considered in <em>b(t)</em>. The selected media bit-rate is the result
of the decisions taken by the adaptation mechanisms (introduced in the previous
chapter, section <a href="#sec:rate-logic-module">4.4.7</a>). These mechanisms use the
throughput (<em><span class="symbol">t</span>(t)</em>) as the only metric to produce adaptation.</p>


<h3><a name="sec:metrics"></a><a name="5.1.6"></a>5.1.6 Metrics</h3>

<p>Table <a href="#tab:metrics">5.8</a> enumerates the metrics defined for our evaluation.
All of these metrics are defined using a common reference (i.e., the session
time, <em>T</em>), to ease comparison regardless of the experiments' session time.
These metrics are calculated over all the data accumulated over the session.
Thus, the disparity of the results obtained by running the same experiment several
times is minimized. The following subsections explain in detail how these
metrics are computed.</p>

<div class="table">
	<a name="tab:metrics"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.8</span> The metrics that will be used for our evaluation.
	</div>
	
	<table>
		<tr class="header">
			<td>Notation</td><td>Unit</td><td>Definition</td>
		</tr>
		<tr>
			<td><em>u(t)</em></td><td>%</td><td>Bandwidth utilization</td>
		</tr>
		<tr>
			<td><em>e(t)</em></td><td>s</td><td>End-to-end latency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>bw</sub></em></td><td>%</td><td>Bandwidth utilization efficiency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>buf</sub></em></td><td>%</td><td>Buffering efficiency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td>%</td><td>Segment fetch efficiency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>retry</sub></em></td><td>%</td><td>Segment-retry efficiency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>active</sub></em></td><td>%</td><td>Active efficiency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td>%</td><td>Start-up efficiency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>up</sub></em></td><td>%</td><td>Reaction (switching-up) efficiency</td>
		</tr>
		<tr>
			<td><em><span class="symbol">e</span><sub>down</sub></em></td><td>%</td><td>Reaction (switching-down) efficiency</td>
		</tr>
	</table>
</div>


<h4><a name="sec:weighted-functions"></a><a name="5.1.6.1"></a>5.1.6.1 Weighted functions</h4>

<p>A pair of weighted functions (depicted in figure <a href="#fig:weight-functions">5.4</a>
have been proposed to define some of the metrics of this evaluation. In
particular, these functions are used for the active efficiency
(section <a href="#sec:active-efficiency">5.1.6.8</a>), the start-up efficiency
(section <a href="#sec:start-efficiency">5.1.6.9</a>), and the reaction efficiency
(section <a href="#sec:reaction-efficiency">5.1.6.10</a>).</p>

<p>The first weighting function (<em>w<sub>long</sub>(t)</em>) is a linear, monotonically
decreasing function which assigns higher weights at the beginning than at the end of
session <em>T</em>. The function <em>w<sub>long</sub>(t)</em> is defined to provide weight to metrics
which involve measurements during the whole session time (<em>T</em>).</p>

<div class="equation">
	<table>
		<tr>
			<td>
				w<sub>long</sub>(t) = 1 &ndash;
			</td>
			<td>
				t
				<div class="hrcomp"><hr/></div>
				T
			</td>
			<td>
				<span class="bigspace">0 &lt; t &lt; T</span>
			</td>
		</tr>
	</table>
</div>  

<p><em>w<sub>short</sub>(t)</em> is a non-linear function which assigns higher
weights given small time values. <em>w<sub>short</sub>(t)</em> decreases much faster
than <em>w<sub>long</sub>(t)</em>, such that a delay of 20 s (the maximum segment duration in
the experiments) is assigned<a href="#footnote35" name="note35"><sup>35</sup></a> a weight of
50% (<em>w<sub>short</sub>(20) = </em>0.5). As a result, <em>w<sub>short</sub>(t)</em> provides higher
weight to metrics which measure short delays or intervals shorter than the
session time (<em>T</em>), such as reaction times.</p>

<div class="equation">
	<table>
		<tr>
			<td>
				<span class="top10">w<sub>short</sub>(t) =</span>
			</td>
			<td>
				1
				<div class="hrcomp"><hr/></div>
				<table>
					<tr>
						<td>
							t
							<div class="hrcomp"><hr/></div>
							20
						</td>
						<td>
							+ 1
						</td>
					</tr>
				</table>
			</td>
			<td>
				<span class="bigspace top10">0 &lt; t &lt; T</span>
			</td>
		</tr>
	</table>
</div>  

<div class="figure">
	<a name="fig:weight-functions"></a>
	<img src="img/weight-functions.png" alt="Weighted functions."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.4</span> Weighted functions. <em>w<sub>long</sub>(t)</em> weights metrics
	over the whole session <em>T</em>, whereas <em>w<sub>short</sub>(t)</em> is more suitable to
	weight delays and short intervals.
	</div>
</div>

<h4><a name="5.1.6.2"></a>5.1.6.2 Bandwidth utilization</h4>

<p>The bandwidth utilization function (<em>u(t)</em>) is defined as the selected bandwidth
of the client (<em>b(t)</em>) normalized by the maximum available bandwidth
(<em><span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span></em>). Thus, given a time <em>T</em>, this function compares the
media bit-rate selected on the client's side compared to the maximum available
bit-rate in the network. Values of <em>u(t)</em> lower than 1 represent an
<em>underestimation</em> of the maximum available bandwidth, whereas values
greater than 1 denote an <em>overuse</em> of the available bandwidth.</p>

<div class="equation">
	<table>
		<tr>
			<td>
				u(t) =
			</td>
			<td>
				b(t)
				<div class="hrcomp"><hr/></div>
				<span class="tilde symbol">&#126;</span><span class="subtilde">bw<sub>avail</sub>(t)</span>
				
			</td>
			<td>
				<span class="equationnum">(5.2)</span>
			</td>
		</tr>
	</table>
</div>  

<h4><a name="5.1.6.3"></a>5.1.6.3 Bandwidth efficiency</h4>

<p>The bandwidth utilization efficiency (<em><span class="symbol">e</span><sub>bw</sub></em>) is defined as the
integral of the bandwidth utilization (<em>u(t)</em>), normalized to the session
time <em>T</em>. As a result <em><span class="symbol">e</span><sub>bw</sub></em> represents the efficacy of the
bandwidth utilization throughout the whole session. This coefficient is
calculated as follows:</p>
  
<p>First, the bandwidth utilization function (<em>u(t)</em>) is separated into the sum
of two sub-functions <em>u<sub>under</sub>(t)</em> and <em>u<sub>over</sub>(t)</em>:</p>
 
<div class="equation">
	<table>
		<tr>
			<td>
				u(t) = u<sub>under</sub>(t) + u<sub>over</sub>(t)
			</td>
		</tr>
	</table>
</div>
  
<p>Where <em>u<sub>under</sub>(t)</em> and <em>u<sub>over</sub>(t)</em> contain respectively the values of <em>u(t)</em>
that are lower and greater than 1:</p>

<div class="equation">
	<table>
		<tr>
			<td>
				u<sub>under</sub>(t) =
			</td>
			<td class="leftborder"></td>
			<td>
				<table>
					<tr>
						<td>
							u(t)
						</td>
						<td>
							<span class="bigspace">u(t) &lt; 1</span>
						</td>
					</tr>
					<tr>
						<td>
							0
						</td>
						<td>
							<span class="bigspace">u(t) &gt; 1</span>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>
</div>

<div class="equation">
	<table>
		<tr>
			<td>
				u<sub>over</sub>(t) =
			</td>
			<td class="leftborder"></td>
			<td>
				<table>
					<tr>
						<td>
							0
						</td>
						<td>
							<span class="bigspace">u(t) &lt; 1</span>
						</td>
					</tr>
					<tr>
						<td>
							u(t)
						</td>
						<td>
							<span class="bigspace">u(t) &gt; 1</span>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>
</div>
	
<p>Given these two functions, the bandwidth utilization efficiency is calculated as
follows:</p>

<div class="equation">
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>bw</sub> = 
			</td>
			<td>
				1
				<div class="hrcomp"><hr/></div>
				T
			</td>
			<td>		
			</td>
			<td class="leftborder">&nbsp;</td>
			<td>
			</td>
			<td>
				<span class="symbol normal">ó<br/><span class="top3">õ</span></span>
			</td>
			<td>
				<table>
					<tr>
						<td>
							&nbsp;
						</td>
					</tr>
					<tr>
						<td>
							<sub>T</sub>
						</td>
					</tr>
				</table>
			</td>
			<td>
				u<sub>under</sub>(t) dt +
		
			</td>
			<td>
				<span class="symbol normal">ó<br/><span class="top3">õ</span></span>
			</td>
			<td>
				<table>
					<tr>
						<td>
							&nbsp;
						</td>
					</tr>
					<tr>
						<td>
							<sub>T</sub>
						</td>
					</tr>
				</table>
			</td>
			<td>
				1
				<div class="hrcomp"><hr/></div>
				u<sub>over</sub>(t) dt
			</td>
			<td class="rightborder">&nbsp;</td>
			<td>
				<span class="equationnum">(5.3)</span>
			</td>
		</tr>
	</table>
</div>
	
<p>Note that the second integral is calculated using the inverse of <em>u<sub>over</sub>(t)</em> 
since all values of <em>u<sub>over</sub>(t)</em> are all greater than 1. The inverse is utilized
to correlate <em><span class="symbol">e</span><sub>bw</sub></em> with the underestimation and overestimation of
the available bandwidth.</p>

<h4><a name="5.1.6.4"></a>5.1.6.4 Buffering efficiency</h4>
  
<p>The buffering efficiency (<em><span class="symbol">e</span><sub>buf</sub></em>) measures the total playback time
over the session. Note that in our evaluation it is assumed that there is always
playable content during the experiments, therefore the player can be only
switched between two states: <em>buffering</em> or <em>playing</em>.
<em><span class="symbol">e</span><sub>buf</sub></em> is calculated using the cumulative buffering time
(<em>T<sub>buf</sub></em>), i.e., the time the playback has been interrupted due to
<em>buffer underrun</em> as indicated in equation <a href="#eq:buffering-efficiency">5.4</a>.</p>
  
<div class="equation">
	<a name="eq:buffering-efficiency"></a>
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>buf</sub> = 1 &ndash; 
			</td>
			<td>
				T<sub>buf</sub>
				<div class="hrcomp"><hr/></div>
				T
			</td>
			<td>
				<span class="equationnum">(5.4)</span>
			</td>
		</tr>
	</table>
</div>    
	
<h4><a name="5.1.6.5"></a>5.1.6.5 Segment-fetch efficiency</h4>

<p>The segment-fetch efficiency (<em><span class="symbol">e</span><sub>fetch</sub></em>) is the ratio of segments
successfully downloaded (<em>S - S<sub>missed</sub></em>) over the total number of segments
that should have been in the session (<em>S</em>). It provides a comparison between the
number of segments that have been successfully fetched and the missed segments
due to <span class="code">HTTP-404</span> responses.</p>
	
<div class="equation">
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>fetch</sub> = 1 &ndash; 
			</td>
			<td>
				S<sub>missed</sub>
				<div class="hrcomp"><hr/></div>
				S
			</td>
			<td>
				<span class="equationnum">(5.5)</span>
			</td>
		</tr>
	</table>
</div> 	
	
<h4><a name="5.1.6.6"></a>5.1.6.6 Segment-retry efficiency</h4>

<p>The segment-retry efficiency (<em><span class="symbol">e</span><sub>retry</sub></em>) represents the proportion
of the time consumed when the client re-downloads segments at
different qualities, compared to the session time (<em>T</em>). The consumed time is
calculated as the number of re-attempts (<em>S<sub>retry</sub></em>) multiplied by the maximum
time to download a segment (<em>T<sub>timeout</sub></em>). Note that <em>T<sub>timeout</sub></em> is defined
as the 80% of the segment duration (<em>S<sub>d</sub></em>).</p>

<div class="equation">
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>retry</sub> = 1 &ndash; 
			</td>
			<td>
				S<sub>retry</sub> · T<sub>timeout</sub>
				<div class="hrcomp"><hr/></div>
				T
			</td>
			<td>
				<span class="equationnum">(5.6)</span>
			</td>
		</tr>
	</table>
</div> 
	
<h4><a name="5.1.6.7"></a>5.1.6.7 End-to-end latency</h4>

<p>The end-to-end latency (<em>e(t)</em>) indicates the delay from when a new
segment is available on the server's side until the client actually plays the
segment (<strong>not</strong> when the segment is stored in buffer). <em>e(t)</em> is an
stepwise function calculated as follows:</p>
	
<div class="equation">
	<table>
		<tr>
			<td>
				e(t<sub>i</sub> ) = s<sub>client</sub>(t<sub>i</sub> ) &ndash; s<sub>server</sub>(t<sub>i</sub> )
			</td>
			<td>
				<span class="equationnum">(5.7)</span>
			</td>
		</tr>
	</table>
</div> 	
	
<p>Where <em>e(t<sub>i</sub>)</em> is the end-to-end latency of the segment <em>i</em>, <em>s<sub>client</sub>(t_i)</em>
represents the time-stamp of the actual playback of segment <em>i</em> on the client's
side, and <em>s<sub>server</sub>(t<sub>i</sub>)</em> is the time-stamp when segment <em>i</em> is produced on
the server's side. Time-stamps are synchronized with a NTP server (as explained in
section <a href="#sec:synchronization">4.2</a>).</p>

	
<h4><a name="sec:active-efficiency"></a><a name="5.1.6.8"></a>5.1.6.8 Active efficiency</h4>

<p>The active efficiency (<em><span class="symbol">e</span><sub>active</sub></em>) provides a weighted sum of the
total time the client is in the <em>active</em> state, normalized by the total
active time during the session. <em><span class="symbol">e</span><sub>active</sub></em> is calculated as:</p>

<div class="equation">
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>active</sub> = 
			</td>
			<td>
				1 &ndash;
			</td>
			<td>
			</td>
			<td>
				1
				<div class="hrcomp"><hr/></div>
				T<sub>inactive</sub>
			</td>
			<td>
				<span class="symbol normal">ó<br/><span class="top3">õ</span></span>
			</td>
			<td>
				<table>
					<tr>
						<td>
							&nbsp;
						</td>
					</tr>
					<tr>
						<td>
							<sub>T</sub>
						</td>
					</tr>
				</table>
			</td>
			<td>
				c<sub>state</sub>(t) · w<sub>long</sub>(t) dt
			</td>
			<td>
				<span class="equationnum">(5.8)</span>
			</td>
		</tr>
	</table>
</div>
	
<p>Where <em>w<sub>long</sub>(t)</em> is a weighting function (defined in
section <a href="#sec:weighted-functions">5.1.6.1</a>) and <em>c<sub>state</sub>(t)</em> denotes the stepwise
<em>state function</em>:</p>

<div class="equation">
	<table>
		<tr>
			<td>
				c<sub>state</sub>(t) =
			</td>
			<td class="leftborder">&nbsp;</td>
			<td>
				<table>
					<tr>
						<td>
							1
						</td>
						<td>
							<span class="bigspace normal">inactive</span>
						</td>
					</tr>
					<tr>
						<td>
							u(t)
						</td>
						<td>
							<span class="bigspace normal">active</span>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>
</div>
	

<h4><a name="sec:start-efficiency"></a><a name="5.1.6.9"></a>5.1.6.9 Start-up efficiency</h4>
	
<p>The start-up efficiency (<em><span class="symbol">e</span><sub>start-up</sub></em>) is defined as the
weighted start-up time, that is, the <em>w<sub>short</sub>(t)</em> weighted function is applied
to the time it takes to start playback (<em>T<sub>start-up</sub></em>). <em>T<sub>start-up</sub></em> is defined as the
interval since the begin of the experiment (<em>t = 0</em> s) until the first media
segment starts to play.</p>

<div class="equation">
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>start-up</sub> = w<sub>short</sub> (T<sub>start-up</sub> )
			</td>
			<td>
				<span class="equationnum">(5.9)</span>
			</td>
		</tr>
	</table>
</div>  


<h4><a name="sec:reaction-efficiency"></a><a name="5.1.6.10"></a>5.1.6.10 Reaction efficiency</h4>

<p>Figure <a href="#fig:reaction">5.5</a> illustrates the criteria employed to calculate the
reaction times in our evaluation. Two types of intervals are defined:</p>

<ol>
  <li>The switching-up reaction interval (<em>T<sub>up</sub></em>) is the elapsed time since
  a bit-rate increase occured in the network (<em>bw<sub>avail</sub>(t)</em>) until the client
  <em>increments</em> the quality level (<em>b(t)</em>). Note that further switching
  operations might be performed; however, they are not considered in the
  <em>T<sub>up</sub></em> interval.</li>
  
  <li>The switching-down reaction interval (<em>T<sub>down</sub></em>) is defined similarly
  to <em>T<sub>up</sub></em>, in case of bit-rate reductions in <em>bw<sub>avail</sub>(t)</em>.</li>
</ol>

<p>Thus, the reaction efficiencies for switching-up and switching-down
(<em><span class="symbol">e</span><sub>up</sub></em>, <em><span class="symbol">e</span><sub>down</sub></em>) are defined as <em>T<sub>up</sub></em> and
<em>T<sub>down</sub></em> weighted by the <em>w<sub>short</sub>(t)</em> function.</p>

<div class="equation">
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>up</sub> = w<sub>short</sub> (T<sub>up</sub> )
			</td>
			<td>
				<span class="equationnum">(5.10)</span>
			</td>
		</tr>
	</table>
	<table>
		<tr>
			<td>
				<span class="symbol">e</span><sub>down</sub> = w<sub>short</sub> (T<sub>down</sub> )
			</td>
			<td>
				<span class="equationnum">(5.11)</span>
			</td>
		</tr>
	</table>
</div> 

<div class="figure">
	<a name="fig:reaction"></a>
	<img src="img/reaction-time.png" alt="Reaction times."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.5</span> Reaction times.
	</div>
</div>

<h3><a name="sec:network-scenarios"></a><a name="5.1.7"></a>5.1.7 Network scenarios</h3>

<p>Figure <a href="#fig:scenarios">5.6</a> depicts the network scenarios considered in our
evaluation. These scenarios provide persistent and non-persistent bandwidth
fluctuations. The session time for all the experiments was defined to be
sufficiently long enough to determine the long-term behaviour of the system: an
experimental run of 10 minutes (<em>T = 600</em> s) prevents bias of the results.
Considering the longest segment duration evaluated on this scenarios (<em>S<sub>d</sub> = 20</em> s), the
session duration is 600 <span class="symbol">±</span> 20 s (<span class="symbol">±</span> 3.33%), thus providing a confidence of
96.67%. For all scenarios, two families of experiments were performed:</p>

<ol>
  <li>Performance of the three adaptive mechanisms during the session.
  The duration of segments is fixed to 10 seconds (<em>S<sub>d</sub> = 10</em> s) in this set of
  experiments.</li>
  
  <li>Performance of the conservative mechanism with different
  segment durations (5 s, 10 s, and 20 s).</li>
</ol>

<div class="figure">
	<a name="fig:scenarios"></a>
	<img src="img/scenarios.png" alt="Network scenarios emulated during the evaluation."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.6</span> Network scenarios emulated during the evaluation. All of them produce
  variations in the available bandwidth.
	</div>
</div>

<h2><a name="sec:scenario1"></a><a name="5.2"></a>5.2 Scenario 1: long-term variations of the available bandwidth</h2>

<p>The first network scenario (depicted in figure <a href="#fig:scenario1">5.7</a>) produces
long-term variations of the available bandwidth. The scenario has two parts:
from <em>t = 0</em> s to <em>t = 300</em> s, bit-rates are decreased at intervals of one
minute from 4 Mb/s to 250 kb/s. In the second part, bit-rates are increased
from 250 kb/s to 4 Mb/s at one minute intervals.</p>

<div class="figure">
	<a name="fig:scenario1"></a>
	<img src="img/scenario1-pipe.png" alt="Series of Dummynet pipes for the first network scenario."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.7</span> Series of <em>Dummynet</em> pipes for the first network scenario.
	</div>
</div>

<p>This network scenario evaluates the behaviour of the client to
abrupt, but not frequent fluctuations in the available bandwidth. The first part
forces the client to reduce the selected media level, whereas in the second
part the client may gradually switch to the highest possible representation
level.</p>

<h3><a name="sec:scenario1-mechanisms"></a><a name="5.2.1"></a>5.2.1 Performance of the adaptation mechanisms</h3>

<p>Figure <a href="#fig:A1-1">5.8</a>, figure <a href="#fig:A1-2">5.9</a> and figure <a href="#fig:A1-3">5.10</a> represent
respectively the performance of the aggressive, conservative, and
mean mechanisms in terms of bandwidth utilization and buffer state. The
analysis of the figures show that all algorithms started with the lowest
quality level and successfully switched to the highest possible level within the
first 12 s (aggressive: <em>t = 11</em> s; conservative: <em>t = 8</em> s; mean <em>t = 8</em> s),
subsequently achieving the 100% use of bandwidth (<em>u(t) = 1.0</em>).</p>
  
<p>The aggressive mechanism (figure <a href="#fig:A1-1">5.8</a>) was significantly more
sensitive to bandwidth fluctuations. Consequently, the selected
media levels (<em>b(t)</em>) suffered unnecessary variations during the 250-350 s
interval. This high sensitivity is illustrated at <em>t = 145</em> s when there was a
sudden diminution of bandwidth and the aggressive algorithm detected it
appropriately.</p>

<div class="figure">
	<a name="fig:A1-1"></a>
	<img src="img/A1-1.png" alt="Performance of the aggressive mechanism over the scenario 1."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.8</span> Performance of the aggressive mechanism over the scenario 1.
	</div>
</div>
  
<p>The conservative mechanism (figure <a href="#fig:A1-2">5.9</a>) selected different quality
levels with considerably fewer fluctuations than the aggressive method. During the
whole session the aggressive mechanism performed 32 switching operations,
whereas the conservative mechanism only switched 21 times (versus 25 switching
operations in the case of the mean algorithm). However, in the 500-560 s interval, the
conservative mechanism produced a notably lower efficiency (50%) than the
other two mechanisms (about 70%).</p>

<div class="figure">
	<a name="fig:A1-2"></a>
	<img src="img/A1-2.png" alt="Performance of the conservative mechanism over the scenario 1."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.9</span> Performance of the conservative mechanism over the scenario 1.
	</div>
</div>
  
<p>The mean mechanism (figure <a href="#fig:A1-3">5.10</a>) required two (or more) switching
steps to select the appropriate bandwidth level. This behaviour is a consequence
of the nature of the algorithm, which considers the last three throughput
measurements (as defined in algorithm <a href="#alg:mean">3</a>). These additional steps
took place during the intervals 165-176 s and 360-385 s. Furthermore, this
mechanism is the slowest one to adapt during the first part of the
scenario (monotonically decreasing bit-rate). The bandwidth utilization
remained at 200% (<em>u(t) = 2.0</em>) during 45.2 s, meaning that the quality of the
segments was overestimated, i.e., segments' downloading times increased.</p>

<div class="figure">
	<a name="fig:A1-3"></a>
	<img src="img/A1-3.png" alt="Performance of the mean mechanism over the scenario 1."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.10</span> Performance of the mean mechanism over the scenario 1.
	</div>
</div>

<h4><a name="5.2.1.1"></a>5.2.1.1 Impact on the metrics</h4>

<p>Results of this experiment have been summarized in Table <a href="#tab:A1-m">5.9</a>
and figure <a href="#fig:A1-m">5.11</a> according to the metrics defined previously in
section <a href="#sec:metrics">5.1.6</a>. Figure <a href="#fig:A1-m">5.11</a> provides a graphical
representation of table <a href="#tab:A1-m">5.9</a>, where the differences among the
metrics for each mechanism are easily discernible.</p>

<div class="figure">
	<a name="fig:A1-m"></a>
	<img src="img/A1-mechanism.png" alt="Graphical comparison under network scenario 1 for aggressive,
	conservative, and mean adaptive mechanisms."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.11</span> Graphical comparison under network scenario 1 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
</div>

<div class="table">
	<a name="tab:A1-m"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.9</span> Computed metrics under network scenario 1 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Mechanism</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em> </td>
		</tr>
		<tr>
			<td>Aggressive</td><td>60.22%</td><td>96.28%</td><td>91.6%</td><td>79.78%</td><td>49.99%</td><td>87.43%</td><td>69.70%</td><td>52.26%</td>
		</tr>
		<tr>
			<td>Conservative</td><td>61.59%</td><td>100%</td><td>100%</td><td>92.59%</td><td>72.3%</td><td>87.51%</td><td>50.01%</td><td>43.22%</td>
		</tr>
		<tr>
			<td>Mean</td><td>62.14%</td><td>94.71%</td><td>95%</td><td>84.26%</td><td>96.85%</td><td>90.08%</td><td>72.92%</td><td>37.14%</td>
		</tr>
	</table>
</div>
  
<p>The bandwidth, buffering, and start-up efficiencies (<em><span class="symbol">e</span><sub>bw</sub></em>,
<em><span class="symbol">e</span><sub>buf</sub></em>, <em><span class="symbol">e</span><sub>start-up</sub></em>) are not affected by the adaptive
mechanism selected, whereas the active efficiency (<em><span class="symbol">e</span><sub>active</sub></em>) and
the segment-retry efficiency (<em><span class="symbol">e</span><sub>retry</sub></em>) reveal a remarkable
dependency.</p>

<p>The bandwidth efficiency (<em><span class="symbol">e</span><sub>bw</sub></em>) exceeds the 50% in all adaptive
mechanisms (aggressive: 60.22%; conservative: 61.59%; and mean: 62.14%).
With regard to the buffering efficiency (<em><span class="symbol">e</span><sub>buf</sub></em>), the conservative
mechanism did not stop during the whole session (denoted by <em><span class="symbol">e</span><sub>buf</sub> =
100%</em>), whereas the aggressive and the mean mechanism produced interruptions
in playback during 3.72% (<em><span class="symbol">e</span><sub>buf</sub> = 96.28%</em>) and 5.29%
(<em><span class="symbol">e</span><sub>buf</sub> = 94.71%</em>) of the session time due to buffer
underrun.</p>

<p>The segment-fetch efficiency (<em><span class="symbol">e</span><sub>fetch</sub></em>) is over 90% in all
mechanisms, with the worst value in the aggressive one (91.6%). This means that
less than 10% of the media content is not played. The aggressive mechanism
re-downloads more segments due to more frequent switching operations (it
presents the worst segment-retry efficiency, <em><span class="symbol">e</span><sub>retry</sub> = 79.78%</em>).</p>

<p>The active efficiency (<em><span class="symbol">e</span><sub>active</sub></em>) is significantly better for the
mean mechanism (96.85%) than the aggressive (49.99%) and conservative mechanism (72.3%).</p>

<h3><a name="5.2.2"></a>5.2.2 Performance with different duration segments</h3>

<p>Figure <a href="#fig:A1-2-5">5.12</a> and figure <a href="#fig:A1-2-20">5.13</a>
represent the behaviour of the conservative mechanism over time with a shorter
and longer duration segments, respectively (for 10s-long segments, see
figure <a href="#fig:A1-2">5.9</a>). The fundamental difference between the three
cases is the adaptability to the available bandwidth. It is notable that the
reaction times are dependent of the duration of the segments, since the
throughput is measured after segments have been transferred. Switching
operations are produced earlier for 5s-long segments than 10s-long segments and
consequently, even earlier than for 20s-long segments.</p>

<div class="figure">
	<a name="fig:A1-2-5"></a>
	<img src="img/A1-2-5.png" alt="Performance of the conservative mechanism over the scenario 1
  with a segment duration of 5 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.12</span> Performance of the conservative mechanism over the scenario 1
  with a segment duration of 5 s.
	</div>
</div>

<p>Figure <a href="#fig:A1-2-20">5.13</a> shows that the selection of
the adequate bit-rate level for 20s-long segments is performed inadequately
during the first 300 s of the session. The conservative mechanism downloaded the
first and second media segments at the lowest quality levels, therefore it takes
more than 40 s (a pair of 20s-long segments) to switch to the highest possible
quality level. Upon reduction of the available bandwidth, segments are more
likely to be discarded therefore the buffer can be empty. In consequence,
levels are unnecessarily lowered to the minimum quality level in four occasions
(<em>t = 77</em> s, <em>t = 146</em> s, <em>t = 207</em> s and <em>t = 283</em> s) due to buffer underrun.</p>

<div class="figure">
	<a name="fig:A1-2-20"></a>
	<img src="img/A1-2-20.png" alt="Performance of the conservative mechanism over the scenario 1
  with a segment duration of 20 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.13</span> Performance of the conservative mechanism over the scenario 1
  with a segment duration of 20 s.
	</div>
</div>

<h4><a name="5.2.2.1"></a>5.2.2.1 Impact on the metrics</h4>

<p>Table <a href="#tab:A1-d">5.10</a> and figure <a href="#fig:A1-d">5.14</a> summarize the results of this
experiment. The buffering, segment-fetch, segment-retry, and start-up
efficiencies (<em><span class="symbol">e</span><sub>buf</sub></em>, <em><span class="symbol">e</span><sub>fetch</sub></em>,
<em><span class="symbol">e</span><sub>retry</sub></em>, and <em><span class="symbol">e</span><sub>start-up</sub></em>, respectively) show minor
dependency on the duration of the segments. In contrast, the active
efficiency (<em><span class="symbol">e</span><sub>active</sub></em>) and the reaction efficiencies
(<em><span class="symbol">e</span><sub>up</sub></em> and <em><span class="symbol">e</span><sub>down</sub></em>) are improved for shorter duration
of segments (best case for 5s-long, worse case for 20s-long).</p>
  
<p>In general, a shorter segment duration improves the efficiencies defined in
our evaluation. In the case of 5s-long segments, only the start-up efficiency
(<em><span class="symbol">e</span><sub>start-up</sub></em>) presents worse values than 20s-long segments. Since
the required buffered time to start playback is set to 10 s
(<em>T<sub>min-buf</sub> = 10</em> s), two 5s-long segments need to be downloaded to start 
playback, whereas only one segment is enough if <em>S_d <span class="symbol">³</span> 10</em> s.</p>

<div class="table">
	<a name="tab:A1-d"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.10</span> Metrics comparison under network scenario 1 for
		5s-long, 10s-long and 20s-long segments.
	</div>
	
	<table class="center">
		<tr class="header">
			<td><em>S<sub>d</sub></em> (s)</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em></td>
		</tr>
		<tr>
			<td>5</td><td>64.18%</td><td>96.7%</td><td>95.83%</td><td>96.77%</td><td>100%</td><td>80.55%</td><td>62.11%</td><td>63.82%</td>
		</tr>
		<tr>
			<td>10</td><td>61.59%</td><td>100%</td><td>100%</td><td>92.59%</td><td>72.3%</td><td>87.51%</td><td>50.01%</td><td>43.22%</td>
		</tr>
		<tr>
			<td>20</td><td>52.52%</td><td>97.23%</td><td>100%</td><td>90.36%</td><td>53.4%</td><td>88.69%</td><td>37.24%</td><td>41.12%</td>
		</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:A1-d"></a>
	<img src="img/A1-duration.png" alt="Graphical comparison under network scenario 1 for 5s-long,
  10s-long, and 20s-long segments."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.14</span> Graphical comparison under network scenario 1 for 5s-long,
  10s-long, and 20s-long segments.
	</div>
</div>

<h3><a name="5.2.3"></a>5.2.3 Analysis of the end-to-end latency</h3>

<p>Figure <a href="#fig:end-to-end-A1">5.15</a> represents the end-to-end latency (<em>e(t)</em>) for
the first scenario in two situations: (1) experiments with the three adaptation
mechanisms and (2) experiments with different segment durations.</p>

<p>Note that in our evaluation, the end-to-end latency is <strong>decreased</strong> if
segments are missed (the client receives <span class="code">HTTP-404</span> responses), since the
next segment in the playlist will be played earlier<a href="#footnote36" name="note36"><sup>36</sup></a>.
In contrast, the end-to-end latency is <strong>increased</strong> if the
playback is interrupted due to an empty buffer, since the next segment will be
player after the interruption in playback. Thus, the important aspects of the
<em>e(t)</em> function are related to the variations produced by these two occurrences
(missed segments and playback interruptions).</p>

<p>According to figure <a href="#fig:e2e-A1-m">5.15a</a>, the aggressive
mechanism presents the most significant variation in latency, with a delay of
40 s at the beginning of the session (<em>t = 0</em> s) and 25 s at the end (<em>t =
600</em> s). The effects of packet losses can be seen for the mean mechanism around
<em>T</em> = 190 s and <em>t = 250</em> s when the latency is reduced. The conservative mechanism
is the only one whose latency function remains stable throughout the whole
experiment, meaning that there were no segments lost and there were no playback
interruptions.</p>

<p>The effects of segment durations on the end-to-end latency are shown
in figure <a href="#fig:e2e-A1-d">5.15b</a>. A shorter duration of the segments (<em>S<sub>d</sub> =
5</em> s) produced more variation of the end-to-end delay: 47 s at the beginning of
the session and 25 s at the end. A longer duration of the segments (<em>S<sub>d</sub> =
20</em> s) only produced an increase of end-to-end delay at <em>t = 250</em> s.</p>

<div class="figure">
	<a name="fig:end-to-end-A1"></a>
	<table class="center">
		<tr>
			<td>
				<a name="fig:e2e-A1-m"></a><img src="img/end-to-end-A1-a.png" alt="Different adaptation mechanisms."/>
			</td>
			<td>
				<a name="fig:e2e-A1-d"></a><img src="img/end-to-end-A1-b.png" alt="Different duration of segments."/>
			</td>
		</tr>
		<tr>
			<td><strong>(a)</strong> Different adaptation mechanisms</td><td><strong>(b)</strong> Different duration of segments</td>
		</tr>
	</table>
	
	<div class="caption">
		<span class="captionTitle">Figure 5.15</span> End-to-end latency throughout the session for scenario 1.
	</div>
</div>

<h3><a name="5.2.4"></a>5.2.4 Discussion</h3>

<p>The first scenario was intended to test the behaviour of the adaptation
algorithms under the reduction of the available bit-rate (performed in four steps until
the middle of the session) and subsequent increase of the available bit-rate
until the end of the experiment.</p>

<p>All of the adaptation mechanism were able to detect the bit-rate fluctuations
although they exhibited major differences. The aggressive mechanism performed
multiple switching operations due to small variations in the throughput
measurements. The quality levels selected by the client oscillated more
with the aggressive and mean mechanism, without any real improvement in the
utilization of bandwidth.</p>

<p>The mean algorithm reacts later than the other mechanisms to the bit-rate
fluctuations, requiring several switching steps to select the highest possible
representation. The conservative mechanism performed fewer switching
operations, thus seeming to be the most appropriate algorithm for this scenario,
with the cost of underestimation of the available bandwidth at nearly the end of
the session.</p>

<p>The effects of different segment durations take place in the
first part of the scenario 1 (when bit-rates are decreased). Longer segment
duration lead to a inadequate utilization of the available bandwidth, since
throughput measurements are taken less frequently. Consequently, buffer
underrun events are more probable, resulting in a abrupt reduction of the media
bit-rate selected.</p>

<h2><a name="sec:scenario2"></a><a name="5.3"></a>5.3 Scenario 2: short-term variations of the available bandwidth</h2>

<p>The second network scenario (represented in figure <a href="#fig:scenario2">5.16</a>)
produces periodic short-term variations of the available bandwidth. In this
context, short-term changes are fluctuations produced at intervals of a
maximum of 30 s. The duration of these intervals is deliberately
chosen to analyze their impact on the mean algorithm, which considers the last
three throughput measurements.</p>

<p>Two parts are defined in this network scenario: in the first 5 minutes (300 s)
the bit-rates oscillate between 250 kb/s and 1 Mb/s with a frequency of 1/30 Hz. In the
second part, bit-rates switch between 1 Mb/s and 4 Mb/s. This scenario
is intended to evaluate the behaviour of the client with more frequent bit-rate
fluctuations than scenario 1.</p>

<div class="figure">
	<a name="fig:scenario2"></a>
	<img src="img/scenario2-pipe.png" alt="Series of Dummynet pipes for the second network scenario."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.16</span> Series of Dummynet pipes for the second network scenario.
	</div>
</div>

<h3><a name="5.3.1"></a>5.3.1 Performance of the adaptation mechanisms</h3>

<p>Figure <a href="#fig:A2-1">5.17</a>, figure <a href="#fig:A2-2">5.18</a>, and figure <a href="#fig:A2-3">5.19</a>
illustrate the selected bandwidth by the three different
mechanisms over time. The conservative mechanism offers the most stable
behaviour in comparison with the aggressive and mean algorithms.</p>

<p>The aggressive mechanism (figure <a href="#fig:A2-1">5.17</a>) seems to produce better
adaptation upon the bandwidth restrictions in the first part of the experiments
(from 0 s to 300 s). In this part, it only overuses the offered network
bit-rate during two short intervals (60-66 s and 180-199 s). However, it is
noticeable that there is an excessive number of switching operations
(i.e., 28 switches). In the second part (from 300 s to 600 s) the bandwidth is
doubly overestimated on four occasions<a href="#footnote37" name="note37"><sup>37</sup></a> (intervals 360-390 s, 420-450 s, 480-496 s and 540-565 s). Finally, in <em>t =
500</em> s the mechanism needed to switch to the lowest quality level.</p>

<div class="figure">
	<a name="fig:A2-1"></a>
	<img src="img/A2-1.png" alt="Performance of the aggressive mechanism over the scenario 2."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.17</span> Performance of the aggressive mechanism over the scenario 2.
	</div>
</div>
  
<p>The conservative mechanism (figure <a href="#fig:A2-2">5.18</a>) appropriately followed the
fluctuations produced by this scenario. During the first part, the bandwidth utilization
(<em>u(t)</em>) is better than the aggressive scenario (about 50%), although it is overestimated
in more occasions (up to four). After <em>t = 300</em> s, <em>u(t)</em> oscillates between
100% an 200%, with short intervals where the utilization decreases to
50%. This mechanism did not need to switch down abruptly to the lowest quality
level during the second part of the experiment.</p>
  
<div class="figure">
	<a name="fig:A2-2"></a>
	<img src="img/A2-2.png" alt="Performance of the conservative mechanism over the scenario 2."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.18</span> Performance of the conservative mechanism over the scenario 2.
	</div>
</div>  

<p>The mean mechanism (figure <a href="#fig:A2-3">5.19</a>) presented a mixed behaviour between the two
previous mechanisms. In the first part of the experiment, 25 switching
operations were performed. The available bandwidth was overestimated for
four intervals, although the duration of those intervals differ (unlike
the conservative mechanism). In the second part, the mean mechanism needed to
switch to the lowest quality levels on two occasions, increasing to 100%
of bandwidth utilization in several switching steps.</p>

<div class="figure">
	<a name="fig:A2-3"></a>
	<img src="img/A2-3.png" alt="Performance of the mean mechanism over the scenario 2."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.19</span> Performance of the mean mechanism over the scenario 2.
	</div>
</div>

<h4><a name="5.3.1.1"></a>5.3.1.1 Impact on the metrics</h4>
 
<p>Table <a href="#tab:A2-m">5.11</a> and figure <a href="#fig:A2-m">5.20</a> collect the
performance results of the three adaptive mechanisms under the
second scenario. The majority of coefficients have similar
values. Only the segment-retry efficiency (<em><span class="symbol">e</span><sub>retry</sub></em>) turns out to be
considerable better for the conservative mechanism (86.66%) than the
aggressive (51.66%) and mean (48.33%) mechanisms.</p>

<p>Active efficiency (<em><span class="symbol">e</span><sub>active</sub></em>), start-up efficiency
(<em><span class="symbol">e</span><sub>start-up</sub></em>), and reaction efficiencies (<em><span class="symbol">e</span><sub>up</sub></em> and
<em><span class="symbol">e</span><sub>down</sub></em>) are slightly better for the mean mechanism and aggressive
mechanisms.</p>

<div class="table">
	<a name="tab:A2-m"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.11</span> Metrics comparison under network scenario 2 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Mechanism</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em></td>
		</tr>
		<tr>
			<td>Aggressive</td><td>45.60%</td><td>98.34%</td><td>95%</td><td>74.25%</td><td>70.67%</td><td>66.17%</td><td>72.73%</td><td>47.65%</td>
		</tr>
		<tr>
			<td>Conservative</td><td>52.95%</td><td>100%</td><td>98.33%</td><td>91.46%</td><td>70.3%</td><td>59.14%</td><td>70.09%</td><td>39.72%</td>
		</tr>
		<tr>
			<td>Mean</td><td>48.87%</td><td>94.36%</td><td>91.66%</td><td>74.25%</td><td>83.07%</td><td>70.17%</td><td>79.91%</td><td>46.29%</td>
		</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:A2-m"></a>
	<img src="img/A2-mechanism.png" alt="Graphical comparison under network scenario 2 for aggressive,
	conservative, and mean adaptive mechanisms."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.20</span> Graphical comparison under network scenario 2 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
</div>

<h3><a name="5.3.2"></a>5.3.2 Performance with different duration segments</h3>

<p>Figure <a href="#fig:A2-2-5">5.21</a> represents the performance
of the conservative mechanism with 5s-long segments. Reaction
times are significantly improved although the overall adaptation is similar
that achieved for segments of 10 s (figure <a href="#fig:A2-2">5.18</a>).</p>

<div class="figure">
	<a name="fig:A2-2-5"></a>
	<img src="img/A2-2-5.png" alt="Performance of the conservative mechanism over the scenario 2
  with a segment duration of 5 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.21</span> Performance of the conservative mechanism over the scenario 2
  with a segment duration of 5 s.
	</div>
</div>

<p>Deficient reaction times are illustrated in figure <a href="#fig:A2-2-20">5.22</a>,
due to use of 20s-long segments. Under these conditions, the
conservative algorithm is not able to follow rapid bit-rate variations, since
the algorithm is only run after a whole segment has been downloaded. This
disadvantage is present in the throughput curves (<em><span class="symbol">t</span>(t)</em>) of both figures.
For segments of 5 s, <em><span class="symbol">t</span>(t)</em> is contained within the limits of the available
bandwidth (<em>bw<sub>avail</sub>(t)</em>) whereas for 20 s, the throughput measurements are
more inaccurate.</p>

<div class="figure">
	<a name="fig:A2-2-20"></a>
	<img src="img/A2-2-20.png" alt="Performance of the conservative mechanism over the scenario 2 with a segment duration of 20 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.22</span> Performance of the conservative mechanism over the scenario 2 with a segment duration of 20 s.
	</div>
</div>

<h4><a name="5.3.2.1"></a>5.3.2.1 Impact on the metrics</h4>

<p>The metrics for the scenario 2 are collected in table <a href="#tab:A2-d">5.12</a> and
figure <a href="#fig:A2-d">5.23</a>. 5s-long segments provide a better performance for
most of the parameters (<em><span class="symbol">e</span><sub>bw</sub></em>, <em><span class="symbol">e</span><sub>buf</sub></em>,
<em><span class="symbol">e</span><sub>retry</sub></em>, <em><span class="symbol">e</span><sub>active</sub></em>, <em><span class="symbol">e</span><sub>up</sub></em>, and
<em><span class="symbol">e</span><sub>down</sub></em>), and similar results for the segment-fetch efficiency.</p>

<p>The most influenced metrics in this scenario are the active efficiency
(<em><span class="symbol">e</span><sub>active</sub></em>) and the reaction time for switching up operations (<em><span class="symbol">e</span><sub>up</sub></em>). The
improvement of <em><span class="symbol">e</span><sub>active</sub></em> relates to the number of requests needed for shorter
segments. In order to download 20 s of video content, four
5s-long segments have to be downloaded, whereas just one for
20s-long segments. <em><span class="symbol">e</span><sub>up</sub></em> is completely dependent of the
segment length (5s-long: 84.11%, 10s-long: 39.06%, 20s-long: 17.25%).</p>

<div class="figure">
	<a name="fig:A2-d"></a>
	<img src="img/A2-duration.png" alt="Graphical comparison under network scenario 2 for 5s-long,
  10s-long and 20-seconds long segments."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.23</span> Graphical comparison under network scenario 2 for 5s-long,
  10s-long and 20-seconds long segments.
	</div>
</div>

<div class="table">
	<a name="tab:A2-d"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.12</span> Metrics comparison under network scenario 2 for
		5s-long, 10s-long and 20s-long segments.
	</div>
	
	<table class="center">
		<tr class="header">
			<td><em>S<sub>d</sub></em> (s)</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em></td>
		</tr>
		<tr>
			<td>5</td><td>57.96%</td><td>99.03%</td><td>95%</td><td>93.75%</td><td>86.7%</td><td>47.78%</td><td>83.46%</td><td>49.26%</td>
		</tr>
		<tr>
			<td>10</td><td>52.95%</td><td>100%</td><td>98.33%</td><td>91.46%</td><td>70.3%</td><td>59.14%</td><td>70.09%</td><td>39.72%</td>
		</tr>
		<tr>
			<td>20</td><td>50.6%</td><td>98.01%</td><td>96.66%</td><td>78.94%</td><td>52.5%</td><td>61.59%</td><td>50.87%</td><td>38.93%</td>
		</tr>
	</table>
</div>

<h3><a name="5.3.3"></a>5.3.3 Analysis of the end-to-end latency</h3>

<p>Figure <a href="#fig:end-to-end-A2">5.24</a> depicts the end-to-end latency during the
session for the second scenario. As it can be seen in figure <a href="#fig:e2e-A2-m">5.24a</a>,
the mean mechanism has the strongest variation of end-to-end delay, starting
with 45 s of delay at the beginning of the session, being reduced to 11 s at
the end of the experiment (<em>t = 600</em> s). This behaviour is caused by successive
segments missed in <em>t = 200</em> s, <em>t = 450</em> s, and <em>t = 500</em> s. The aggressive
mechanism presents a similar behaviour to the mean mechanism, reducing the
end-to-end delay from 40 s (<em>t = 0</em> s) to 20 s (<em>t = 600</em> s).</p>

<p>Considering different segment durations (see figure <a href="#fig:e2e-A2-d">5.24b</a>), a
shorter segment duration (5s-long) reduces the performance in the first part (up
to <em>t = 300</em> s), since more segments are missed, reducing on several occasions
from an end-to-end delay of 70 s to 40 s. A longer segment duration improves
the stability of the end-to-end delay function throughout the session. Only one
change in the <em>e(t)</em> function occurs for 20s-long segments (<em>t = 150</em> s).</p>

<div class="figure">
	<a name="fig:end-to-end-A2"></a>
	<table class="center">
		<tr>
			<td>
				<a name="fig:e2e-A2-m"></a><img src="img/end-to-end-A2-a.png" alt="Different adaptation mechanisms."/>
			</td>
			<td>
				<a name="fig:e2e-A2-d"></a><img src="img/end-to-end-A2-b.png" alt="Different duration of segments."/>
			</td>
		</tr>
		<tr>
			<td><strong>(a)</strong> Different adaptation mechanisms</td><td><strong>(b)</strong> Different duration of segments</td>
		</tr>
	</table>
	
	<div class="caption">
		<span class="captionTitle">Figure 5.24</span> End-to-end latency throughout the session for scenario 2.
	</div>
</div>

<h3><a name="5.3.4"></a>5.3.4 Discussion</h3>

<p>The second network scenario was intended to test the behaviour of the adaptation
algorithms under more frequent variations of the available bandwidth. The
conservative mechanism selected more appropriately the bit-rates levels,
whereas the aggressive mechanism produced unnecessarily fluctuations,
especially in the first part of the experiment. The mean mechanism performed
with a mixed behaviour between that of the aggressive and the conservative
mechanisms, as it was able to select the appropriate quality level but required
an additional switching step. The most significant difference among the adaptation mechanism
is the number of segments which are re-downloaded due to variations in the
available bandwidth.</p>

<p>A larger size of the segment drastically reduced the reaction times, resulting
in a delayed selection of the bit-rate levels. In contrast, a shorter
segment duration produced the opposite effects, reducing the delay
between switching operations.</p>

<h2><a name="sec:scenario3"></a><a name="5.4"></a>5.4 Scenario 3: peaks in the available bandwidth</h2>

<p>The third network scenario (represented in figure <a href="#fig:scenario3">5.25</a>) supplies
a constant low bit-rate of 250 kb/s with high bit-rate peaks (more than 
ten times higher, 3 Mb/s) at intervals of one minute. The duration of these
peaks is increased over time, starting from 5 s up to 35 s. This
scenario aims to measure the reaction time (detection) of the client upon
extreme variations of bit-rate. This scenario is especially tricky and
challenging for the adaptive mechanisms: detecting the end of the peak becomes
the most important issue in order to avoid an empty buffer.</p>

<div class="figure">
	<a name="fig:scenario3"></a>
	<img src="img/scenario3-pipe.png" alt="Series of Dummynet pipes for the third network scenario over time."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.25</span> Series of Dummynet pipes for the third network scenario over time.
	</div>
</div>

<h3><a name="5.4.1"></a>5.4.1 Performance of the adaptation mechanisms</h3>

<p>The quality level selected for the three adaptive mechanism is depicted in
figures <a href="#fig:A3-1">5.26</a>, <a href="#fig:A3-2">5.27</a>, and <a href="#fig:A3-3">5.28</a>. Under this
scenario, the aggressive and conservative mechanisms illustrate a equal
behaviour in terms of detecting the bit-rate peaks. Neither of these two methods
switches to a higher quality level for the first peak (of 5 s duration).
However, adaptation is successfully performed in the following six peaks.</p>

<div class="figure">
	<a name="fig:A3-1"></a>
	<img src="img/A3-1.png" alt="Performance of the aggressive mechanism over the scenario 3."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.26</span> Performance of the aggressive mechanism over the scenario 3.
	</div>
</div>

<div class="figure">
	<a name="fig:A3-2"></a>
	<img src="img/A3-2.png" alt="Performance of the conservative mechanism over the scenario 3."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.27</span> Performance of the conservative mechanism over the scenario 3.
	</div>
</div>

<p>The most inadequate behaviour was observed in the case of the mean algorithm
(figure <a href="#fig:A3-3">5.28</a>), which presents an irregular adaptation throughout the
session. It is capable of detecting all the peaks, but the switching down is
performed too late as compared to the other two mechanisms. The worst situation
happens at <em>t = 290 s</em>, when bit-rate is reduced from 3 Mb/s to 250 kb/s.
The algorithm maintained the quality level for 62 s, leading to an empty
buffer - hence no playout.</p>

<div class="figure">
	<a name="fig:A3-3"></a>
	<img src="img/A3-3.png" alt="Performance of the mean mechanism over the scenario 3."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.28</span> Performance of the mean mechanism over the scenario 3.
	</div>
</div>

<h4><a name="5.4.1.1"></a>5.4.1.1 Impact on the metrics</h4>

<p>Results of this experiments are listed in table <a href="#tab:A3-m">5.13</a> and represented
graphically in figure <a href="#fig:A3-m">5.29</a>. The stops during playback and the number
of missed segments are frequent occurrences in this network
scenario, consequently reducing <em><span class="symbol">e</span><sub>retry</sub></em>, <em><span class="symbol">e</span><sub>fetch</sub></em>,
and <em><span class="symbol">e</span><sub>buf</sub></em>.</p>

<div class="table">
	<a name="tab:A3-m"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.13</span> Metrics comparison under network scenario 3 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Mechanism</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em></td>
		</tr>
		<tr>
			<td>Aggressive</td><td>38.87%</td><td>84.37%</td><td>80%</td><td>77.31%</td><td>100%</td><td>74.25%</td><td>85.71%</td><td>40.23%</td>
		</tr>
		<tr>
			<td>Conservative</td><td>34.54%</td><td>94.22%</td><td>91%</td><td>83.33%</td><td>100%</td><td>68.89%</td><td>68.74%</td><td>36.63%</td>
		</tr>
		<tr>
			<td>Mean</td><td>31.22%</td><td>83.07%</td><td>78.33%</td><td>77.31%</td><td>100%</td><td>71.43%</td><td>80.85%</td><td>34.60%</td>
		</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:A3-m"></a>
	<img src="img/A3-mechanism.png" alt="Graphical comparison under network scenario 3 for aggressive,
	conservative, and mean adaptive mechanisms."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.29</span> Graphical comparison under network scenario 3 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
</div>

<p>The conservative mechanism provides the best management of the
fluctuations produced by the scenario, but it has poor reaction
(<em><span class="symbol">e</span><sub>up</sub> = 68.74%</em>, <em><span class="symbol">e</span><sub>down</sub> = 36.63%</em>) and start-up
efficiencies (<em><span class="symbol">e</span><sub>start-up</sub> = 68.89%</em>). In particular, <em><span class="symbol">e</span><sub>up</sub></em>
shows better values for the aggressive (85.71%) and the mean mechanisms
(80.85%).</p>

<h3><a name="5.4.2"></a>5.4.2 Performance with different duration of segments</h3>

<p>Figure <a href="#fig:A3-2-5">5.30</a> and figure <a href="#fig:A3-2-20">5.31</a> depict the performance of
the conservative mechanism with 5s-long and 20s-long
segments respectively. All peaks are successfully detected in both cases (as
the throughput curve denotes). However, the essential difference is present in
the first peaks (shorter in time, produced at <em>t = 60</em> s and <em>t = 125</em> s) which
are adequately utilized with shorter segments (figure <a href="#fig:A3-2-5">5.30</a>) in
comparison with longer segments (figure <a href="#fig:A3-2-20">5.31</a>).</p>

<div class="figure">
	<a name="fig:A3-2-5"></a>
	<img src="img/A3-2-5.png" alt="Performance of the conservative mechanism over the scenario 3
  with a segment duration of 5 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.30</span> Performance of the conservative mechanism over the scenario 3
  with a segment duration of 5 s.
	</div>
</div>

<div class="figure">
	<a name="fig:A3-2-20"></a>
	<img src="img/A3-2-20.png" alt="Performance of the conservative mechanism over the scenario 3
  with a segment duration of 20 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.31</span> Performance of the conservative mechanism over the scenario 3
  with a segment duration of 20 s.
	</div>
</div>

<h4><a name="5.4.2.1"></a>5.4.2.1 Impact on the metrics</h4>

<p>Results of this experiments are expressed in table <a href="#tab:A3-d">5.14</a> and
figure <a href="#fig:A3-d">5.32</a>. Results present high disparity for all the coefficients,
meaning that the segment duration has a strong importance on this type of
scenario.</p>

<p>The reaction (up and down) efficiencies and the active efficiency are the
metrics which presents more variation (more than 50%), whereas the buffering
efficiency (<em><span class="symbol">e</span><sub>buf</sub></em>) is less affected by the length of segments
(from 6.97% to 19.43%, greater for shorter segments).</p>

<div class="table">
	<a name="tab:A3-d"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.14</span> Metrics comparison under network scenario 3 for
		5s-long, 10s-long, and 20s-long segments.
	</div>
	
	<table class="center">
		<tr class="header">
			<td><em>S<sub>d</sub></em> (s)</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em></td>
		</tr>
		<tr>
			<td>5</td><td>42.87%</td><td>86.4%</td><td>79.16%</td><td>86.70%</td><td>100%</td><td>50.88%</td><td>85.01%</td><td>53.83%</td>
		</tr>
		<tr>
			<td>10</td><td>34.54%</td><td>94.22%</td><td>91.66%</td><td>83.33%</td><td>100%</td><td>68.89%</td><td>68.74%</td><td>36.63%</td>
		</tr>
		<tr>
			<td>20</td><td>30.27%</td><td>90.63%</td><td>93.33%</td><td>74.25%</td><td>55.8%</td><td>58.90%</td><td>62.03%</td><td>29.05%</td>
		</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:A3-d"></a>
	<img src="img/A3-duration.png" alt="Graphical comparison under network scenario 3 for 5s-long,
  10s-long, and 20-seconds long segments."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.32</span> Graphical comparison under network scenario 3 for 5s-long,
  10s-long, and 20-seconds long segments.
	</div>
</div>

<h3><a name="5.4.3"></a>5.4.3 Analysis of the end-to-end latency</h3>

<p>Figure <a href="#fig:end-to-end-A3">5.33</a> depicts the end-to-end latency in the experiments
in the third scenario. The most noticeable occurrence is the irregularities of
the end-to-end delay throughout the session for all cases, independent of
the chosen adaptation mechanism or segment duration. Results of these
experiments show that peaks in the network's available bandwidth have a strong
influence on the segments' playback time-stamps, i.e., when the media
segments are played.</p>

<p>The longest playback interruption can be seen in the mean mechanism at <em>t =
323</em> s (see figure <a href="#fig:e2e-A3-m">5.33a</a>), when the player was buffering content
during 27 s. As a result, the end-to-end delay was abruptly increased from 37 s
to 64 s. Several segments are missed after this occurrence, as it can be
seen in a reduction of 26 s in the end-to-end delay (<em>t = 436</em> s).</p>

<p>The aggressive and conservative mechanisms only achieve stability in the <em>e(t)</em>
function in the second part of the experiment (from <em>t = 300</em> s), when bit-rate
peaks last for more than 20 seconds.</p>

<p>Reducing the segment duration does not improve the end-to-end delay stability.
As depicted in figure <a href="#fig:e2e-A3-d">5.33b</a>, using 5s-long segments decreases
monotonically the <em>e(t)</em> function more than 50 s. A longer segment duration
(20 s) produces strongest variations in the end-to-end delay for the shorter
peaks (as it can be seen at <em>t = 96</em> s, <em>t = 143</em> s and <em>t = 240</em> s). Stability
in the <em>e(t)</em> function for 20s-long segments is poorly achieved in the second
part of the experiment due to playback interruptions (<em>t = 404</em> s and <em>t =
410</em> s).</p>

<div class="figure">
	<a name="fig:end-to-end-A3"></a>
	<table class="center">
		<tr>
			<td>
				<a name="fig:e2e-A3-m"></a><img src="img/end-to-end-A3-a.png" alt="Different adaptation mechanisms"/>
			</td>
			<td>
				<a name="fig:e2e-A3-d"></a><img src="img/end-to-end-A3-b.png" alt="Different duration of segments"/>
			</td>
		</tr>
		<tr>
			<td><strong>(a)</strong> Different adaptation mechanisms</td>
			<td><strong>(b)</strong> Different duration of segments</td>
		</tr>
	</table>
	
	<div class="caption">
		<span class="captionTitle">Figure 5.33</span> End-to-end latency throughout the session for scenario 3.
	</div>
</div>

<h3><a name="5.4.4"></a>5.4.4 Discussion</h3>

<p>The third scenario was intended to test the behaviour of the adaptation
algorithms under high-bandwidth-peaks of different durations. The overall performance of
the aggressive and the mean algorithm was worse than the conservative
mechanism, although the latter showed a notable irregularity in the selection
of bit-rate levels. The computed metrics show that the conservative mechanism
prevented playback interruptions better than the other mechanisms.</p>

<p>The size of the segments has a significant influence in this scenario.
The use of shorter segments improves the reaction times, but increases the
probability of missed segments due to <span class="code">HTTP-404</span> responses. A larger size of
the segments leads to a more inactive player, since fewer HTTP requests need to
be sent. As a consequence, peaks in the available bandwidth are not used
appropriately (peaks are detected but segments are more probable to be
re-downloaded since the segment size in bytes is much bigger).</p>


<h2><a name="sec:scenario4"></a><a name="5.5"></a>5.5 Scenario 4: troughs in the available bandwidth</h2>

<p>The fourth network scenario presents the opposite situation of the third
scenario (evaluated in section <a href="#sec:scenario3">5.4</a>). Scenario 4 supplies a
continuous high bit-rate of 3 Mb/s, with low bit-rate troughs (more than
ten times lower, 250 kb/s) every minute, as shown in
figure <a href="#fig:scenario4">5.34</a>. Duration of these troughs is also increased over
time, the first troughs lasts 5 s increasing up to 35 s in the last
one.</p>

<div class="figure">
	<a name="fig:scenario4"></a>
	<img src="img/scenario4-pipe.png" alt="Series of Dummynet pipes for the fourth network scenario."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.34</span> Series of Dummynet pipes for the fourth network scenario.
	</div>
</div>

<h3><a name="5.5.1"></a>5.5.1 Performance of the adaptation mechanisms</h3>

<p>Figure <a href="#fig:A4-1">5.35</a>, <a href="#fig:A4-2">5.36</a>, and <a href="#fig:A4-3">5.37</a> show the performance
of the three adaptive mechanisms under the network conditions of the fourth
network scenario. The most noticeable difference is the election of the representation
level during the abrupt decreases in the available bandwidth.</p>

<p>The aggressive mechanism (figure <a href="#fig:A4-1">5.35</a>) reduced the quality level
following the first trough. However, for the following two troughs it maintained
the maximum level, keeping the bandwidth utilization at 100% between troughs.
After the fourth, fifth, and sixth troughs (<em>t = 270</em> s, <em>t = 350</em> s, and <em>t =
435</em> s, respectively) the level was switch down to an intermediate level, since the measured
throughput was imprecise (the bandwidth detected during troughs was
(incorrectly) estimated to be over 500 kb/s). During the last trough (<em>t =
525</em> s), the buffer reached the minimum due to several segments which were
re-downloaded. As a result, the level was set to the minimum and the bandwidth
was correctly estimated.</p>

<div class="figure">
	<a name="fig:A4-1"></a>
	<img src="img/A4-1.png" alt="Performance of the aggressive mechanism over the scenario 4."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.35</span> Performance of the aggressive mechanism over the scenario 4.
	</div>
</div>

<p>Figure <a href="#fig:A4-2">5.36</a> depicts the behaviour of the conservative mechanism. As
expected, quality levels are selected slightly below the optimal. For all
troughs this mechanism successfully lowered the bit-rate level to 1 Mb/s or
below. The buffering procedure was significantly better for the overall session in
comparison with the aggressive mechanism. From the start until <em>t =
100</em> s the buffer fluctuates between one and two segments stored (after 10 s and
20 s respectively). After that interval, the buffer is never empty.</p>

<div class="figure">
	<a name="fig:A4-2"></a>
	<img src="img/A4-2.png" alt="Performance of the conservative mechanism over the scenario 4."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.36</span> Performance of the conservative mechanism over the scenario 4.
	</div>
</div>

<p>The mean mechanism (figure <a href="#fig:A4-3">5.37</a>) presented a mixed behaviour between
the aggressive and the conservative mechanisms. Troughs are correctly detected,
but the selected bit-rate level is greater than the client's measured
throughput. Only in the trough produced in <em>t = 435</em> s does the quality level
reached the minimum, while in the rest of the troughs the quality selected
was between 1 Mb/s and 1.5 Mb/s. In addition, the buffer becomes empty in the
last three troughs.</p>

<div class="figure">
	<a name="fig:A4-3"></a>
	<img src="img/A4-3.png" alt="Performance of the mean mechanism over the scenario 4."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.37</span> Performance of the mean mechanism over the scenario 4.
	</div>
</div>

<h4><a name="5.5.1.1"></a>5.5.1.1 Impact on the metrics</h4>

<p>Table <a href="#tab:A4-m">5.15</a> and figure <a href="#fig:A4-m">5.38</a> summarize the experimental
results under this scenario. The active efficiency (<em><span class="symbol">e</span><sub>active</sub></em>) is
the metric which shows the biggest variation for the different algorithms (more
than 50%) being the best case <em><span class="symbol">e</span><sub>active</sub> = 86.74</em> for the mean mechanism.
However, the bandwidth utilization efficiency (<em><span class="symbol">e</span><sub>bw</sub></em>) is quite similar in the
three cases (around 70%), meaning that the mean algorithm spent more
time downloading segments without a clear adaptation improvement. This leads to
a reduction of the segment-retry efficiency (<em><span class="symbol">e</span><sub>retry</sub></em>).</p>

<div class="table">
	<a name="tab:A4-m"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.15</span> Metrics comparison under network scenario 4 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Mechanism</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em></td>
		</tr>
		<tr>
			<td>Aggressive</td><td>72.15%</td><td>100%</td><td>96%</td><td>92.59%</td><td>39.41%</td><td>87.26%</td><td>58.77%</td><td>40.59%</td>
		</tr>
		<tr>
			<td>Conservative</td><td>66.45%</td><td>100%</td><td>100%</td><td>90.36%</td><td>36.9%</td><td>89.69%</td><td>61.12%</td><td>58.06%</td>
		</tr>
		<tr>
			<td>Mean</td><td>69.94%</td><td>99%</td><td>98%</td><td>85.22%</td><td>86.74%</td><td>71.91%</td><td>58.06%</td><td>45.96%</td>
		</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:A4-m"></a>
	<img src="img/A4-mechanism.png" alt="Graphical comparison under network scenario 4 for aggressive,
	conservative, and mean adaptive mechanisms."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.38</span> Graphical comparison under network scenario 4 for aggressive,
	conservative, and mean adaptive mechanisms.
	</div>
</div>

<h3><a name="5.5.2"></a>5.5.2 Performance with different duration segments</h3>

<p>Figure <a href="#fig:A4-2-5">5.39</a> and figure <a href="#fig:A4-2-20">5.40</a> illustrate the behaviour of
the conservative mechanism with a segment duration of 5 s and 10 s. Reaction
times are significantly improved for the case of shorter segment duration, as
it is shown in figure <a href="#fig:A4-2-5">5.39</a>. The quality level is reduced in all
troughs accordingly, less than 500 kb/s except for the shortest trough (<em>t =
60</em> s), where the bit-rate is only decreased to 800 kb/s. It is important to
point out that the periods when the quality was lowered are shorter.</p>

<div class="figure">
	<a name="fig:A4-2-5"></a>
	<img src="img/A4-2-5.png" alt="Performance of the conservative mechanism over the scenario 4
  with a segment duration of 5 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.39</span> Performance of the conservative mechanism over the scenario 4
  with a segment duration of 5 s.
	</div>
</div>

<p>The behaviour of the conservative mechanism with a longer duration segments
(20 s) is illustrated in figure <a href="#fig:A4-2-20">5.40</a>. The resulting adaptation is
poor compared to 5s-long and 10s-long segments, presenting no
benefit. The reduction of bit-rate clearly influences the selection of the
quality level, leading to an under-estimation of the available bandwidth in the
network. This effect is illustrated in the troughs produced at <em>t = 60</em> s, <em>t =
125</em> s, <em>t = 195</em> s, and <em>t = 525</em> s. After the fourth trough (<em>t = 270</em> s), the
mechanism could not reach the maximum quality level.</p>

<div class="figure">
	<a name="fig:A4-2-20"></a>
	<img src="img/A4-2-20.png" alt="Performance of the conservative mechanism over the scenario 4
  with a segment duration of 20 s."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.40</span> Performance of the conservative mechanism over the scenario 4
  with a segment duration of 20 s.
	</div>
</div>

<h4><a name="5.5.2.1"></a>5.5.2.1 Impact on the metrics</h4>

<p>Results of scenario 4 with different segment durations are presented in 
Table <a href="#tab:A4-d">5.16</a> and figure <a href="#fig:A4-d">5.41</a>. They reveal that the use of a
shorter segment duration significantly improves the reaction efficiencies
(<em><span class="symbol">e</span><sub>up</sub></em> and <em><span class="symbol">e</span><sub>down</sub></em>), the active efficiency, and most
importantly, the utilization of bandwidth (<em><span class="symbol">e</span><sub>bw</sub></em>) is about 10%
better than using 10s-long segments. The cost of these improvements are mainly
the increased number of segments re-downloaded and missed (leading to a
reduction in the <em><span class="symbol">e</span><sub>retry</sub></em> and <em><span class="symbol">e</span><sub>fetch</sub></em> efficiencies,
respectively).</p>

<p>In contrast, using a longer duration segment (20 s) does not provide
significant benefits. In comparison with 10s-long segments, only the
active efficiency (<em><span class="symbol">e</span><sub>active</sub> = 42.6</em>) and the switching-down
efficiency (<em><span class="symbol">e</span><sub>down</sub> = 47.58</em>) are slightly increased (less than
10%), while the rest of the metrics had equal values.</p>

<div class="figure">
	<a name="fig:A4-d"></a>
	<img src="img/A4-duration.png" alt="Graphical comparison under network scenario 4 for 5s-long,
  10s-long, and 20-seconds long segments."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.41</span> Graphical comparison under network scenario 4 for 5s-long,
  10s-long, and 20-seconds long segments.
	</div>
</div>

<div class="table">
	<a name="tab:A4-d"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.16</span> Metrics comparison under network scenario 4 for
		5s-long, 10s-long, and 20s-long segments.
	</div>
	
	<table class="center">
		<tr class="header">
			<td><em>S<sub>d</sub></em> (s)</td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td><td><em><span class="symbol">e</span><sub>up</sub></em></td><td><em><span class="symbol">e</span><sub>down</sub></em></td>
		</tr>
		<tr>
			<td>5</td><td>76.97%</td><td>99.79%</td><td>92.5%</td><td>91.46%</td><td>100%</td><td>79.70%</td><td>79.69%</td><td>52.07%</td>
		</tr>
		<tr>
			<td>10</td><td>64.45%</td><td>100%</td><td>100%</td><td>90.36%</td><td>36.9%</td><td>89.69%</td><td>61.26%</td><td>44.21%</td>
		</tr>
		<tr>
			<td>20</td><td>52.92%</td><td>100%</td><td>100%</td><td>82.41%</td><td>42.6%</td><td>89.57%</td><td>59.70%</td><td>47.58%</td>
		</tr>
	</table>
</div>

<h3><a name="5.5.3"></a>5.5.3 Analysis of the end-to-end latency</h3>

<p>Figure <a href="#fig:end-to-end-A4">5.42</a> depicts the end-to-end latency for the
experiments performed under the fourth network scenario. As depicted in
figure <a href="#fig:e2e-A4-m">5.42a</a>, the three mechanisms performed equally in the first
part of the experiment (from <em>t = 0</em> s to <em>t = 300</em> s), achieving a constant
end-to-end delay. During the second part of the session (from <em>t = 300</em> s to <em>t
= 600</em> s) the aggressive and mean mechanisms received several <span class="code">HTTP-404</span>
responses (as it can be seen in <em>t = 374</em> s, <em>t = 456</em> s, and <em>t = 558</em> s),
resulting in a reduction of the value of the <em>e(t)</em> function.</p>

<p>Figure <a href="#fig:e2e-A4-d">5.42b</a> represents the resulting end-to-end delay using
different duration of segments. A shorter segment duration (5 s) produces
more variation in the <em>e(t)</em> function when troughs are longer than 20 s
(as in the second part of the experiment, from <em>t = 300</em> s). In contrast,
choosing a segment duration of 10 s or 20 s does not affect the end-to-end
latency during the whole session, meaning that segments are not missed and playback is not
interrupted (the buffer is never empty when the next segment must be played).</p>

<div class="figure">
	<a name="fig:end-to-end-A4"></a>
	<table class="center">
		<tr>
			<td><a name="fig:e2e-A4-m"></a><img src="img/end-to-end-A4-a.png" alt="Different adaptation mechanisms"/></td>
			<td><a name="fig:e2e-A4-d"></a><img src="img/end-to-end-A4-b.png" alt="Different duration of segments"/></td>
		</tr>
		<tr>
			<td><strong>(a)</strong> Different adaptation mechanisms</td>
			<td><strong>(b)</strong> Different duration of segments</td>
		</tr>
	</table>
	
	<div class="caption">
		<span class="captionTitle">Figure 5.42</span> End-to-end latency throughout the session for scenario 4.
	</div>
</div>

<h3><a name="5.5.4"></a>5.5.4 Discussion</h3>

<p>The fourth scenario was intended to test the behaviour of the adaptation
algorithms under low-bandwidth-peaks of different durations. The three proposed
mechanisms performed efficiently under this scenario, preventing playback
interruptions and successfully downloading the media segments when they were
available on the server. Results of these experiments indicated a major
difference in the number of buffered segments during the session. The
conservative mechanism always filled the buffer in time, whereas the aggressive
and mean mechanism experienced a reduction in the buffer level during the second
part of the experiments, when troughs were significantly longer. It is
noteworthy that the mean algorithm consumed most of the session time in the
active state, i.e., downloading segments, whereas the aggressive and the
conservative mechanism spent a significant amount of time in the inactive
state, which could be used to re-download segments which were already buffered
at a higher quality.</p>

<p>Results regarding the effects of the size of the segments are very similar to
the results of the third scenario (peaks in bandwidth). A shorter duration of
the segments improves most of the metrics considered in our evaluation, whereas
a longer duration of segments does not lead to any significant improvement
over the reference duration (10 s).</p>

<h2><a name="5.6"></a>5.6 Effects of packet loss</h2>

<p>The effects of packet loss in the underlying network was measured through a set
of four experiments where packets are lost according to a probability of
error <em>p<sub>e</sub></em>: 5%, 10%, 15%, and 20%. Table <a href="#tab:packet-loss">5.17</a> enumerates
the input parameters which were fixed for the experiments carried out in this
section.</p>

<p>In order to evaluate the effects of packet losses on playback, the conservative
mechanism has been chosen since this algorithm achieved better buffering
efficiency in the previous experiments. The duration of the segments was fixed
at the reference value (10 s). The emulated bandwidth was constant at 2 Mb/s
during the whole session (from <em>t = 0</em> s to <em>t = 600</em> s).</p>

<div class="table">
	<a name="tab:packet-loss"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.17</span> Input parameters.
	</div>
	
	<table class="center">
		<tr class="header">
			<td>Mechanism</td><td><em>T</em> (s)</td><td><em>R</em></td><td><em>bw<sub>avail</sub></em> (Mb/s)</td><td><em>S<sub>d</sub></em> (s)</td>
		</tr>
		<tr>
			<td>Conservative</td><td>600</td><td>10</td><td>2</td><td>10</td>
		</tr>
	</table>
</div>

<h3><a name="5.6.1"></a>5.6.1 Impact on the metrics</h3>

<p>Table <a href="#tab:A5">5.18</a> and figure <a href="#fig:A5">5.43</a> show the results of these
experiments. Note that the reaction efficiencies have not been considered, since the
network's available bandwidth is constant throughout the session (i.e., there
are no bandwidth fluctuations that the client may detect.).</p>

<div class="table">
	<a name="tab:A5"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.18</span> Metrics comparison under different probability of packet losses.
	</div>
	
	<table class="center">
		<tr class="header">
			<td><em>p<sub>e</sub></em></td><td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td>
		</tr>
		<tr>
			<td>5%</td><td>51.17%</td><td>100%</td><td>100%</td><td>100%</td><td>41.80%</td><td>88.97%</td>
		</tr>
		<tr>
			<td>10%</td><td>14.72%</td><td>95.18%</td><td>93.33%</td><td>70.66%</td><td>100%</td><td>83.71%</td>
		</tr>
		<tr>
			<td>15%</td><td>5.66%</td><td>51.70%</td><td>55%</td><td>80%</td><td>100%</td><td>59.83%</td>
		</tr>
		<tr>
			<td>20%</td><td>4.14%</td><td>54.64%</td><td>56.66%</td><td>98.66%</td><td>100%</td><td>55.02%</td>
		</tr>
	</table>
</div>

<p>In general, increasing the probability of packet loss has a strong influence on
all the metrics defined in our evaluation. <em><span class="symbol">e</span><sub>bw</sub></em>,
<em><span class="symbol">e</span><sub>buf</sub></em>, <em><span class="symbol">e</span><sub>fetch</sub></em>, and <em><span class="symbol">e</span><sub>start-up</sub></em> showed
a clear dependency on <em>p<sub>e</sub></em>. The higher the probability of error, the lower
these metrics are. The computed values for the active efficiency
(<em><span class="symbol">e</span><sub>fetch</sub></em>) are rather simple to deduce: with low probability of
error, the client is able to fetch the segments on time, switching to the
inactive state when all available segments are downloaded. Increasing the
probability of error leads to a more active client, as it must spend more time
fetching the segments (due to packet losses, the HTTP transactions last longer).</p>

<p>Interesting results can be observed for the segment-retry efficiency
(<em><span class="symbol">e</span><sub>retry</sub></em>). Up to <em>p<sub>e</sub> = 10</em>% the efficiency decreases since more
segments are being discarded. However, for <em>p<sub>e</sub> = 15</em>% and <em>p<sub>e</sub> = 20</em>% the
efficiency is increased. Although it seems to be an improvement, these values
simply indicate that the segments are not being fully downloaded or are
downloaded at the lowest quality level, therefore the client cannot discard them.</p>

<div class="figure">
	<a name="fig:A5"></a>
	<img src="img/A5.png" alt="Graphical comparison under different probability of packet losses."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.43</span> Graphical comparison under different probability of packet losses.
	</div>
</div>

<h3><a name="5.6.2"></a>5.6.2 Discussion</h3>

<p>Results of these experiments have shown that the player avoids playback
interruptions and fetches the segments in time for packet loss rates of up to
10% of the network traffic transferred between the server and client. When the
probability of error reaches 15% of network traffic, then the quality of
service is drastically reduced, since the playback is interrupted 50% of
the session time and half of the segments are not fetched in time. Therefore, in a
2 Mb/s channel, the client's application is able to maintain an acceptable
quality of service with up to 10% probability of packet losses.</p>

<h2><a name="sec:live-tv"></a><a name="5.7"></a>5.7 Evaluation with real live events</h2>

<p>The performance of the client's application under real live scenarios is
evaluated in this section. A survey has been carried out to determine which TV
channels currently offer an HTTP-based alternative following any of the adaptive
streaming protocols introduced in section <a href="#sec:http-adaptive-streaming">2.4</a>.
During this master's thesis project, there was no TV channel offering content
using the MPEG-DASH protocol, although there were several using Apple-HLS and
Microsoft-LSS.</p>

<p>The final match of the FIFA Women's World Cup (2011) was selected as the
live event for this experiment. The match was broadcasted by the Eurosport
channel. Eurosport provides free live content using
Apple-HLS<a href="#footnote3" name="note38"><sup>38</sup></a>.
Table <a href="#tab:eurosport">5.19</a> summarizes the characteristics the offered media
stream. Eurosport provides segments with a duration of 10 s (<em>S<sub>d</sub> = 10</em> s) and
an available shifting time of 30 s (<em>T<sub>shift</sub> = 30</em> s), i.e., only three media
segments are available at any point of time.</p>

<div class="table">
	<a name="tab:eurosport"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.19</span> Characteristics offered by the Eurosport channel over Apple-HLS.
	</div>
	
	<table class="center">
		<tr class="header">
			<td><em>S<sub>d</sub></em> (s)</td><td><em>T<sub>shift</sub></em> (s)</td><td><em>R</em></td><td>Avg. bit-rate (kb/s)</td><td>CODEC</td><td>Media container</td>
		</tr>
		<tr>
			<td>10</td><td>30</td><td>1</td><td>900</td><td>H.264</td><td>MPEG-TS</td>
		</tr>
	</table>
</div>

<p>The experiment was carried out from 21:00:33 to 23:05:10 on the 17th, July
2011, resulting in a session time of 02:04:37 (<em>t = 7477</em> s).
Listing <a href="#lst:eurosport">5.1</a> shows the extended M3U playlist
offered by the server. This playlist was updated every 10 s (as specified
by the <span class="code">#EXT-X-TARGETDURATION</span> tag). Since only one quality level is offered on
the server's side (with a bit-rate of 900 kb/s), this experiment focused
on the long-term behaviour of the client's application with a live
event. In particular, the costs of the transcoding step (see
section <a href="#sec:transcoder-module">4.1.1</a>) in terms of the quality of service during
playback was analyzed.</p>

<div class="listing">
	<a name="lst:eurosport"></a>
	<div class="caption">
		<span class="captionTitle">Listing 5.1</span>Extended M3U playlist retrieved from the Eurosport's HTTP
server.
	</div>
<pre>
#EXTM3U
#EXT-X-TARGETDURATION:10
#EXT-X-MEDIA-SEQUENCE:
#EXTINF:10,
2011-07-17/V0A0/14/Media_20110717_19452220_19453220_0_0.mp4
#EXTINF:10,
2011-07-17/V0A0/14/Media_20110717_19453220_19454220_0_0.mp4
#EXTINF:10,
2011-07-17/V0A0/14/Media_20110717_19454220_19455220_0_0.mp4
</pre>
</div>

<h3><a name="5.7.1"></a>5.7.1 Impact on the metrics</h3>

<p>Table <a href="#tab:world-cup">5.20</a> and figure <a href="#fig:world-cup">5.44</a> show the results of the
experiment on a real live event. The bandwidth, buffering, fetch, and retry
efficiencies reached their maximum value (<em><span class="symbol">e</span><sub>bw</sub></em>,
<em><span class="symbol">e</span><sub>buf</sub></em>, <em><span class="symbol">e</span><sub>fetch</sub></em>, and <em><span class="symbol">e</span><sub>fetch</sub></em> respectively), hence not a
single segment was missed during the match and there were no interruptions in
playback. The active efficiency (<em><span class="symbol">e</span><sub>active</sub></em>) was about 50%, meaning
that the client was able to fetch the segments in time and spent a significant
amount of time in the inactive state. The start-up efficiency
(<em><span class="symbol">e</span><sub>start-up</sub></em>) indicates that the client is able to start playback
rapidly even though media files needed to be converted into a format
supported by Stagefright.</p>

<div class="table">
	<a name="tab:world-cup"></a>
	<div class="caption">
		<span class="captionTitle">Table 5.20</span> Metrics comparison in a real live event.
	</div>
	
	<table class="center">
		<tr class="header">
			<td><em><span class="symbol">e</span><sub>bw</sub></em></td><td><em><span class="symbol">e</span><sub>buf</sub></em></td><td><em><span class="symbol">e</span><sub>fetch</sub></em></td><td><em><span class="symbol">e</span><sub>retry</sub></em></td><td><em><span class="symbol">e</span><sub>active</sub></em></td><td><em><span class="symbol">e</span><sub>start-up</sub></em></td>
		</tr>
		<tr>
			<td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>50.06%</td><td>74.67%</td>
		</tr>
	</table>
</div>

<div class="figure">
	<a name="fig:world-cup"></a>
	<img src="img/world-cup.png" alt="Graphical comparison of metrics in a real live event."/>
	<div class="caption">
		<span class="captionTitle">Figure 5.44</span> Graphical comparison of metrics in a real live event.
	</div>
</div>

<p>The average downloading time for the 10s-long segments throughout the whole
session was <em>3.84</em> s (38.4% of the segment duration, <em>S<sub>d</sub> = 10</em> s), including
the conversion. The average time consumed by conversion step was <em>2.41</em> s
(24.1% of the segment duration, <em>S<sub>d</sub> = 10</em> s). Hence, the time required for the client to
store Stagefright-compatible segments into the buffer was increased <em>1.43</em> s on
average (14.3% of the segment duration, <em>S<sub>d</sub> = 10</em> s).</p>

<h3><a name="5.7.2">5.7.2 Discussion</a></h3>

<p>This experiment was intended to evaluate the behaviour of the client's
application with a real live event. Results show that the quality of
service provided on the client's side is very high since there were no
interruptions or missed fragments. The client's application
consumes a significant amount of time converting media files. However, this
procedure was performed in the background sufficiently quickly that the quality
of playback was not decreased.</p>


<h1><a name="conclusions"></a><a name="6"></a><span class="num">6</span>Conclusions</h1>

<div class="quoteblock">
	<p class="quote">"The world always seems brighter when you've just made something that wasn't there before."</p>
	<p class="quoteAuthor">- Neil Gaiman</p>
</div>

<p>In this chapter the findings of this master's thesis project are presented.
Section <a href="#sec:discussion">6.1</a> presents a discussion of the results achieved in
the previous chapter. Section <a href="#sec:further-improvements">6.2</a> enumerates some of
the possible improvements to this project which should be considered in future
works.</p>


<h2><a name="sec:discussion"></a><a name="6.1"></a><span class="num">6.1</span>Discussion</h2>

<p>In this master's thesis project a full service has been proposed in order to
evaluate the benefits of different adaptive streaming mechanisms using HTTP as a
delivery protocol. Three mechanisms were proposed to provide bit-rate
adaptation on the client's side: the <em>aggressive</em>, <em>conservative</em>, and
<em>mean</em> algorithms.</p>

<p>Results of the experiments with heterogeneous network scenarios have shown
that the adaptation mechanisms efficiently utilize the available bandwidth of
the network. The aggressive mechanism produces an adequate adaptation to
short-term bandwidth fluctuations, although this mechanism increases
the probability of discarded and missed segments due to bandwidth
underestimation. The conservative mechanism prevents playback stops, at the
cost of a non-optimal utilization of available bandwidth in the
short-term, although experiments have shown that bandwidth utilization
is equal to that of the aggressive mechanism in the long-term. The mean
mechanism presents a similar performance to the aggressive mechanism, although
it consumes more time downloading segments.</p>

<p>Major differences can be seen in the level of activity of the media player.
While using the mean algorithm the player remains mostly in the active state
throughout the session, the aggressive and conservative mechanisms spent a
considerable time in the inactive state. These inactive intervals could have
been used to re-download some segments at a better quality, hence improving
the overall use of the available bandwidth.</p>

<p>The reaction times to switch between media bit-rates were significantly
better when the available bit-rate was increased rather than reduced, since
segments were downloaded faster. As a consequence, the throughput measurements
occur earlier and the adaptation mechanisms decide upon the next appropriate
level of quality sooner.</p>

<p>Reducing the size of the segments improves the reaction times to variation of
the network bit-rate, at the cost of increased activity by the client. More time
is consumed sending and receiving HTTP messages, hence the probability of
missing a segment, i.e., the probability of receiving a <span class="code">HTTP-404</span>
response, is increased. The bit-rate adaptation is also improved for shorter
segments since the bandwidth measurements occur more often. The opposite
situation happens with larger segments, hence the network's available bandwidth
is used less efficiently since measurements are taken less frequently. In
addition, the switching operations are performed significantly later. However,
downloading longer segments leads to a more inactive client, since fewer
segments need to be downloaded in order to play the media content. The client
spends more time in the inactive state, which could be used to perform other
actions, for example, to fetch segments at higher bit-rates.</p>

<p>Finally, experiments indicate that the client's application is able to maintain
an acceptable quality of service with up to 10% packet losses in the
underlying network.</p>

<h2><a name="sec:further-improvements"></a><a name="6.2"></a><span class="num">6.2</span>Future work</h2>

<p>This section explores some of the limitations and assumptions made in the
prototype developed for this master's thesis project, in order to be improved in
future works.</p>
  
<p>For simplicity, the algorithms defined in section <a href="#sec:algorithms">4.4.2</a> are only
invoked when segments are fully downloaded, thus restricting the adaptation
procedure to the boundaries of segments. This leads to infrequent adaptation in
the case of larger segments (as shown throughout the evaluation in the previous
chapter). The algorithms use the throughput calculated for each media segment
as an input parameter. The throughput measurement could be computed several
times in the case of larger segments, in order to more frequently inform the
adaptation algorithms about the characteristics of the underlying network.</p>

<p>The conservative mechanism can be improved by always fetching the next lower
level (than the current long-term bandwidth allows) instead of
multiplying the throughput measurement by a sensitivity parameter (see
algorithm <a href="#alg:conservative">2</a>). Then, if there is additional time fetch
the delta to this lower level that would bring up the quality level. Thus,
there will be always segments to play, improving the quality when there
is extra bandwidth. This improvement meets the first and
third requirements defined in section <a href="#sec:algorithms">4.4.2</a>, while
approximating the second requirement.</p>

<p>In section <a href="#sec:re-download">4.4.6</a> a skipping mechanism was defined to discard
media segments upon reduction of the network's available bandwidth to
re-download segments at a lower quality level. This procedure considers a fixed
fraction of the segment duration (a downloading timeout is defined as the 80%
of the segment duration), rather than a time estimated by subtracting the
estimated download time from the segment duration. This estimated download time
could be computed in terms of median observed download time or maximum observed
time. Improvements to the re-download mechanism would lead to a more efficient
use of the network resources, since the bytes received during the HTTP
transaction are simply discarded and never played by the client.</p>

<p>The MPEG-DASH protocol allows clients to request a single range of bytes
of the media segments, as HTTP/1.1 supports the transmission of a partial
entity-body by means of the <span class="code">Content-Range</span> header field. This feature has
not been considered in our prototype, instead, the full body of the HTTP
responses is always assumed to be transferred. Requesting partial segments
(<em>subsegments</em>) could be useful to improve the re-download mechanism and
to enable faster switching between rates.</p>

<p>In our client's application, segments which are stored in the buffer are played
sequentially. This leads to a reduction of the end-to-end delay function (as it
was defined in section <a href="#sec:metrics">2.10.2</a>) if the client is not able to fetch
some segments on time, since the next segment will be played without
synchronization with the server. Segments do not have to be played
sequentially. The specification of the MPD in the MPEG-DASH standard allows the
clients to determine the time-stamp at which the segments have to be played.
Since server and client are synchronized with an external NTP server, this
feature could be added to the prototype, improving the stability of the
end-to-end delay function presented in the evaluation chapter.</p>

<p>Apple-HLS is natively supported on Android 3.0 onwards
(section <a href="#sec:protocols-android">2.10.2</a> provides a summary of the adaptive
protocols supported on Android). It would be interesting to evaluate the
capabilities of the official implementation of Apple-HLS on Android OS, in
order to determine the behaviour of the adaptation mechanism developed by
Google and potentially providing a comparison with the results achieved in our
evaluation.</p>

<p>The evaluation presented in this work does not cover situations where the
client disconnects from the streaming server and restarts the connection (for
instance, due to a network failure). The adaptation mechanisms could be improved
by storing information about the previous state of the network, in order to
avoid the repetition of the switching operations at the beginning of the session.
Additionally, other reliable transport protocol such as SCTP could be used
instead of TCP. SCTP can improve the measured throughput [<a href="#rajamani">60</a>] and
provides better protection against Denial of Service attacks.</p>

<p>Finally, it would be interesting to study the scalability of the system.
Scalability could be improved if a <em>caching proxy</em> is displaced between
clients and servers. This caching proxy would store the most frequent content
requested by the end-users. Once a client requests a media stream, following
clients which request the same content will experience better throughput
measurements, reducing the influence of the selected adaptation mechanism.</p>

<h1><a name="bibliography"></a>Bibliography</h1>

<div class="bibliography">

	<p>[1] <a name="ts26234"></a>3GPP TS.234. Transparent end-to-end packet switched streaming service
	(PSS); Adaptive HTTP Streaming. Section 12 [cited 2011, April 6].</p>

	<p>[2] <a name="ts26244"></a>3GPP TS.244. Transparent end-to-end packet switched streaming service
	(PSS); 3GPP file format (3GP). Release 10. 2011, June [cited 2011, August 3].</p>

	<p>[3] <a name="abboud"></a>O. Abboud, T. Zinner, K. Pussep, S. Al-Sabea, and R. Steinmetz. On the
	Impact of Quality Adaptation in SVC-based P2P Video-on-Demand Systems. <em>ACM
	Multimedia Systems Conference (MMSys)</em>. 2011, February 23-25. San Jose,
	California, USA.</p>

	<p>[4] <a name="rtmp"></a>Adobe Systems Inc. Real-Time Messaging Protocol (RTMP) specification.
	2009. Available from: <a href="http://www.adobe.com/devnet/rtmp.html">here</a>.</p>

	<p>[5] <a name="akhshabi"></a>S. Akhshabi, A. C. Begen, and C. Dovrolis. An Experimental Evaluation of
	Rate-Adaptation Algorithms in Adaptive Streaming over HTTP. <em>ACM Multimedia
	Systems Conference (MMSys)</em>. 2011, February 23-25. San Jose, California, USA.</p>

	<p>[6] <a name="android-dev"></a>Android developers' site. Available from: <a href="http://developer.android.com">here</a>.</p>

	<p>[7] <a name="android-distribution"></a>Android developer's site: platform versions [updated 2011, July 5; cited 2011,
	July 31]. Available from: <a href="http://developer.android.com/resources/dashboard/platform-versions.html">here</a>.</p>

	<p>[8] <a name="android-ref-mediaplayer"></a>Android's media player reference [cited 2011, July 25]. Available from
	<a href="http://developer.android.com/reference/android/media/MediaPlayer.html">here</a>.</p>

	<p>[9] <a name="androidplot"></a>AndroidPlot. Available from <a href="http://androidplot.com">here</a>.</p>

	<p>[10] <a name="hls-practices"></a>Apple Corporation. Best Practices for Creating and
	Deploying HTTP Live Streaming Media for the iPhone and iPad. <em>iOS Reference
	Library. Technical Note TN2224</em>. 2010, March 19 [updated 2010, April 19; cited
	2011, February 17]. Available from: <a href="http://developer.apple.com/library/ios/#technotes/tn2010/tn2224.html#//apple_ref/doc/uid/DTS40009745">here</a>.</p>

	<p>[11] <a name="hls-overview"></a>
	Apple Corporation. HTTP Live Streaming Overview. <em>iOS Reference Library</em>
	[updated 2010, November 15; cited 2011, July 19]. Available from:
	<a href="http://developer.apple.com/library/mac/documentation/NetworkingInternet/Conceptual/StreamingMediaGuide/StreamingMediaGuide.pdf">here</a>.</p>

	<p>[12] <a name="rfc-6265"></a>A. Barth. HTTP State Management Mechanism. IETF Network Working Group, Request
	For Comments: 6265. 2011, April. Available from:
	<a href="http://tools.ietf.org/html/rfc6265">here</a>.</p>

	<p>[13] <a name="bellard"></a>F. Bellard et al., FFmpeg libraries. Available from:
	<a href="http://www.ffmpeg.org">here</a>.</p>

	<p>[14] <a name="rfc-2396"></a>T. Berners-Lee, R. Fielding, and L. Masinter. Uniform
	Resource Identifiers (URI): Generic Syntax. IETF Network Working Group, Request
	For Comments: 2396. 1998, August. Available from:
	<a href="http://tools.ietf.org/html/rfc2396">here</a>.</p>

	<p>[15] <a name="biron"></a>P. V. Biron and A. Malhotra. XML Schema Part 2: Datatypes Second Edition
	[updated 2004, October 24; cited 2011, August 1]. W3C Recommendation. Available from:
	<a href="http://www.w3.org/TR/xmlschema-2">here</a>.</p>

	<p>[16] <a name="sintel"></a>Blender Foundation. Sintel short film. Released under Creative
	Commons Attribution (CC-A) license. Directed by Colin Levy, produced by Ton
	Roosendaal. 2010, September. Available from <a href="http://www.sintel.org">here</a>. Entry
	in the Internet Movie Database (IMDb): <a href="http://www.imdb.com/title/tt1727587">here</a>.</p>

	<p>[17] <a name="brandenburg"></a>K. Brandenburg. MP3 and AAC explained. <em>AES 17th
	International Conference on High Quality Audio Coding</em>. Fraunh&ouml;fer Institute
	for Integrated Circuits FhG-IIS A, Erlangen, Germany Erlangen, Germany. 1999.</p>

	<p>[18] <a name="bray"></a>T. Bray, J. Paoli, C. M. Sperberg-McQueen, and E. Maler. Extensible Markup
	Language (XML) 1.0 (Second Edition). W3C Working Draft 14. 2000, August.
	Available from: <a href="http://www.w3.org/TR/2000/WD-xml-2e-20000814">here</a>.</p>

	<p>[19] <a name="piff"></a>J. A. Bocharov, Q. Burns, F. Folta, K. Hughes, A. Murching, L. Olson,
	P. Schnell, and J. Simmons Protected Interoperable File Format (PIFF).
	Microsoft Corp. 2009, September 8 [updated 2010, March 9; cited 2011, February 21]. Available from: <a href="http://go.microsoft.com/?linkid=9682897">here</a>.</p>

	<p>[20] <a name="dummynet-planetlab"></a>M. Carbone and L. Rizzo. An emulation tool for PlanetLab. 2010, February.
	Available from <a href="http://info.iet.unipi.it/ luigi/papers/20100316-cc-preprint.pdf">here</a>.</p>

	<p>[21] <a name="dummynet-revisited"></a>M. Carbone and L. Rizzo. Dummynet revisited. <em>SIGCOMM CCR, vol. 40, n. 2</em>.
	2010, April. Available from <a href="http://info.iet.unipi.it/ luigi/papers/20100304-ccr.pdf">here</a>.</p>

	<p>[22] <a name="carlacci"></a>A. F. Carlacci. Ogg Vorbis and MP3 Audio Stream characterization.
	University of Alberta. 2002, September.</p>

	<p>[23] <a name="cicco"></a>L. De Cicco, S. Mascolo, and V. Palmisano. Feedback Control for Adaptive
	Live Streaming. <em>ACM Multimedia Systems Conference (MMSys)</em>. 2011, February
	23-25. San Jose, California, USA.</p>

	<p>[24] <a name="cisco"></a>Cisco. Cisco Visual Networking Index: Global Mobile Data Traffic Forecast
	Update 2010 to 2015. 2011, February 1. Available from:
	<a href="http://www.cisco.com/en/US/solutions/collateral/ns341/ns525/ns537/ns705/ns827/white_paper_c11-520862.html">here</a>.</p>

	<p>[25] <a name="rfc-3466"></a>M. Day, B. Cain, G. Tomlinson, and P. Rzewski. A Model for Content
	Internetworking (CDI). IETF Network Working Group, Request For Comments:
	3466, section 2.4. 2003, February. Available from:
	<a href="http://tools.ietf.org/html/rfc3466">here</a>.</p>

	<p>[26] <a name="eklof"></a>W. Ekl&ouml;f. Adaptive Video Streaming. Master of Science Thesis. KTH
	(COS/CCS 2008-28). Stockholm, Sweden, 2008.</p>

	<p>[27] <a name="evensen"></a>K. Evensen, D. Kaspar, C. Griwodz, and P. Halvorsen. Improving the
	Performance of Quality-Adaptive Video Streaming over Multiple Heterogeneous
	Access Networks. <em>ACM Multimedia Systems Conference (MMSys)</em>. 2011,
	February 23-25. San Jose, California, USA.</p>

	<p>[28] <a name="every"></a>S. V. Every. Pro Android Media: Developing Graphics, Music, Video, and
	Rich Media Apps for Smartphones and Tablets. Apress. USA. 2010, December.</p>

	<p>[29] <a name="andrew"></a>A. Fecheyr-Lippens. A review of HTTP Live Streaming. 2010, January [cited
	2011, February 16]. Available from: <a href="http://andrewsblog.org/a_review_of_http_live_streaming.pdf">here</a>.</p>

	<p>[30] <a name="fettig"></a>A. Fettig. Twisted: Network Programming Essentials. O'Really Media. 2006.</p>

	<p>[31] <a name="rfc-2616"></a>R. Fieldning, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, and T.
	Berners-Lee. Hypertext Transfer Protocol (HTTP/1.1). IETF Network Working
	Group, Request for Comments: 2616. 1999, June. Available from: <a href="http://tools.ietf.org/html/rfc2616">here</a>.</p>

	<p>[32] <a name="rfc-2045"></a>N. Freed and N. Borenstein. Multipurpose Internet Mail Extensions (MIME)
	Part One: Format of Internet Message Bodies. IETF Network Working Group,
	Request for Comments: 4281. 1996, November. Available from:
	<a href="http://tools.ietf.org/html/rfc2045">here</a>.</p>

	<p>[33] <a name="gartner"></a>Gartner, Inc. Market Share Analysis: Mobile Devices, Worldwide, 1Q11.
	2011, May 18. Available from: <a href="http://www.gartner.com/it/page.jsp?id=1689814">here</a>.</p>

	<p>[34] <a name="rfc-4281"></a>R. Gellens, D. Singer, and P. Fr&ouml;jdh. The Codecs Parameter for
	"Bucket" Media Types. IETF Network Working Group, Request for Comments: 4281. 2005,
	November. Available from: <a href="http://tools.ietf.org/html/rfc4281">here</a>.</p>

	<p>[35] <a name="goerzen"></a>J. Goerzen. Foundations of Python Network Programming. Apress. 2004.</p>

	<p>[36] <a name="hacker"></a>S. Hacker. MP3: The Definitive Guide. O'Really Media. 2000, March.</p>

	<p>[37] <a name="hassoun"></a>D. Hassoun. Dynamic streaming in Flash Media Server 3.5: Overview of the
	new capabilities [updated 2010, August 16; cited 2011, February 15]. Available
	from: <a href="http://www.adobe.com/devnet/flashmediaserver/articles/dynstream_advanced_pt1.html">here</a>.</p>

	<p>[38] <a name="html5"></a>I. Hickson. HTML5. A vocabulary and associated APIs for HTML and XHTML.
	W3C Working Draft. 2011, May 25 [cited 2011, August 3]. Available from:
	<a href="http://www.w3.org/TR/html5">here</a>.</p>

	<p>[39] <a name="mp3"></a>ISO/IEC 11172-3:199. Information Technology. Coding of moving pictures
	and associated audio for digital storage media at up to about 1,5 Mbit/s. Part
	3: Audio. 1993.</p>

	<p>[40] <a name="mpeg2-aac"></a>ISO/IEC 13818-7:2006. Information technology. Generic coding of
	moving pictures and associated audio information. Part 7: Advanced Audio Coding
	(AAC). 2006.</p>

	<p>[41] <a name="mpeg4-aac"></a>ISO/IEC 14496-3:2009. Information technology. Coding of audio-visual
	objects. Part 3: Audio. 2009.</p>

	<p>[42] <a name="h264"></a>ITU-T and ISO/IEC JTC1, H.264 and ISO/IEC 14 496-10 (MPEG-4) AVC
	Recommendation. Advanced video coding for generic audiovisual services.
	2003. Available from: <a href="http://www.itu.int/rec/T-REC-H.264-201003-I/en">here</a>.</p>

	<p>[43] <a name="m2ts"></a>ITU-T H.222 Recommendation. ISO/IEC 13818-1:2000. Available from
	<a href="http://www.itu.int/rec/T-REC-H.222.0-199507-S/">here</a>.</p>

	<p>[44] <a name="h263"></a>ITU-T H.263 Recommendation. Video coding for low bit rate communication.
	2005, January.</p>

	<p>[45] <a name="khemmarat"></a>S. Khemmarat, R. Zhou, L. Gao, and M. Zink. Watching User Generated Videos
	with Prefetching. <em>ACM Multimedia Systems Conference (MMSys)</em>. 2011,
	February 23-25. San Jose, California, USA.</p>

	<p>[46] <a name="kuschnig"></a>R. Kuschnig, I. Kofler, and H. Hellwagner. Evaluation of HTTP-based
	Request-Response Streams for Internet Video Streaming. <em>ACM Multimedia
	Systems Conference (MMSys)</em>. 2011, February 23-25. San Jose, California, USA.</p>

	<p>[47] <a name="laurie"></a>B. Laurie and P. Laurie. Apache: The Definitive Guide, Third Edition.
	O'Really Media. 2002, December.</p>

	<p>[48] <a name="lee"></a>J. Y. B. Lee. Scalable continuous media streaming systems: Architecture,
	design, analysis and implementation. John Wiley &amp; Sons, Ltd. Kong Kong,
	China. 2005.</p>

	<p>[49] <a name="liu"></a>C. Liu, I. Bouazizi, and M. Gabbouj. Rate Adaptation for Adaptive HTTP
	Streaming. <em>ACM Multimedia Systems Conference (MMSys)</em>. 2011, February
	23-25. San Jose, California, USA.</p>

	<p>[50] <a name="longtail"></a>Longtail video. Adaptive HTTP Streaming Framework. Release 1.0 alpha. 2011,
	February.</p>

	<p>[51] <a name="mcdonald-segmenter"></a>C. McDonald. HTTP Live Video Stream Segmenter and Distributor, 2009, July
	[updated 2010, April 5; cited 2011, February 16]. Available from:
	<a href="http://www.ioncannon.net/projects/http-live-video-stream-segmenter-and-distributor">here</a>.</p>

	<p>[52] <a name="mcdonald"></a>C. McDonald. iPhone Windowed HTTP Live Streaming Using Amazon S3 and
	Cloudfront Proof of Concept, 2009, July 5 [cited 2011, February 16]. Available
	from:
	<a href="http://www.ioncannon.net/programming/475/iphone-windowed-http-live-streaming-using-amazon-s3-and-cloudfront-proof-of-concept">here</a>.</p>

	<p>[53] <a name="iss-smooth"></a>Microsoft Corporation. ISS Smooth Streaming Transport Protocol. 2009,
	September.</p>

	<p>[54] <a name="silverlight"></a>Microsoft Corporation. Silverlight 5 Beta. Technical features. [cited 2011,
	August 8]. Available from:
	<a href="http://i1.silverlight.net/content/downloads/silverlight_5_beta_features.pdf">here</a>.</p>

	<p>[55] <a name="smooth"></a>Microsoft Corporation. Microsoft Live Smooth Streaming. Available from:
	<a href="http://www.iis.net/download/LiveSmoothStreaming">here</a>. Technical Overview
	available from:
	<a href="http://www.microsoft.com/downloads/en/details.aspx?displaylang=en&amp;FamilyID=03d22583-3ed6-44da-8464-b1b4b5ca7520">here</a>.</p>

	<p>[56] <a name="sntp-rfc4330"></a>D. Mills. Simple Network Time Protocol (SNTP) Version 4, IETF Networking
	Working Group, Request For Comments: 4330. January 2006. Available from:
	<a href="http://tools.ietf.org/html/rfc4330">here</a>.</p>

	<p>[57] <a name="muller"></a>C. M&uuml;ller and C. Timmerer. A Test-Bed for the Dynamic Adaptive
	Streaming over HTTP featuring Session Mobility. <em>ACM Multimedia Systems
	Conference (MMSys)</em>. 2011, February 23-25. San Jose, California, USA.</p>

	<p>[58] <a name="nussbaum"></a>L. Nussbaum and O. Richard. A comparative study of network link
	emulators. Proceedings of the 2009 Spring Simulation Multiconference.
	2009. Available from:
	<a href="http://www.loria.fr/ lnussbau/files/netemulators-cns09.pdf">here</a>.</p>

	<p>[59] <a name="pantos"></a>R. Pantos and W. May. HTTP Live Streaming, version 6. IETF Internet-Draft
	[updated 2011, March 31; cited 2011, April 6]. Expires 2011, October 2.
	Available from: <a href="http://tools.ietf.org/html/draft-pantos-http-live-streaming-06">here</a>.</p>

	<p>[60] <a name="rajamani"></a>R. Rajamani, S. Kumar, N. Gupta. SCTP versus TCP: Comparing the
	Performance of Transport Protocols for Web Traffic. 2002, July 22. University of
	Wisconsin-Madison, USA.</p>

	<p>[61] <a name="ransburg"></a>M. Ransburg, M. Jonke, and H. Hellwagner. An Evaluation of Mobile End
	Devices in Multimedia Streaming Scenarios. First International Workshop on
	Mobile Multimedia Networking (IWMMN). 2010, June 30. Chicago, USA. Available
	from: <a href="http://www-itec.uni-klu.ac.at/publications/mmc/paper9355.pdf">here</a>.</p>

	<p>[62] <a name="safiqul"></a>Md. Safiqul Islam. A HTTP Streaming Video Server with Dynamic
	Advertisement Splicing. Master of Science Thesis. KTH (TRITA-ICT-EX-2010:46).
	Stockholm, Sweden, 2010.</p>

	<p>[63] <a name="s5830"></a>Samsung. Galaxy Ace GT-S5830 specifications [cited 2011, August 6]. Available
	from: <a href="http://www.samsung.com/uk/consumer/mobile-devices/mobile-phones/touch-screen/GT-S5830OKAXEU/index.idx?pagetype=prd_detail&amp;tab=specification">here</a>.</p>

	<p>[64] <a name="sanchez"></a>Y. S&aacute;nchez, T. Schierl, C. Hellge, D. De Vleeschauwer, and W. Van Leekwijck.
	iDASH: Improved Dynamic Adaptive Streaming over HTTP using Scalable Video
	Coding. <em>ACM Multimedia Systems Conference (MMSys)</em>. 2011, February 23-25.
	San Jose, California, USA.</p>

	<p>[65] <a name="rfc-3550"></a>H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson. RTP: A Transport
	Protocol for Real-Time Applications. IETF Network Working Group, Request for
	Comments: 3550. 2003, July. Available from: <a href="http://tools.ietf.org/html/rfc3550">here</a>.</p>

	<p>[66] <a name="rfc-2326"></a>H. Schulzrinne, A. Rao, and R. Lanphier. Real Time Streaming Protocol
	(RTSP). IETF Network Working Group, Request for Comments: 2326. 1998, April.
	Available from: <a href="http://tools.ietf.org/html/rfc2326">here</a>.</p>

	<p>[67] <a name="siraj"></a>M. F. Siraj. HTTP Based Adaptive Streaming over HSPA. Master of Science Thesis. KTH
	(EES 2011-04). Stockholm, Sweden, 2011, April.</p>

	<p>[68] <a name="sqlite"></a>SQLite. Available from <a href="http://www.sqlite.org">here</a>.</p>

	<p>[69] <a name="stockhammer"></a>T. Stockhammer. Dynamic Adaptive Streaming over HTTP - Standards and
	Design Principles. <em>ACM Multimedia Systems Conference (MMSys)</em>. 2011,
	February 23-25. San Jose, California, USA.</p>

	<p>[70] <a name="webm"></a>WebM. An open web media project. Available from:
	<a href="http://www.webmproject.org">here</a>.</p>

	<p>[71] <a name="x264"></a>x264 VideoLan libraries. Available from:
	<a href="http://www.videolan.org/developers/x264.html">here</a>.</p>

	<p>[72] <a name="xiph"></a>Xiph.Org Foundation. Vorbis I specification [updated 2010, February 3;
	cited 2011, August 3].</p>

	<p>[73] <a name="yao"></a>J. Yao, S. S. Kanhere, I. Hossain, and M. Hassan. Empirical Evaluation of HTTP Adaptive
	Streaming under Vehicular Mobility. 2011. Sydney, Australia.</p>

	<p>[74] <a name="zambelli"></a>A. Zambelli. ISS smooth streaming technical overview. Microsoft
	Corporation, 2009, March.</p>

</div>

<h1><a name="app:gui"></a><a name="A"></a><span class="num">A</span>Demonstration of the client's application</h1>

<p>This appendix explains in more detail the features of the client's
application. Section <a href="#sec:graph">A.1</a> and section <a href="#sec:logs">A.2</a>
introduce the graph generator and the logging system.
Section <a href="#sec:screenshots">A.3</a> presents the elements of the client's
graphical user interface (GUI).</p>

<h2><a name="sec:graph"></a><a name="A.1"></a><span class="num">A.1</span>Graph generator</h2>

<p>The client's GUI shows several graphs which are dynamically generated
during the streaming session. These graphs are created with
AndroidPlot [<a href="#androidplot">9</a>], a Java API designed exclusively for the Android
platform<a href="#footnote40" name="note40"><sup>40</sup></a>. Two
types of graphs are available in our prototype (see figure <a href="#fig:graphs">A.1</a>):</p>

<ul>
	<li><strong>Bandwidth graph</strong> Compares the measured bandwidth (green
	plot) with the selected media bit-rate (red plot).</li>
	
	<li><strong>Download graph</strong> Represents the segments' downloading time (pink
	plot) and the segments' loading time (blue bars). The average of each
	plot is calculated and depicted in purple and dark blue colours,
	respectively. Note that the first blue bar corresponds to the start-up delay,
	whereas successive bars illustrate the time it takes to load the media segment
	into the <em>fake</em> player.</li>
</ul>

<div class="figure">
	<a name="fig:graphs"></a>
	<table class="center">
		<tr>
			<td><a name="fig:graph1"></a><img src="img/bandwidth-graph.png" alt="Sample bandwidth graph."/></td>
			<td><a name="fig:profile"></a><img src="img/download-graph.png" alt="Sample download graph."/></td>
		</tr>
		<tr>
			<td><strong>(a)</strong> Sample bandwidth graph.</td>
			<td><strong>(b)</strong> Sample download graph.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.1</span>Available graphs in the client's application.
	</div>
</div>

<h2><a name="sec:logs"></a><a name="A.2"></a><span class="num">A.2</span>Logging system</h2> 

<p>The client's application creates several plain-text files (<em>logs</em>). These
files are updated throughout the session for each measurement (segments'
downloading times, measured throughput, selected media bit-rate, size of the
buffer...), as defined in section <a href="#sec:metrics">5.1.6</a>. At the end of the session, a summary file is
generated. Listing <a href="#lst:log-summary">A.1</a> shows an example:</p>

<div class="listing">
	<a name="lst:log-summary"></a>
	<div class="caption">
		<span class="captionTitle">Listing A.1</span> Summary log file.
	</div>
<pre>
# Session log file
# Starting at 20110709T210900
# Model: GT-S5830
# Brand: Samsung
# Id: FROYO
# Display: FROYO.XWKA9
# 602.482 END SESSION. TOTALS:
# HTTP requests (manifest): 138
# HTTP requests (segments): 78
# Start-up delay: 8.802 s
# Total pause (excluding initial pause): 0.0 s
# Segments played: 70
# Segments missed (404 errors): 1
# Segments skipped: 0
# Segments skipped (due to FFmpeg error): 0
</pre>
</div>

<h2><a name="sec:screenshots"></a><a name="A.3"></a><span class="num">A.3</span>Overview of the client's GUI</h2>

<p>The following subsections explain the features of the client's GUI.</p>

<h3><a name="A.3.1"></a><span class="num">A.3.1</span>Adding media sources</h3>

<p>Media sources can be added into the list using the <em>options menu</em> (see
figure <a href="#fig:add">A.2a</a>). <em>New manifest URL</em> opens a new text dialog, where
the URL of the manifest can be inserted (figure <a href="#fig:url-dialog">A.2b</a>). The
list of media sources is stored in a private database (i.e., only accessible from
the client's application) using SQLite [<a href="#sqlite">68</a>], since it is fully
supported on Android.</p>

<div class="figure">
	<table class="center">
		<tr>
			<td><a name="fig:add"></a><img src="img/device-2011-10-05-131208.png" alt="Options menu."/></td>
			<td><a name="fig:url-dialog"></a><img src="img/device-2011-10-05-131236.png" alt="New manifest dialog."/></td>
			<td><a name="fig:updated"></a><img src="img/device-2011-10-05-131300.png" alt="List updated."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Options menu.</td>
			<td><span class="captionTitle">(b)</span> New manifest dialog.</td>
			<td><span class="captionTitle">(c)</span> List updated.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.2</span> Adding media sources.
	</div>
</div>

<p>The application checks the existing entries on the database to avoid
duplicate URLs. If the inserted URL was not registered on the database, the list
of media sources is updated with the new URL (figure <a href="#fig:updated">A.2c</a>). The application
distinguishes between MPEG-DASH and Apple-HLS sources (<span class="code">.3gm</span>} and
<span class="code">.m3u8</span> file extensions, respectively), displaying them with different
icons.</p>

<h3><a name="A.3.2"></a><span class="num">A.3.2</span>Importing multiple media sources</h3>

<p>Multiple URIs can be added using the option <em>Import list</em>. This option
opens a new dialog to select the file which contains a list of URIs. This list
is parsed and the database is updated.</p>

<h3><a name="A.3.3"></a><span class="num">A.3.3</span>Searching for media sources</h3>

<p>The option <em>Search manifests</em> opens a new dialog to specify a web server's
URI (figure <a href="#fig:search">A.3a</a>). The client's application performs an HTTP request
and parses the HTTP response, searching for MPEG-DASH or Apple-HLS sources,
i.e., URIs with <span class="code">.3gm</span> or <span class="code">.m3u8</span> extensions
(figure <a href="#fig:search-loading">A.3b</a>). If so, URIs are added to the list of sources
(figure <a href="#fig:search-updated">A.3c</a>).</p>

<div class="figure">
	<table class="center">
		<tr>
			<td><a name="fig:search"></a><img src="img/002.png" alt="Text dialog."/></td>
			<td><a name="fig:search-loading"></a><img src="img/003.png" alt="Loading widget."/></td>
			<td><a name="fig:search-updated"></a><img src="img/004.png" alt="List updated."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Text dialog.</td>
			<td><span class="captionTitle">(b)</span> Loading widget.</td>
			<td><span class="captionTitle">(c)</span> List updated.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.3</span> Searching for media sources.
	</div>
</div>

<h3><a name="A.3.4"></a><span class="num">A.3.4</span>Modifying and deleting media sources</h3>

<p>Media sources can be modified of deleted using the <em>context menu</em> as
shown in figure <a href="#fig:context">A.4a</a>. The context menu is triggered by
long-pressing any element of the list. Figure <a href="#fig:delete-all">A.4b</a> shows how to
delete all the media sources. This action is followed by a confirmation dialog
(figure <a href="#fig:confirmation">A.4c</a>).</p>

<div class="figure">
	<table class="center">
		<tr>
			<td><a name="fig:context"></a><img src="img/device-2011-10-05-225148.png" alt="Context menu."/></td>
			<td><img src="img/device-2011-10-05-231632.png" alt="Options menu."/></td>
			<td><img src="img/device-2011-10-05-231658.png" alt="Confirmation dialog."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Context menu.</td>
			<td><span class="captionTitle">(b)</span> Options menu.</td>
			<td><span class="captionTitle">(c)</span> Confirmation dialog.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.4</span> Modifying and deleting media sources.
	</div>
</div>

<h3><a name="A.3.5"></a><span class="num">A.3.5</span>Opening a media source</h3>

<p>If a media source is selected from the list, the application shows
the <em>Settings Activity</em>w (figure <a href="#fig:settings">A.5a</a>). There are two
parameters that can be selected in this view: (1) the adaptation mechanism and
(2) the graph to be displayed on the screen. Note that the displayed graph can
be switched during playback, but the adaptation mechanism can not be changed
once the streaming session is started (<em>play</em> button).</p>

<div class="figure">
	<table class="center">
		<tr>
			<td><a name="fig:settings"></a><img src="img/device-2011-10-05-131711.png" alt="Settings Activity."/></td>
			<td><a name="fig:profile"></a><img src="img/device-2011-10-05-131654.png" alt="Adaptation dialog."/></td>
			<td><a name="fig:graph"></a><img src="img/device-2011-10-05-131746.png" alt="Graph dialog."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Settings Activity.</td>
			<td><span class="captionTitle">(b)</span> Adaptation dialog.</td>
			<td><span class="captionTitle">(c)</span> Graph dialog.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.5</span> Selection of the session parameters.
	</div>
</div>

<h3><a name="A.3.6"></a><span class="num">A.3.6</span>Playback during the streaming session</h3>

<p>Figure <a href="#fig:seg1">A.6a</a> shows the state of the graph at the beginning of the
streaming session, when the first segment has been downloaded. The graph is
updated when successive segments are downloaded, as it is shown in
figure <a href="#fig:seg2">A.6b</a> and figure <a href="#fig:seg3">A.6c</a>.</p>

<div class="figure">
	<table class="center">
		<tr>
			<td><a name="fig:seg1"></a><img src="img/device-2011-10-05-143355.png" alt="Playing segment 1."/></td>
			<td><a name="fig:seg2"></a><img src="img/device-2011-10-05-143408.png" alt="Playing segment 2."/></td>
			<td><a name="fig:seg3"></a><img src="img/device-2011-10-05-143416.png" alt="Playing segment 3."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Playing segment 1.</td>
			<td><span class="captionTitle">(b)</span> Playing segment 2.</td>
			<td><span class="captionTitle">(c)</span> Playing segment 3.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.6</span> Dynamic graphs.
	</div>
</div>

<p>Figure <a href="#fig:shot1">A.7</a> to <a href="#fig:shot4">A.10</a> show the client's application during
the performance evaluation over the scenarios 1 to 4 (defined in
section <a href="#sec:network-scenarios">5.1.7</a>).</p>

<div class="figure">
	<a name="fig:shot1"></a>
	<table class="center">
		<tr>
			<td><img src="img/device-2011-10-05-135633.png" alt="Displaying bandwidth graph."/></td>
			<td><img src="img/device-2011-10-05-135643.png" alt="Displaying download graph."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Displaying bandwidth graph.</td>
			<td><span class="captionTitle">(b)</span> Displaying download graph.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.7</span> Sample of playback using the conservative mechanism over the
	scenario 1.
	</div>
</div>

<div class="figure">
	<a name="fig:shot2"></a>
	<table class="center">
		<tr>
			<td><img src="img/device-2011-10-05-140740.png" alt="Displaying bandwidth graph."/></td>
			<td><a name="shot2-b"></a><img src="img/device-2011-10-05-140803.png" alt="Displaying download graph."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Displaying bandwidth graph.</td>
			<td><span class="captionTitle">(b)</span> Displaying download graph.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.8</span> Sample of playback using the conservative mechanism over the
	scenario 2.
	</div>
</div>

<div class="figure">
	<a name="fig:shot3"></a>
	<table class="center">
		<tr>
			<td><img src="img/device-2011-10-05-141908.png" alt="Displaying bandwidth graph."/></td>
			<td><a name="shot3-b"></a><img src="img/device-2011-10-05-141935.png" alt="Displaying download graph."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Displaying bandwidth graph.</td>
			<td><span class="captionTitle">(b)</span> Displaying download graph.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.9</span> Sample of playback using the conservative mechanism over the
	scenario 3.
	</div>
</div>

<div class="figure">
	<a name="fig:shot4"></a>
	<table class="center">
		<tr>
			<td><img src="img/device-2011-10-05-143016.png" alt="Displaying bandwidth graph."/></td>
			<td><img src="img/device-2011-10-05-143034.png" alt="Displaying download graph."/></td>
		</tr>
		<tr>
			<td><span class="captionTitle">(a)</span> Displaying bandwidth graph.</td>
			<td><span class="captionTitle">(b)</span> Displaying download graph.</td>
		</tr>
	</table>
	<div class="caption">
		<span class="captionTitle">Figure A.10</span> Sample of playback using the conservative mechanism over the
	scenario 4.
	</div>
</div>

<p>Note that the blue bars depicted in figure <a href="#fig:shot2-b">A.8b</a>
and <a href="#fig:shot3-b">A.9b</a> represent interruptions on playback due to buffer
underflow. If so, the client's GUI displays the last video frame on the screen
and a <em>loading wheel animation</em> until the next media segment starts
playing.</p>

<h1><a name="app:ffmpeg"></a><a name="B"></a><span class="num">B</span>FFmpeg capabilities</h1>

<p>This appendix includes further information about the transcoding capabilities
of the FFmpeg libraries. Tables <a href="#tab:ffmpeg-audio-codecs">B.1</a>
to <a href="#tab:ffmpeg-video-codecs">B.3</a> summarize the supported audio/video CODECs and
container file formats.</p>

<div class="table">
	<div class="caption">
		<span class="captionTitle">Table B.1</span> FFmpeg supported audio CODECs. Extracted from [<a href="#bellard">13</a>].
	</div>
	
	<table>
		<tr class="header">
			<td>Name</td><td>Encoding</td><td>Decoding</td><td>Details</td>
		</tr>
		<tr>
			<td>AAC</td><td>Yes</td><td>Yes</td><td>Encoding supported through external library <span class="code">libfaac</span></td>
		</tr>
		<tr>
			<td>MP3 (MPEG audio layer 3)</td><td>Yes</td><td>Yes</td><td>Encoding supported through external library LAME, ADU MP3 and MP3 on MP4 also supported</td>
		</tr>
		<tr>
			<td>Vorbis</td><td>Yes</td><td>Yes</td><td>A native but very primitive encoder exists</td>
		</tr>
	</table>
</div>

<div class="table">
	<div class="caption">
		<span class="captionTitle">Table B.2</span> FFmpeg supported container formats. Extracted from [<a href="#bellard">13</a>].
	</div>
	
	<table>
		<tr class="header">
			<td>Name</td><td>Encoding</td><td>Decoding</td><td>Details</td>
		</tr>
		<tr>
			<td>Flash (SWF)</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>Flash Video (FLV)</td><td>No</td><td>Yes</td><td>Macromedia Flash video files</td>
		</tr>
		<tr>
			<td>MOV/QuickTime/MP4</td><td>Yes</td><td>Yes</td><td>3GP, 3GP2, PSP, iPod variants supported</td>
		</tr>
		<tr>
			<td>MPEG-TS (transport stream)</td><td>Yes</td><td>Yes</td><td>Also known as DVB Transport Stream</td>
		</tr>
		<tr>
			<td>MPEG-4</td><td>Yes</td><td>Yes</td><td>MPEG-4 is a variant of QuickTime</td>
		</tr>
		<tr>
			<td>Ogg</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>Raw H.263</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>Raw H.264</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>Raw MPEG-2</td><td>No</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>Raw MPEG-4</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>WAV</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
	</table>
</div>

<div class="table">
	<div class="caption">
		<span class="captionTitle">Table B.3</span> FFmpeg supported video CODECs. Extracted from [<a href="#bellard">13</a>].
	</div>
	
	<table>
		<tr class="header">
			<td>Name</td><td>Encoding</td><td>Decoding</td><td>Details</td>
		</tr>
		<tr>
			<td>Flash Video (FLV)</td><td>Yes</td><td>Yes</td><td>Sorenson H.263 used in Flash</td>
		</tr>
		<tr>
			<td>H.263/H.263-1996</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>H.263+/H.263-1998/H.263 version 2</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>H.264/AVC/MPEG-4 AVC/MPEG-4 part 10</td><td>Yes</td><td>Yes</td><td>Encoding supported through external library <span class="code">libx264</span></td>
		</tr>
		<tr>
			<td>MPEG-2 video</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>MPEG-4 part 2</td><td>Yes</td><td>Yes</td><td></td>
		</tr>
		<tr>
			<td>VP8</td><td>Yes</td><td>Yes</td><td>fourcc: VP80, encoding supported through external library <span class="code">libvpx</span></td>
		</tr>
		<tr>
			<td>Theora</td><td>Yes</td><td>Yes</td><td>Encoding supported through external library <span class="code">libtheora</span></td>
		</tr>
	</table>
</div>

<h1><a name="app:integration"></a><a name="C"></a><span class="num">C</span>Integration of FFmpeg libraries using the Android NDK</h1>

<p>This appendix explains in more detail the process of integration of the
FFmpeg libraries into the client's application for Android.
Listing <a href="#lst:god">C.1</a> shows the configuration steps that are needed to compile
the FFmpeg libraries for the ARM architecture<a href="#footnote39" name="note39"><sup>39</sup></a> (see the specifications of the client's
device in table <a href="#tab:devices">5.1</a>). The
<span class="code">./configure</span> command (listing <a href="#lst:god">C.1</a>, line 12) simply specifies the flags which enable or
disable the features provided by the libraries. Note that the most important
arguments are <span class="code">--arch=arm</span> and
<span class="code">--enable-cross-compile</span> which allow the compilation for the
ARM architecture. Additional flags are provided by the arguments
<span class="code">--extra-cflags</span> and <span class="code">--extra-ldflags</span>.</p>

<div class="listing">
	<a name="lst:god"></a>
	<div class="caption">
		<span class="captionTitle">Listing C.1</span> Integration script.
	</div>
<pre>
#!/bin/bash
# Path to the prebuild directory of the Android NDK
PREBUILT=/usr/share/android-ndk/toolchains/arm-eabi-4.4.0/prebuilt/linux-x86

# Path to the platform directory of the Android NDK
PLATFORM=/usr/share/android-ndk/platforms/android-8/arch-arm

# Update PATH
export PATH=$PREBUILT/bin:$PATH

# Configure the FFmpeg libraries with the following command line
./configure --target-os=linux \
	--arch=arm \
	--enable-version3 \
	--enable-gpl \
	--enable-nonfree \
	--disable-stripping \
	--disable-ffmpeg \
	--disable-ffplay \
	--disable-ffserver \
	--disable-ffprobe \
	--disable-encoders \
	--disable-muxers \
	--disable-devices \
	--disable-protocols \
	--enable-protocol=file \
	--enable-avfilter \
	--disable-network \
	--disable-mpegaudio-hp \
	--disable-avdevice \
	--enable-cross-compile \
	--cc=$PREBUILT/bin/arm-eabi-gcc \
	--cross-prefix=$PREBUILT/bin/arm-eabi- \
	--nm=$PREBUILT/bin/arm-eabi-nm \
	--extra-cflags="-fPIC -DANDROID" \
	--disable-asm \
	--enable-neon \
	--enable-armv5te \
	--extra-ldflags="-Wl,-T,$PREBUILT/arm-eabi/lib/ldscripts/armelf.x -Wl,
	-rpath-link=$PLATFORM/usr/lib -L$PLATFORM/usr/lib
	-nostdlib $PREBUILT/lib/gcc/arm-eabi/4.4.0/crtbegin.o
	$PREBUILT/lib/gcc/arm-eabi/4.4.0/crtend.o -lc -lm -ldl"

# Compile the FFmpeg libraries using the Android NDK
ndk-build
</pre>
</div>

<p>Listing <a href="#lst:makefile">C.2</a> shows the <span class="code">Makefile</span> utilized in our prototype to
generate the static and dynamic libraries loaded by the Java application.</p>

<div class="listing">
	<a name="lst:makefile"></a>
	<div class="caption">
		<span class="captionTitle">Listing C.2</span> Android makefile (<span class="code">Android.mk</span>).
	</div>
<pre>
LOCAL_PATH := $(call my-dir)
include $(CLEAR_VARS)
...
LOCAL_CFLAGS := -D__STDC_CONSTANT_MACROS
...
LOCAL_C_INCLUDES += \
$(LOCAL_PATH)/../libffmpeg \
$(LOCAL_PATH)/../include

LOCAL_SRC_FILES := \
	onLoad.cpp \
	com_media_ffmpeg_FFMpegAVFrame.cpp \
	com_media_ffmpeg_FFMpegAVInputFormat.c \
	com_media_ffmpeg_FFMpegAVRational.c \
	com_media_ffmpeg_FFMpegAVFormatContext.c \
	com_media_ffmpeg_FFMpegAVCodecContext.cpp \
	com_media_ffmpeg_FFMpegUtils.cpp

LOCAL_SRC_FILES += \
com_media_ffmpeg_FFMpeg.c \
	../libffmpeg/cmdutils.c

LOCAL_LDLIBS := -llog

LOCAL_SHARED_LIBRARIES := libjniaudio libjnivideo
LOCAL_STATIC_LIBRARIES := libavcodec libavformat libavutil libpostproc libswscale

LOCAL_MODULE := libffmpeg_jni

include $(BUILD_SHARED_LIBRARY)
</pre>
</div>

<h1><a name="trivia"></a>Trivia</h1>

<div class="quoteblock">
	<p class="quote"><em>"I can't go to a restaurant and order food because I keep looking at the fonts on the menu."</em></p>
	<p class="quoteAuthor">- Donald Knuth</p>
</div>

<ul>
	<li>This master's thesis report is written entirely on Latex. It uses a
	template from the Royal Institute of Technology (KTH), available from:
	<a href="http://system.csc.kth.se/misc/tex">http://system.csc.kth.se/misc/tex</a>.</li>
	
	<li>The following Latex packages were used: <span class="code">graphicx</span>, <span class="code">hypens</span>,
	<span class="code">hyperref</span>, <span class="code">parskip</span>, <span class="code">subfig</span>, <span class="code">colortbl</span>, <span class="code">xcolor</span>,
	<span class="code">multirow</span>, <span class="code">tabulary</span>, <span class="code">longtable</span>, <span class="code">listings</span>,
	<span class="code">caption</span>, <span class="code">minted</span>, <span class="code">algorithm2e</span>, <span class="code">fourier</span>, and
	<span class="code">epstopdf</span>. All of them are available via <a href="http://ctan.org">http://ctan.org</a>.</li>
	
	<li>The Android logo is published by Google Inc. under the terms of the
	Creative Commons Attribution (CC-A) license. According to the brand
	guidelines: <em>"the android robot can be used, reproduced, and modified
	freely in marketing communications"</em>. <a href="http://www.android.com/branding.html">http://www.android.com/branding.html</a>.</li>
	
	<li>All figures (except screenshots of Sintel) were drawn in vectorial
	format (SVG) using Inkscape (available from <a href="http://inkscape.org">http://inkscape.org</a>). Imported into
	Latex in EPS format. The Latex package <span class="code">epstopdf</span> eases the integration.</li>
		
	<li>Plots were generated with the Graphics Layout Engine (GLE), available
	from <a href="http://glx.sourceforge.net">http://glx.sourceforge.net</a>.</li>
	
	<li>Source code listings were highlighted with the <span class="code">minted</span> Latex
	package, available from
	<a href="http://ctan.org/tex-archive/macros/latex/contrib/minted">http://ctan.org/tex-archive/macros/latex/contrib/minted</a>. It is based on the
	powerful Pygments library, available from <a href="http://www.pygments.org">http://www.pygments.org</a>.</li>
	
	<li>The word cloud showed at the end of this report was created with Wordle
	(available from <a href="http://www.wordle.net">http://www.wordle.net</a>), based on the most repeated words of this
	document.</li>
	
	<li>This master's thesis project was presented on 27 September 2011 at KTH
	(Kista campus, H&ouml;rby seminar room).</li>
</ul>

<p class="center"><img src="img/cloud.jpg" alt="Word cloud"/></p>

<h1>Notes</h1>
<div class="footnotes">
	<ol>
		<li><a name="footnote1"></a><a href="#note1"><strong>^</strong></a> Other
		approaches [<a href="#abboud">3</a>] employ peer-to-peer overlay networks to provide
		multimedia content.</li>

		<li><a name="footnote2"></a><a href="#note2"><strong>^</strong></a> Actually, cookies can be used to make HTTP stateful [<a href="#rfc-6265">12</a>]. 
		In addition, HTTP 1.1 can use persistent connections as a performance improvement [<a href="#rfc-2616">12</a>, section 8.1].</li>

		<li><a name="footnote3"></a><a href="#note3"><strong>^</strong></a> Although Microsoft has not
		adopted an official acronym for Live Smooth Streaming, it will be
		referred as LSS in this master's thesis.</li>

		<li><a name="footnote4"></a><a href="#note4"><strong>^</strong></a> Experience Smooth Streaming.
		<a href="http://www.iis.net/media/experiencesmoothstreaming">http://www.iis.net/media/experiencesmoothstreaming</a>.</li>

		<li><a name="footnote5"></a><a href="#note5"><strong>^</strong></a> Time-shifting involves recording content to a storage
		medium to be watched at a later time that is more suitable for the user.</li>

		<li><a name="footnote6"></a><a href="#note6"><strong>^</strong></a> Note that representation is
			a synonym of <em>quality level</em> or <em>bit-rate level</em> in the context of
			this master's thesis project. The three terms will be used indistinctly.</li>

		<li><a name="footnote7"></a><a href="#note7"><strong>^</strong></a> Mainly used for stereoscopic 3D video encoding.</li>

		<li><a name="footnote8"></a><a href="#note8"><strong>^</strong></a> The Android
		Development Kit (SDK) is freely available from the developers site:
		<a href="http://developer.android.com/sdk">http://developer.android.com/sdk</a>.</li>

		<li><a name="footnote9"></a><a href="#note9"><strong>^</strong></a> A free trial of VPlayer 0.9.9 can be downloaded from
		  <a href="https://market.android.com/details?id=me.abitno.vplayer.t">https://market.android.com/details?id=me.abitno.vplayer.t</a>.</li>

		<li><a name="footnote10"></a><a href="#note10"><strong>^</strong></a> Daroon Player 1.0.1 is
		  available from <a href="https://market.android.com/details?id=com.daroonsoft.player">https://market.android.com/details?id=com.daroonsoft.player</a>.</li>

		<li><a name="footnote11"></a><a href="#note11"><strong>^</strong></a> Silverlight is a Microsoft's application framework for
		creating rich Internet applications, with features similar to Adobe's Flash.</li>

		<li><a name="footnote12"></a><a href="#note12"><strong>^</strong></a> More information can be found at
		<a href="http://jeffreystedfast.blogspot.com/2011/04/moonlight-on-android.html">http://jeffreystedfast.blogspot.com/2011/04/moonlight-on-android.html</a>.</li>

		<li><a name="footnote13"></a><a href="#note13"><strong>^</strong></a> Detailed
		features and requirements can be read at <a href="http://kb2.adobe.com/cps/860/cpsid_86018.html">http://kb2.adobe.com/cps/860/cpsid_86018.html</a>.</li>

		<li><a name="footnote14"></a><a href="#note14"><strong>^</strong></a> Certified devices are listed at Adobe's official site
		<a href="http://www.adobe.com/flashplatform/certified_devices">http://www.adobe.com/flashplatform/certified_devices</a>.</li>
		
		<li><a name="footnote15"></a><a href="#note15"><strong>^</strong></a> The FFmpeg capabilities can be found in the
appendix <a href="#app:ffmpeg">B</a>.</li>

		<li><a name="footnote16"></a><a href="#note16"><strong>^</strong></a> In 3GPP's terminology,
the <em>ftyp</em> box, <em>moov</em> box and optionally the <em>pdin</em> box.</li>
		
		<li><a name="footnote17"></a><a href="#note17"><strong>^</strong></a> This is an assumption for this prototype. MPEG-DASH supports
segments which do not start with a key-frame.</li>
		
		<li><a name="footnote18"></a><a href="#note18"><strong>^</strong></a> MP4box. Available from:
<a href="http://gpac.wp.institut-telecom.fr/mp4box">http://gpac.wp.institut-telecom.fr/mp4box</a></li>
		
		<li><a name="footnote19"></a><a href="#note19"><strong>^</strong></a> The pool.ntp.org project is a virtual cluster of
timeservers which provides reliable NTP service. Available from <a href="http://www.pool.ntp.org">http://www.pool.ntp.org</a>.</li>
		
		<li><a name="footnote20"></a><a href="#note20"><strong>^</strong></a> The conditions of a MIT license can be found in <a href="http://www.opensource.org/licenses/mit-license.php">http://www.opensource.org/licenses/mit-license.php</a>.</li>
		
		<li><a name="footnote21"></a><a href="#note21"><strong>^</strong></a> Another
statistical operation such as the median could have been used. However, the
median is <em>less</em> sensitive than the mean to extreme fluctuations. Consider
the following example: the last three throughput measurements are 100 kb/s, 200 kb/s and
300 kb/s. The mean and median of the ordered list <em>{100, 200, 300}</em> are both
200 kb/s. If the next measurement is, for instance, 600 kb/s, the ordered list
is updated to <em>{100, 200, 600}</em>, where the mean is increased to 300 kb/s and
the median has not changed.</li>
		
		<li><a name="footnote22"></a><a href="#note22"><strong>^</strong></a> Detailed information about these
Java methods and the states of an activity can be found at <a href="http://developer.android.com/reference/android/app/Activity.html">http://developer.android.com/reference/android/app/Activity.html</a>.</li>
		
		<li><a name="footnote23"></a><a href="#note23"><strong>^</strong></a>This timeout is set
to avoid unnecessary consumption of CPU cycles within the segment's
load interval. Any arbitrary value longer that a segment's load average loading
time (500 ms) would be acceptable.</li>
		
		<li><a name="footnote24"></a><a href="#note24"><strong>^</strong></a> Modifications of the manifest file
are detecting by reading attributes which are meant to change, such as the
time-stamps (available start time) or the last segment index.</li>
		
		<li><a name="footnote25"></a><a href="#note25"><strong>^</strong></a> Specific parsers such as the
Piccolo XML parser for Java (see <a href="http://piccolo.sourceforge.net">http://piccolo.sourceforge.net</a>) could
have been chosen for this prototype. However, Android does not run a standard
Java virtual machine, therefore customized parsers might not offer the same
performance benefits when running on the Android virtual machine (Dalvik).
Thus, using the DOM or SAX APIs ensures a better compatibility with any version
of Android since they are officially supported.</li>
		
		<li><a name="footnote26"></a><a href="#note26"><strong>^</strong></a> The reason to choose the external storage device as a buffer is rather simple:
internal memory in Android cellphones is accessible and writable, but only
plain-text files can be saved directly. Other file types such as video
files can only be saved into the internal storage if they are transferred from
the external storage, by means of the <span class="code">openFileInput</span> and
<span class="code">openFileOutput</span> Java methods, therefore segments have to be saved into
the external storage in any case. More information about data storage on
Android devices can be found at <a href="http://developer.android.com/guide/topics/data/data-storage.html">http://developer.android.com/guide/topics/data/data-storage.html</a>.</li>
		
		<li><a name="footnote27"></a><a href="#note27"><strong>^</strong></a> External libraries (FFmpeg) perform the
conversion of container format and require a significant amount of time
smartphones' CPUs.</li>
		
		<li><a name="footnote28"></a><a href="#note28"><strong>^</strong></a> Android NDK. Available from <a href="http://developer.android.com/sdk/ndk">http://developer.android.com/sdk/ndk</a>.</li>
		
		<li><a name="footnote29"></a><a href="#note29"><strong>^</strong></a> A second alternative would have
been <strong>tc</strong>. Tc is a Linux kernel tool which can be
used to configure network traffic with similar functionality as Dummynet. A comparison
study [<a href="#nussbaum">58</a>] determined that both tc and Dummynet equally emulated
bit-rates up to 500 Mb/s.</li>
		
		<li><a name="footnote30"></a><a href="#note30"><strong>^</strong></a> WANem. Available from: <a href="http://wanem.sourceforge.net">http://wanem.sourceforge.net</a>.</li>
		
		<li><a name="footnote31"></a><a href="#note31"><strong>^</strong></a> Trickle. Available
	from: <a href="http://monkey.org/~marius/pages/?page=trickle">http://monkey.org/~marius/pages/?page=trickle</a>.</li>
		
		<li><a name="footnote32"></a><a href="#note32"><strong>^</strong></a> Gnu/Linux daemons
	are intended to be as user-independent as possible. Daemons are executed in
	background and typically provide a reduced set of commands such as
	<span class="code">start</span>, <span class="code">stop</span>, <span class="code">status</span>, and <span class="code">restart</span>.</li>
		
		<li><a name="footnote33"></a><a href="#note33"><strong>^</strong></a> The live servers analyzes all the media segments
	encoded at different bit-rates (for all the media clips), in
	order to detect missing segments (i.e., removed from the system for any
	reason) and generate manifest files accordingly. The manifest file is not
	available until this procedure has finished.</li>

		<li><a name="footnote34"></a><a href="#note34"><strong>^</strong></a> A fixed GOP setting
in the encoder simplifies the stream switching between video levels.</li>
		
		<li><a name="footnote35"></a><a href="#note35"><strong>^</strong></a> Many other conditions could have been
chosen arbitrarily. For our evaluation, the condition for <em>w<sub>short</sub>(t)</em> is defined based on the largest
value of <em>S<sub>d</sub></em> considered in our experiments, i.e., 20 s.</li>
		
		<li><a name="footnote36"></a><a href="#note36"><strong>^</strong></a> In
this master's thesis project, segments are played sequentially. If there is a
missed segment, the next buffered segment is played directly without waiting.</li>
		
		<li><a name="footnote37"></a><a href="#note37"><strong>^</strong></a> Note that the starting time of
the intervals always corresponds to a bit-rate fluctuation in the network's
emulated bandwidth.</li>

		<li><a name="footnote38"></a><a href="#note38"><strong>^</strong></a> Eurosport's live channel can be found at <a href="http://live.iphone.eurosport.com/uk1/stc_0_0.m3u8">http://live.iphone.eurosport.com/uk1/stc_0_0.m3u8</a>.</li>
		
		<li><a name="footnote39"></a><a href="#note39"><strong>^</strong></a> More information can be found at <a href="http://www.arm.com">http://www.arm.com</a>.</li>
		
		<li><a name="footnote40"></a><a href="#note40"><strong>^</strong></a> A feature comparison of graph libraries for Android can be found at <a href="http://androidplot.com/wiki/Feature_Comparison">http://androidplot.com/wiki/Feature_Comparison</a>.</li>
	
	</ol>
</div>
</body>
</html>
